{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFhVPoTySbdt",
        "outputId": "ac472e54-c0a0-4e28-d9bf-0fdd81dce833"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.8.0+cpu | CUDA available: False\n",
            "Device: cpu | Name: CPU\n"
          ]
        }
      ],
      "source": [
        "# CELL 1 — GPU sanity check  (rerun after every restart)\n",
        "import torch\n",
        "\n",
        "gpu_available = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if gpu_available else \"cpu\")\n",
        "gpu_name = torch.cuda.get_device_name(0) if gpu_available else \"CPU\"\n",
        "\n",
        "print(f\"Torch: {torch.__version__} | CUDA available: {gpu_available}\")\n",
        "print(f\"Device: {device} | Name: {gpu_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tytVS_2CSiyh",
        "outputId": "98c61774-775e-4d39-b74e-cbbf1f2c9b39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "# CELL 2 — Pin working versions (rerun once after a fresh session)\n",
        "# PyTorch 2.4.0 (CUDA 12.1 build) + matching libs\n",
        "!pip -q install --force-reinstall --no-cache-dir \\\n",
        "  torch==2.4.0+cu121 torchvision==0.19.0+cu121 torchaudio==2.4.0+cu121 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip -q install --force-reinstall --no-cache-dir \\\n",
        "  numpy==2.0.2 pandas==2.2.2 scikit-learn==1.6.1 matplotlib==3.10.0 seaborn==0.13.2 \\\n",
        "  tqdm==4.66.4 tabulate==0.9.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrSlu9azUZZY",
        "outputId": "c5616720-281d-43ae-9c73-86a50213f1fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Created folders under: /content/drive/MyDrive/FFNN_Healing_Thesis\n"
          ]
        }
      ],
      "source": [
        "#rerun this too\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/FFNN_Healing_Thesis\"\n",
        "import os\n",
        "for d in [\"data\",\"logs\",\"models\",\"figures\",\"notebooks\",\"results\"]:\n",
        "    os.makedirs(f\"{PROJECT_ROOT}/{d}\", exist_ok=True)\n",
        "print(\"Created folders under:\", PROJECT_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE0CsIHpYLY_",
        "outputId": "b4667485-a9df-47f9-818d-f87a999cb1bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Seed set. Log file will be: /content/drive/MyDrive/FFNN_Healing_Thesis/results/experiments_log.csv\n"
          ]
        }
      ],
      "source": [
        "#rerun this too\n",
        "import os, random, torch, numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/FFNN_Healing_Thesis\")\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "def timestamp():\n",
        "    return time.strftime(\"%Y%m%d-%H%M%S\")  # <— added\n",
        "\n",
        "def exp_path(kind=\"results\"):\n",
        "    p = PROJECT_ROOT / kind\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def log_result(row: dict, csv_name=\"experiments_log.csv\"):\n",
        "    csv_file = exp_path(\"results\") / csv_name\n",
        "    df = pd.DataFrame([row])\n",
        "    if csv_file.exists():\n",
        "        df0 = pd.read_csv(csv_file); df = pd.concat([df0, df], ignore_index=True)\n",
        "    df.to_csv(csv_file, index=False)\n",
        "    return str(csv_file)\n",
        "\n",
        "set_seed(123)\n",
        "print(\"Seed set. Log file will be:\", exp_path('results') / 'experiments_log.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jje3u76fZJ44",
        "outputId": "7d939c65-3b89-48ff-b623-92d6329a0374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test OK. Grad norm: 0.03132587671279907\n"
          ]
        }
      ],
      "source": [
        "set_seed(0)\n",
        "x = torch.randn(4096, 4096, device=DEVICE, requires_grad=True)\n",
        "y = (x @ x.t()).mean()\n",
        "y.backward()\n",
        "print(\"Test OK. Grad norm:\", float(x.grad.norm().item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKj8Cq8bZjmx",
        "outputId": "574ef937-ced5-4415-cee7-a61bd27b208a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV exists: True | path: /content/drive/MyDrive/FFNN_Healing_Thesis/results/experiments_log.csv\n",
            "         timestamp   phase         note\n",
            "1  20250812-083557  sanity  logger test\n"
          ]
        }
      ],
      "source": [
        "def timestamp():\n",
        "    import time\n",
        "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "_ = log_result({\n",
        "    \"timestamp\": timestamp(),\n",
        "    \"phase\": \"sanity\",\n",
        "    \"note\": \"logger test\"\n",
        "})\n",
        "import pandas as pd, os\n",
        "csv_path = \"/content/drive/MyDrive/FFNN_Healing_Thesis/results/experiments_log.csv\"\n",
        "print(\"CSV exists:\", os.path.exists(csv_path), \"| path:\", csv_path)\n",
        "print(pd.read_csv(csv_path).tail(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMeE3MAfZzhC",
        "outputId": "88673cce-26a8-46af-872c-4b97a88af420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[breast_cancer] Saved. features=30 | splits={'train': 398, 'val': 85, 'test': 86}\n",
            "  Class balance (train/val/test): {'train': {1: 250, 0: 148}, 'val': {1: 53, 0: 32}, 'test': {1: 54, 0: 32}}\n",
            "[california_housing] Saved. features=8 | splits={'train': 14448, 'val': 3096, 'test': 3096}\n",
            "\n",
            "Saved folders:\n",
            "/content/drive/MyDrive/FFNN_Healing_Thesis/data:\n",
            "breast_cancer  california_housing\n",
            "\n",
            "/content/drive/MyDrive/FFNN_Healing_Thesis/data/breast_cancer:\n",
            "meta.json      X_test.npy   X_val.npy\ty_train.npy\n",
            "scaler.joblib  X_train.npy  y_test.npy\ty_val.npy\n",
            "\n",
            "/content/drive/MyDrive/FFNN_Healing_Thesis/data/california_housing:\n",
            "meta.json      X_test.npy   X_val.npy\ty_train.npy\n",
            "scaler.joblib  X_train.npy  y_test.npy\ty_val.npy\n"
          ]
        }
      ],
      "source": [
        "# === PHASE 2: DATASET PREP (Option A) ===\n",
        "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np, os, json, joblib, pandas as pd, pathlib, collections\n",
        "\n",
        "set_seed(42)  # from our bootstrap\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/FFNN_Healing_Thesis/data\"\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "def split_scale_save(X, y, name, task_type, feature_names=None):\n",
        "    \"\"\"\n",
        "    Splits (70/15/15), scales features (fit on train), saves arrays+scaler+meta to Drive.\n",
        "    task_type: 'clf' or 'reg'\n",
        "    \"\"\"\n",
        "    # Split\n",
        "    strat = y if task_type == \"clf\" else None\n",
        "    X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
        "        X, y, test_size=0.30, random_state=42, stratify=strat\n",
        "    )\n",
        "    strat_tmp = y_tmp if task_type == \"clf\" else None\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_tmp, y_tmp, test_size=0.50, random_state=42, stratify=strat_tmp\n",
        "    )\n",
        "\n",
        "    # Scale features using train stats only\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train = scaler.transform(X_train).astype(np.float32)\n",
        "    X_val   = scaler.transform(X_val).astype(np.float32)\n",
        "    X_test  = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "    # Ensure y dtypes\n",
        "    if task_type == \"clf\":\n",
        "        y_train = y_train.astype(np.int64); y_val = y_val.astype(np.int64); y_test = y_test.astype(np.int64)\n",
        "    else:\n",
        "        y_train = y_train.astype(np.float32); y_val = y_val.astype(np.float32); y_test = y_test.astype(np.float32)\n",
        "\n",
        "    # Save\n",
        "    OUT = f\"{DATA_ROOT}/{name}\"\n",
        "    os.makedirs(OUT, exist_ok=True)\n",
        "    np.save(f\"{OUT}/X_train.npy\", X_train); np.save(f\"{OUT}/y_train.npy\", y_train)\n",
        "    np.save(f\"{OUT}/X_val.npy\",   X_val);   np.save(f\"{OUT}/y_val.npy\",   y_val)\n",
        "    np.save(f\"{OUT}/X_test.npy\",  X_test);  np.save(f\"{OUT}/y_test.npy\",  y_test)\n",
        "    joblib.dump(scaler, f\"{OUT}/scaler.joblib\")\n",
        "\n",
        "    # Metadata\n",
        "    meta = {\n",
        "        \"name\": name,\n",
        "        \"task\": task_type,\n",
        "        \"n_features\": int(X_train.shape[1]),\n",
        "        \"splits\": {\"train\": int(len(y_train)), \"val\": int(len(y_val)), \"test\": int(len(y_test))},\n",
        "        \"feature_names\": list(feature_names) if feature_names is not None else None\n",
        "    }\n",
        "    if task_type == \"clf\":\n",
        "        meta[\"class_counts\"] = {\n",
        "            \"train\": {int(k): int(v) for k,v in collections.Counter(y_train).items()},\n",
        "            \"val\":   {int(k): int(v) for k,v in collections.Counter(y_val).items()},\n",
        "            \"test\":  {int(k): int(v) for k,v in collections.Counter(y_test).items()},\n",
        "        }\n",
        "\n",
        "    with open(f\"{OUT}/meta.json\",\"w\") as f:\n",
        "        f.write(json.dumps(meta, indent=2))\n",
        "\n",
        "    # Log a one-line summary to our experiments log\n",
        "    log_result({\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"data_prep\",\n",
        "        \"dataset\": name,\n",
        "        \"task\": task_type,\n",
        "        \"n_features\": meta[\"n_features\"],\n",
        "        \"train\": meta[\"splits\"][\"train\"],\n",
        "        \"val\": meta[\"splits\"][\"val\"],\n",
        "        \"test\": meta[\"splits\"][\"test\"]\n",
        "    })\n",
        "\n",
        "    print(f\"[{name}] Saved. features={meta['n_features']} | splits={meta['splits']}\")\n",
        "    if task_type == \"clf\":\n",
        "        print(f\"  Class balance (train/val/test): {meta['class_counts']}\")\n",
        "    return meta\n",
        "\n",
        "# ---- Classification: Breast Cancer (binary) ----\n",
        "bc = load_breast_cancer()\n",
        "meta_bc = split_scale_save(\n",
        "    bc.data, bc.target, name=\"breast_cancer\", task_type=\"clf\", feature_names=bc.feature_names\n",
        ")\n",
        "\n",
        "# ---- Regression: California Housing ----\n",
        "cal = fetch_california_housing()\n",
        "meta_cal = split_scale_save(\n",
        "    cal.data, cal.target, name=\"california_housing\", task_type=\"reg\", feature_names=cal.feature_names\n",
        ")\n",
        "\n",
        "print(\"\\nSaved folders:\")\n",
        "!ls -R /content/drive/MyDrive/FFNN_Healing_Thesis/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGc544Hgc8Io",
        "outputId": "c7dc8c94-97fa-4a64-ff31-3cacb5bac646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Ready. Torch: 2.4.0+cu121 | Device: cuda | Root: /content/drive/MyDrive/FFNN_Healing_Thesis\n"
          ]
        }
      ],
      "source": [
        "# === MINIMAL BOOTSTRAP (run after every restart) ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, random, time, torch, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/FFNN_Healing_Thesis\")\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def timestamp(): return time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "def exp_path(kind=\"results\"):\n",
        "    p = PROJECT_ROOT / kind\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def log_result(row: dict, csv_name=\"experiments_log.csv\"):\n",
        "    csv_file = exp_path(\"results\") / csv_name\n",
        "    df = pd.DataFrame([row])\n",
        "    if csv_file.exists():\n",
        "        df0 = pd.read_csv(csv_file); df = pd.concat([df0, df], ignore_index=True)\n",
        "    df.to_csv(csv_file, index=False)\n",
        "    return str(csv_file)\n",
        "\n",
        "print(\"Ready. Torch:\", torch.__version__, \"| Device:\", DEVICE, \"| Root:\", PROJECT_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugwlz1C-d2P9"
      },
      "outputs": [],
      "source": [
        "# === PHASE 3 COMMONS === rerun this too\n",
        "import os, json, math, time\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "set_seed(42)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/FFNN_Healing_Thesis\")\n",
        "FIG_DIR = PROJECT_ROOT / \"figures\"\n",
        "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
        "RES_DIR = PROJECT_ROOT / \"results\"\n",
        "for d in [FIG_DIR, MODEL_DIR, RES_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, X_path, y_path, task):\n",
        "        self.X = np.load(X_path).astype(np.float32)\n",
        "        self.y = np.load(y_path)\n",
        "        self.task = task\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i):\n",
        "        x = torch.from_numpy(self.X[i])\n",
        "        if self.task == \"clf\":\n",
        "            y = torch.tensor(int(self.y[i]), dtype=torch.long)\n",
        "        else:\n",
        "            y = torch.tensor(float(self.y[i]), dtype=torch.float32).unsqueeze(0)\n",
        "        return x, y\n",
        "\n",
        "def make_loaders(data_dir, task, batch_size=128):\n",
        "    data_dir = Path(data_dir)\n",
        "    ds_train = NumpyDataset(data_dir/\"X_train.npy\", data_dir/\"y_train.npy\", task)\n",
        "    ds_val   = NumpyDataset(data_dir/\"X_val.npy\",   data_dir/\"y_val.npy\",   task)\n",
        "    ds_test  = NumpyDataset(data_dir/\"X_test.npy\",  data_dir/\"y_test.npy\",  task)\n",
        "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    val_loader   = DataLoader(ds_val,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "    test_loader  = DataLoader(ds_test,  batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "    n_features = ds_train.X.shape[1]\n",
        "    return train_loader, val_loader, test_loader, n_features\n",
        "\n",
        "def init_kaiming(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "class FFNN_Classifier(nn.Module):\n",
        "    def __init__(self, n_in, hidden=[64, 64, 32], n_out=2, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, prev = [], n_in\n",
        "        for h in hidden:\n",
        "            layers += [nn.Linear(prev, h), nn.ReLU(), nn.Dropout(p_drop)]\n",
        "            prev = h\n",
        "        layers += [nn.Linear(prev, n_out)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.apply(init_kaiming)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class FFNN_Regression(nn.Module):\n",
        "    def __init__(self, n_in, hidden=[64, 64, 32], p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, prev = [], n_in\n",
        "        for h in hidden:\n",
        "            layers += [nn.Linear(prev, h), nn.ReLU(), nn.Dropout(p_drop)]\n",
        "            prev = h\n",
        "        layers += [nn.Linear(prev, 1)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.apply(init_kaiming)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, task, max_epochs=200, lr=1e-3, weight_decay=1e-4, patience=20, run_name=\"run\"):\n",
        "    model = model.to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss() if task==\"clf\" else nn.MSELoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5, verbose=False)\n",
        "\n",
        "    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"val_metric\": []}\n",
        "    best_val, best_path, epochs_no_improve = float(\"inf\"), MODEL_DIR / f\"{run_name}_{timestamp()}_best.pt\", 0\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        model.train(); train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb if task==\"clf\" else yb)\n",
        "            loss.backward(); opt.step()\n",
        "            train_losses.append(loss.item())\n",
        "        train_loss = float(np.mean(train_losses)) if train_losses else 0.0\n",
        "\n",
        "        model.eval(); val_losses = []; y_true_list, y_pred_list = [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                out = model(xb)\n",
        "                loss = criterion(out, yb if task==\"clf\" else yb)\n",
        "                val_losses.append(loss.item())\n",
        "                if task == \"clf\":\n",
        "                    preds = out.argmax(dim=1).detach().cpu().numpy()\n",
        "                    y_true_list.append(yb.detach().cpu().numpy())\n",
        "                    y_pred_list.append(preds)\n",
        "                else:\n",
        "                    y_true_list.append(yb.detach().cpu().numpy().squeeze())\n",
        "                    y_pred_list.append(out.detach().cpu().numpy().squeeze())\n",
        "        val_loss = float(np.mean(val_losses)) if val_losses else 0.0\n",
        "\n",
        "        if task == \"clf\":\n",
        "            y_true = np.concatenate(y_true_list); y_pred = np.concatenate(y_pred_list)\n",
        "            val_metric = float(accuracy_score(y_true, y_pred))\n",
        "        else:\n",
        "            y_true = np.array(np.concatenate([np.atleast_1d(a) for a in y_true_list]))\n",
        "            y_pred = np.array(np.concatenate([np.atleast_1d(a) for a in y_pred_list]))\n",
        "            val_metric = float(math.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history[\"epoch\"].append(epoch); history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss); history[\"val_metric\"].append(val_metric)\n",
        "\n",
        "        if val_loss < best_val - 1e-8:\n",
        "            best_val = val_loss; epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), best_path); best_mark=\"*\"\n",
        "        else:\n",
        "            epochs_no_improve += 1; best_mark=\"\"\n",
        "        if epoch % 10 == 0 or best_mark == \"*\":\n",
        "            print(f\"[{run_name}] epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | metric {val_metric:.4f} {best_mark}\")\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"[{run_name}] Early stopping at epoch {epoch}. Best val_loss={best_val:.4f}\")\n",
        "            break\n",
        "    return best_path, history\n",
        "\n",
        "def evaluate_model(model, loader, task):\n",
        "    model.eval(); y_true_list, y_pred_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            out = model(xb)\n",
        "            if task == \"clf\":\n",
        "                preds = out.argmax(dim=1)\n",
        "                y_true_list.append(yb.cpu().numpy()); y_pred_list.append(preds.cpu().numpy())\n",
        "            else:\n",
        "                y_true_list.append(yb.cpu().numpy().squeeze()); y_pred_list.append(out.cpu().numpy().squeeze())\n",
        "    if task == \"clf\":\n",
        "        y_true = np.concatenate(y_true_list); y_pred = np.concatenate(y_pred_list)\n",
        "        return {\"accuracy\": float(accuracy_score(y_true, y_pred)), \"f1\": float(f1_score(y_true, y_pred))}\n",
        "    else:\n",
        "        y_true = np.array(np.concatenate([np.atleast_1d(a) for a in y_true_list]))\n",
        "        y_pred = np.array(np.concatenate([np.atleast_1d(a) for a in y_pred_list]))\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        return {\"mae\": float(mae), \"rmse\": float(rmse)}\n",
        "\n",
        "def plot_history(history, run_name, task):\n",
        "    fig = plt.figure(figsize=(6,4))\n",
        "    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train_loss\")\n",
        "    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.title(f\"{run_name} loss\")\n",
        "    fig_path1 = FIG_DIR / f\"{run_name}_loss.png\"\n",
        "    plt.savefig(fig_path1, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "    fig = plt.figure(figsize=(6,4))\n",
        "    ylabel = \"accuracy\" if task==\"clf\" else \"RMSE\"\n",
        "    plt.plot(history[\"epoch\"], history[\"val_metric\"], label=f\"val_{ylabel}\")\n",
        "    plt.xlabel(\"epoch\"); plt.ylabel(ylabel); plt.legend(); plt.title(f\"{run_name} {ylabel}\")\n",
        "    fig_path2 = FIG_DIR / f\"{run_name}_{ylabel}.png\"\n",
        "    plt.savefig(fig_path2, bbox_inches=\"tight\"); plt.close()\n",
        "    return str(fig_path1), str(fig_path2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgNzxzUKeYaA",
        "outputId": "d912bf95-ce56-4c0b-f185-82baeebc408f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_features: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[clf_breast_cancer_baseline] epoch 001 | train 0.6450 | val 0.4289 | metric 0.9059 *\n",
            "[clf_breast_cancer_baseline] epoch 002 | train 0.4461 | val 0.3217 | metric 0.9412 *\n",
            "[clf_breast_cancer_baseline] epoch 003 | train 0.3843 | val 0.2555 | metric 0.9529 *\n",
            "[clf_breast_cancer_baseline] epoch 004 | train 0.3345 | val 0.2047 | metric 0.9529 *\n",
            "[clf_breast_cancer_baseline] epoch 005 | train 0.2393 | val 0.1680 | metric 0.9529 *\n",
            "[clf_breast_cancer_baseline] epoch 006 | train 0.2416 | val 0.1412 | metric 0.9529 *\n",
            "[clf_breast_cancer_baseline] epoch 007 | train 0.2242 | val 0.1216 | metric 0.9529 *\n",
            "[clf_breast_cancer_baseline] epoch 008 | train 0.1688 | val 0.1073 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 009 | train 0.1800 | val 0.0971 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 010 | train 0.1485 | val 0.0919 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 011 | train 0.1821 | val 0.0899 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 013 | train 0.1138 | val 0.0886 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 014 | train 0.1093 | val 0.0839 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 015 | train 0.1058 | val 0.0779 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 016 | train 0.0810 | val 0.0738 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 017 | train 0.0979 | val 0.0701 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 018 | train 0.1939 | val 0.0673 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 019 | train 0.1503 | val 0.0664 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 020 | train 0.0725 | val 0.0682 | metric 0.9647 \n",
            "[clf_breast_cancer_baseline] epoch 023 | train 0.1071 | val 0.0639 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 024 | train 0.0617 | val 0.0611 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 025 | train 0.0629 | val 0.0589 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 026 | train 0.0670 | val 0.0583 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 030 | train 0.0589 | val 0.0619 | metric 0.9647 \n",
            "[clf_breast_cancer_baseline] epoch 035 | train 0.0616 | val 0.0580 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 036 | train 0.0436 | val 0.0579 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 037 | train 0.0557 | val 0.0578 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 039 | train 0.0587 | val 0.0577 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 040 | train 0.0612 | val 0.0575 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 041 | train 0.1032 | val 0.0569 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 050 | train 0.0584 | val 0.0630 | metric 0.9765 \n",
            "[clf_breast_cancer_baseline] epoch 060 | train 0.0440 | val 0.0583 | metric 0.9765 \n",
            "[clf_breast_cancer_baseline] Early stopping at epoch 61. Best val_loss=0.0569\n",
            "TEST metrics: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4073422324.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved plots: /content/drive/MyDrive/FFNN_Healing_Thesis/figures/clf_breast_cancer_baseline_loss.png | /content/drive/MyDrive/FFNN_Healing_Thesis/figures/clf_breast_cancer_baseline_accuracy.png\n",
            "Best model path: /content/drive/MyDrive/FFNN_Healing_Thesis/models/clf_breast_cancer_baseline_20250812-085243_best.pt\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/FFNN_Healing_Thesis/data/breast_cancer\"\n",
        "train_loader, val_loader, test_loader, n_features = make_loaders(DATA_DIR, task=\"clf\", batch_size=128)\n",
        "print(\"n_features:\", n_features)\n",
        "\n",
        "model_clf = FFNN_Classifier(n_in=n_features, hidden=[64,64,32], n_out=2, p_drop=0.1)\n",
        "run_name = \"clf_breast_cancer_baseline\"\n",
        "\n",
        "best_path, hist = train_model(\n",
        "    model_clf, train_loader, val_loader, task=\"clf\",\n",
        "    max_epochs=200, lr=1e-3, weight_decay=1e-4, patience=20, run_name=run_name\n",
        ")\n",
        "\n",
        "best_model = FFNN_Classifier(n_in=n_features, hidden=[64,64,32], n_out=2, p_drop=0.1).to(DEVICE)\n",
        "best_model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
        "metrics_test = evaluate_model(best_model, test_loader, task=\"clf\")\n",
        "print(\"TEST metrics:\", metrics_test)\n",
        "\n",
        "fig_loss, fig_metric = plot_history(hist, run_name, task=\"clf\")\n",
        "with open(RES_DIR / f\"{run_name}_{timestamp()}_metrics.json\", \"w\") as f:\n",
        "    json.dump({\"test\": metrics_test}, f, indent=2)\n",
        "\n",
        "log_result({\n",
        "    \"timestamp\": timestamp(),\n",
        "    \"phase\": \"baseline\",\n",
        "    \"dataset\": \"breast_cancer\",\n",
        "    \"task\": \"clf\",\n",
        "    \"model\": \"FFNN_Classifier[64,64,32]\",\n",
        "    \"test_accuracy\": metrics_test[\"accuracy\"],\n",
        "    \"test_f1\": metrics_test[\"f1\"],\n",
        "    \"best_ckpt\": str(best_path),\n",
        "    \"fig_loss\": fig_loss,\n",
        "    \"fig_metric\": fig_metric\n",
        "})\n",
        "print(\"Saved plots:\", fig_loss, \"|\", fig_metric)\n",
        "print(\"Best model path:\", best_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGPSf8qFe1Qd",
        "outputId": "f0d3c6fa-198f-4f5e-c262-ea72722cf0a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_features: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[reg_california_baseline] epoch 001 | train 1.5993 | val 0.7407 | metric 0.8434 *\n",
            "[reg_california_baseline] epoch 002 | train 0.9427 | val 0.5928 | metric 0.7548 *\n",
            "[reg_california_baseline] epoch 003 | train 0.7787 | val 0.5837 | metric 0.7416 *\n",
            "[reg_california_baseline] epoch 004 | train 0.7164 | val 0.5455 | metric 0.7256 *\n",
            "[reg_california_baseline] epoch 005 | train 0.6750 | val 0.5004 | metric 0.6962 *\n",
            "[reg_california_baseline] epoch 006 | train 0.6111 | val 0.4641 | metric 0.6724 *\n",
            "[reg_california_baseline] epoch 010 | train 0.5285 | val 0.4724 | metric 0.6762 \n",
            "[reg_california_baseline] epoch 011 | train 0.5073 | val 0.4414 | metric 0.6512 *\n",
            "[reg_california_baseline] epoch 014 | train 0.4751 | val 0.4292 | metric 0.6439 *\n",
            "[reg_california_baseline] epoch 015 | train 0.4632 | val 0.4009 | metric 0.6251 *\n",
            "[reg_california_baseline] epoch 016 | train 0.4568 | val 0.3972 | metric 0.6218 *\n",
            "[reg_california_baseline] epoch 018 | train 0.4391 | val 0.3969 | metric 0.6174 *\n",
            "[reg_california_baseline] epoch 020 | train 0.4420 | val 0.4090 | metric 0.6266 \n",
            "[reg_california_baseline] epoch 024 | train 0.4136 | val 0.3926 | metric 0.6118 *\n",
            "[reg_california_baseline] epoch 026 | train 0.4119 | val 0.3764 | metric 0.6049 *\n",
            "[reg_california_baseline] epoch 027 | train 0.4142 | val 0.3691 | metric 0.5973 *\n",
            "[reg_california_baseline] epoch 030 | train 0.3947 | val 0.3630 | metric 0.5904 *\n",
            "[reg_california_baseline] epoch 035 | train 0.3831 | val 0.3598 | metric 0.5921 *\n",
            "[reg_california_baseline] epoch 037 | train 0.3781 | val 0.3554 | metric 0.5820 *\n",
            "[reg_california_baseline] epoch 038 | train 0.3717 | val 0.3526 | metric 0.5859 *\n",
            "[reg_california_baseline] epoch 040 | train 0.3638 | val 0.3730 | metric 0.5991 \n",
            "[reg_california_baseline] epoch 042 | train 0.3675 | val 0.3429 | metric 0.5772 *\n",
            "[reg_california_baseline] epoch 050 | train 0.3490 | val 0.3451 | metric 0.5760 \n",
            "[reg_california_baseline] epoch 053 | train 0.3480 | val 0.3423 | metric 0.5694 *\n",
            "[reg_california_baseline] epoch 055 | train 0.3456 | val 0.3381 | metric 0.5712 *\n",
            "[reg_california_baseline] epoch 060 | train 0.3478 | val 0.3328 | metric 0.5651 *\n",
            "[reg_california_baseline] epoch 061 | train 0.3429 | val 0.3326 | metric 0.5667 *\n",
            "[reg_california_baseline] epoch 062 | train 0.3383 | val 0.3309 | metric 0.5641 *\n",
            "[reg_california_baseline] epoch 066 | train 0.3385 | val 0.3242 | metric 0.5621 *\n",
            "[reg_california_baseline] epoch 070 | train 0.3364 | val 0.3381 | metric 0.5680 \n",
            "[reg_california_baseline] epoch 078 | train 0.3275 | val 0.3230 | metric 0.5601 *\n",
            "[reg_california_baseline] epoch 080 | train 0.3290 | val 0.3209 | metric 0.5582 *\n",
            "[reg_california_baseline] epoch 090 | train 0.3281 | val 0.3235 | metric 0.5604 \n",
            "[reg_california_baseline] epoch 096 | train 0.3231 | val 0.3209 | metric 0.5581 *\n",
            "[reg_california_baseline] epoch 100 | train 0.3235 | val 0.3227 | metric 0.5587 \n",
            "[reg_california_baseline] epoch 110 | train 0.3242 | val 0.3245 | metric 0.5596 \n",
            "[reg_california_baseline] epoch 120 | train 0.3206 | val 0.3245 | metric 0.5593 \n",
            "[reg_california_baseline] Early stopping at epoch 121. Best val_loss=0.3209\n",
            "TEST metrics: {'mae': 0.36256006360054016, 'rmse': 0.5222732064061352}\n",
            "Saved plots: /content/drive/MyDrive/FFNN_Healing_Thesis/figures/reg_california_baseline_loss.png  |  /content/drive/MyDrive/FFNN_Healing_Thesis/figures/reg_california_baseline_RMSE.png\n",
            "Best model path: /content/drive/MyDrive/FFNN_Healing_Thesis/models/reg_california_baseline_20250812-085508_best.pt\n"
          ]
        }
      ],
      "source": [
        "# === REGRESSION BASELINE (California Housing) ===\n",
        "DATA_DIR = \"/content/drive/MyDrive/FFNN_Healing_Thesis/data/california_housing\"\n",
        "train_loader, val_loader, test_loader, n_features = make_loaders(DATA_DIR, task=\"reg\", batch_size=256)\n",
        "print(\"n_features:\", n_features)\n",
        "\n",
        "model_reg = FFNN_Regression(n_in=n_features, hidden=[128,64,32], p_drop=0.1)\n",
        "run_name = \"reg_california_baseline\"\n",
        "\n",
        "best_path, hist = train_model(\n",
        "    model_reg, train_loader, val_loader, task=\"reg\",\n",
        "    max_epochs=250, lr=1e-3, weight_decay=1e-4, patience=25, run_name=run_name\n",
        ")\n",
        "\n",
        "# Load best checkpoint safely (silences the FutureWarning on torch.load)\n",
        "best_model = FFNN_Regression(n_in=n_features, hidden=[128,64,32], p_drop=0.1).to(DEVICE)\n",
        "try:\n",
        "    state = torch.load(best_path, map_location=DEVICE, weights_only=True)\n",
        "except TypeError:\n",
        "    # Fallback for older torch that doesn't support weights_only\n",
        "    state = torch.load(best_path, map_location=DEVICE)\n",
        "best_model.load_state_dict(state)\n",
        "\n",
        "metrics_test = evaluate_model(best_model, test_loader, task=\"reg\")\n",
        "print(\"TEST metrics:\", metrics_test)  # MAE and RMSE\n",
        "\n",
        "fig_loss, fig_metric = plot_history(hist, run_name, task=\"reg\")\n",
        "with open(RES_DIR / f\"{run_name}_{timestamp()}_metrics.json\", \"w\") as f:\n",
        "    json.dump({\"test\": metrics_test}, f, indent=2)\n",
        "\n",
        "log_result({\n",
        "    \"timestamp\": timestamp(),\n",
        "    \"phase\": \"baseline\",\n",
        "    \"dataset\": \"california_housing\",\n",
        "    \"task\": \"reg\",\n",
        "    \"model\": \"FFNN_Regression[128,64,32]\",\n",
        "    \"test_mae\": metrics_test[\"mae\"],\n",
        "    \"test_rmse\": metrics_test[\"rmse\"],\n",
        "    \"best_ckpt\": str(best_path),\n",
        "    \"fig_loss\": fig_loss,\n",
        "    \"fig_metric\": fig_metric\n",
        "})\n",
        "print(\"Saved plots:\", fig_loss, \" | \", fig_metric)\n",
        "print(\"Best model path:\", best_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK9Gxp8-fZej"
      },
      "outputs": [],
      "source": [
        "#rerun\n",
        "# === PHASE 4: DAMAGE ENGINE ===\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "\n",
        "# Re-use: DEVICE, set_seed, log_result, evaluate_model, make_loaders, timestamp, PROJECT_ROOT, MODEL_DIR already defined\n",
        "\n",
        "def get_linear_layers(model: nn.Module):\n",
        "    \"\"\"Return list of (idx, module) for nn.Linear layers in forward order.\"\"\"\n",
        "    layers = []\n",
        "    for i, m in enumerate(model.net):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            layers.append((i, m))\n",
        "    return layers\n",
        "\n",
        "def summarize_linear_layers(model):\n",
        "    info = []\n",
        "    for li, lin in get_linear_layers(model):\n",
        "        info.append({\"seq_index\": li, \"in_features\": lin.in_features, \"out_features\": lin.out_features})\n",
        "    return info\n",
        "\n",
        "def clone_from_checkpoint(model_ctor, ckpt_path: str):\n",
        "    m = model_ctor().to(DEVICE)\n",
        "    state = None\n",
        "    try:\n",
        "        state = torch.load(ckpt_path, map_location=DEVICE, weights_only=True)\n",
        "    except TypeError:\n",
        "        state = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    m.load_state_dict(state)\n",
        "    return m\n",
        "\n",
        "def damage_neurons_in_layer(model: nn.Module, layer_lin_seq_index: int, pct_neurons: float, rng: np.random.Generator):\n",
        "    \"\"\"\n",
        "    Damage a percentage of *neurons* in a given Linear layer:\n",
        "    - Zero the corresponding rows of this layer's weight (incoming weights to that neuron) and its bias.\n",
        "    - Also zero the corresponding *columns* in the *next* Linear layer (outgoing connections from this neuron),\n",
        "      if a next Linear layer exists.\n",
        "    Returns list of damaged neuron indices.\n",
        "    \"\"\"\n",
        "    # locate target Linear layer and next Linear layer (if any)\n",
        "    linear_layers = get_linear_layers(model)\n",
        "    # map from sequential linear index order to the actual module index in model.net\n",
        "    target_idx = None\n",
        "    next_idx = None\n",
        "    for k, (seq_i, lin) in enumerate(linear_layers):\n",
        "        if seq_i == layer_lin_seq_index:\n",
        "            target_idx = k\n",
        "            break\n",
        "    assert target_idx is not None, f\"No Linear layer at seq index {layer_lin_seq_index}\"\n",
        "\n",
        "    target_seq_i, target_lin = linear_layers[target_idx]\n",
        "    out_feats = target_lin.out_features\n",
        "    n_dmg = max(1, int(round(pct_neurons * out_feats)))\n",
        "    dmg_neurons = rng.choice(out_feats, size=n_dmg, replace=False)\n",
        "\n",
        "    # Zero rows (incoming weights to damaged neuron)\n",
        "    with torch.no_grad():\n",
        "        W = target_lin.weight  # shape [out_features, in_features]\n",
        "        W[dmg_neurons, :] = 0.0\n",
        "        if target_lin.bias is not None:\n",
        "            target_lin.bias[dmg_neurons] = 0.0\n",
        "\n",
        "    # Zero columns in the *next* linear layer weight (outgoing from damaged neurons)\n",
        "    if target_idx + 1 < len(linear_layers):\n",
        "        _, next_lin = linear_layers[target_idx + 1]\n",
        "        with torch.no_grad():\n",
        "            # next_lin.weight: [next_out, next_in] ; next_in == out_feats of target_lin\n",
        "            next_lin.weight[:, dmg_neurons] = 0.0\n",
        "\n",
        "    return dmg_neurons.tolist()\n",
        "\n",
        "def damage_weights_in_layer(model: nn.Module, layer_lin_seq_index: int, pct_weights: float, rng: np.random.Generator, mode=\"random\"):\n",
        "    \"\"\"\n",
        "    Damage a percentage of *weights* in a given Linear layer:\n",
        "    - mode='random': independent random mask over all weights\n",
        "    - mode='block': zero a contiguous rectangular block (for 'specific area' effect)\n",
        "    Returns number of weights zeroed.\n",
        "    \"\"\"\n",
        "    linear_layers = get_linear_layers(model)\n",
        "    target_lin = None\n",
        "    for seq_i, lin in linear_layers:\n",
        "        if seq_i == layer_lin_seq_index:\n",
        "            target_lin = lin\n",
        "            break\n",
        "    assert target_lin is not None, f\"No Linear layer at seq index {layer_lin_seq_index}\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        W = target_lin.weight  # [out_features, in_features]\n",
        "        H, K = W.shape\n",
        "        total = H * K\n",
        "        n_dmg = max(1, int(round(pct_weights * total)))\n",
        "\n",
        "        if mode == \"random\":\n",
        "            idx = rng.choice(total, size=n_dmg, replace=False)\n",
        "            rows = (idx // K).astype(int)\n",
        "            cols = (idx % K).astype(int)\n",
        "            W[rows, cols] = 0.0\n",
        "        elif mode == \"block\":\n",
        "            # choose a block whose area ≈ n_dmg\n",
        "            h = max(1, int(round(np.sqrt(n_dmg))))\n",
        "            k = max(1, int(round(n_dmg / h)))\n",
        "            r0 = int(rng.integers(0, max(1, H - h + 1)))\n",
        "            c0 = int(rng.integers(0, max(1, K - k + 1)))\n",
        "            W[r0:r0+h, c0:c0+k] = 0.0\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'random' or 'block'\")\n",
        "\n",
        "    return n_dmg\n",
        "\n",
        "def eval_task(model_ctor, ckpt_path, task, data_dir, damage_fn=None, repeats=1, **damage_kwargs):\n",
        "    \"\"\"\n",
        "    Load a fresh model from checkpoint, optionally apply damage (repeated, accumulating), and evaluate.\n",
        "    Returns list of dicts for each repeat step (including 0=baseline if repeats>0).\n",
        "    \"\"\"\n",
        "    set_seed(42)\n",
        "    if task == \"clf\":\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"clf\", batch_size=256)\n",
        "    else:\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"reg\", batch_size=512)\n",
        "\n",
        "    # baseline evaluation\n",
        "    model = clone_from_checkpoint(model_ctor, ckpt_path)\n",
        "    base_metrics = evaluate_model(model, test_loader, task)\n",
        "    out = []\n",
        "    out.append({\"repeat\": 0, \"metrics\": base_metrics})\n",
        "\n",
        "    if damage_fn is None or repeats <= 0:\n",
        "        return out\n",
        "\n",
        "    rng = np.random.default_rng(12345)\n",
        "    # Progressive damage: cumulative within this run\n",
        "    for r in range(1, repeats+1):\n",
        "        damage_fn(model, **damage_kwargs, rng=rng)\n",
        "        m = evaluate_model(model, test_loader, task)\n",
        "        out.append({\"repeat\": r, \"metrics\": m})\n",
        "    return out\n",
        "\n",
        "def save_experiment_log(rows, csv_name=\"damage_immediate_drop.csv\"):\n",
        "    # flatten list of dicts and append to results CSV\n",
        "    for row in rows:\n",
        "        log_result(row, csv_name=csv_name)\n",
        "    return str((PROJECT_ROOT / \"results\" / csv_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi2TA-LEgHBi",
        "outputId": "925b91f1-fa9a-499e-9c99-c3b0c52cd584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier linear layers: [{'seq_index': 0, 'in_features': 30, 'out_features': 64}, {'seq_index': 3, 'in_features': 64, 'out_features': 64}, {'seq_index': 6, 'in_features': 64, 'out_features': 32}, {'seq_index': 9, 'in_features': 32, 'out_features': 2}]\n",
            "Regression linear layers: [{'seq_index': 0, 'in_features': 8, 'out_features': 128}, {'seq_index': 3, 'in_features': 128, 'out_features': 64}, {'seq_index': 6, 'in_features': 64, 'out_features': 32}, {'seq_index': 9, 'in_features': 32, 'out_features': 1}]\n"
          ]
        }
      ],
      "source": [
        "#rerun\n",
        "# === CONFIG: checkpoint paths (use your printed paths) ===\n",
        "CLF_CKPT = \"/content/drive/MyDrive/FFNN_Healing_Thesis/models/clf_breast_cancer_baseline_20250812-085243_best.pt\"\n",
        "REG_CKPT = \"/content/drive/MyDrive/FFNN_Healing_Thesis/models/reg_california_baseline_20250812-085508_best.pt\"\n",
        "\n",
        "# Recreate model-ctor lambdas with correct input sizes:\n",
        "# (We’ll detect n_features from the saved arrays)\n",
        "import numpy as np, json\n",
        "\n",
        "# classification\n",
        "clf_dir = \"/content/drive/MyDrive/FFNN_Healing_Thesis/data/breast_cancer\"\n",
        "n_in_clf = np.load(f\"{clf_dir}/X_train.npy\").shape[1]\n",
        "clf_ctor = lambda: FFNN_Classifier(n_in=n_in_clf, hidden=[64,64,32], n_out=2, p_drop=0.0)\n",
        "\n",
        "# regression\n",
        "reg_dir = \"/content/drive/MyDrive/FFNN_Healing_Thesis/data/california_housing\"\n",
        "n_in_reg = np.load(f\"{reg_dir}/X_train.npy\").shape[1]\n",
        "reg_ctor = lambda: FFNN_Regression(n_in=n_in_reg, hidden=[128,64,32], p_drop=0.0)\n",
        "\n",
        "# Inspect linear layers to know which seq indices are Linear:\n",
        "tmp_model = clf_ctor().to(DEVICE)\n",
        "print(\"Classifier linear layers:\", summarize_linear_layers(tmp_model))\n",
        "tmp_model = reg_ctor().to(DEVICE)\n",
        "print(\"Regression linear layers:\", summarize_linear_layers(tmp_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWUd9KR5hJDN",
        "outputId": "e900a15f-3b16-4bd2-caa4-5f8f775ea836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repeat 0: ACC=0.9535, F1=0.9630\n",
            "Repeat 1: ACC=0.9419, F1=0.9550\n",
            "Repeat 2: ACC=0.9186, F1=0.9346\n",
            "Repeat 3: ACC=0.8721, F1=0.8952\n",
            "Repeat 4: ACC=0.8721, F1=0.8932\n",
            "Repeat 5: ACC=0.8721, F1=0.8991\n",
            "Logged to: /content/drive/MyDrive/FFNN_Healing_Thesis/results/damage_immediate_drop.csv\n"
          ]
        }
      ],
      "source": [
        "#rerun\n",
        "# === 4.3 SANITY EXPERIMENT: classification, 20% neurons in first hidden, 5 repeats ===\n",
        "set_seed(42)\n",
        "\n",
        "# first Linear layer's seq index (from your summary it's 0)\n",
        "FIRST_LINEAR = summarize_linear_layers(clone_from_checkpoint(clf_ctor, CLF_CKPT))[0][\"seq_index\"]\n",
        "\n",
        "rows_to_log = []\n",
        "res = eval_task(\n",
        "    model_ctor=clf_ctor,\n",
        "    ckpt_path=CLF_CKPT,\n",
        "    task=\"clf\",\n",
        "    data_dir=clf_dir,\n",
        "    damage_fn=damage_neurons_in_layer,\n",
        "    repeats=5,                 # progressive damage\n",
        "    layer_lin_seq_index=FIRST_LINEAR,\n",
        "    pct_neurons=0.20           # 20% neurons\n",
        ")\n",
        "\n",
        "for step in res:\n",
        "    r = step[\"repeat\"]; m = step[\"metrics\"]\n",
        "    print(f\"Repeat {r}: ACC={m['accuracy']:.4f}, F1={m['f1']:.4f}\")\n",
        "    rows_to_log.append({\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"damage_immediate\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"damage_type\": \"neurons\",\n",
        "        \"layer_seq_index\": FIRST_LINEAR,\n",
        "        \"pct\": 0.20,\n",
        "        \"repeat\": r,\n",
        "        \"test_accuracy\": m[\"accuracy\"],\n",
        "        \"test_f1\": m[\"f1\"]\n",
        "    })\n",
        "\n",
        "csv_path = save_experiment_log(rows_to_log, \"damage_immediate_drop.csv\")\n",
        "print(\"Logged to:\", csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Go_3K0XhoMJ",
        "outputId": "957124f5-b02c-4c19-e2b9-b6fd0ac7d72f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log path exists: True\n",
            "Last 8 rows:\n",
            "         timestamp             phase        dataset task damage_type  \\\n",
            "0  20250812-090453  damage_immediate  breast_cancer  clf     neurons   \n",
            "1  20250812-090453  damage_immediate  breast_cancer  clf     neurons   \n",
            "2  20250812-090453  damage_immediate  breast_cancer  clf     neurons   \n",
            "3  20250812-090453  damage_immediate  breast_cancer  clf     neurons   \n",
            "4  20250812-090453  damage_immediate  breast_cancer  clf     neurons   \n",
            "5  20250812-090453  damage_immediate  breast_cancer  clf     neurons   \n",
            "\n",
            "   layer_seq_index  pct  repeat  test_accuracy   test_f1  \n",
            "0                0  0.2       0       0.953488  0.962963  \n",
            "1                0  0.2       1       0.941860  0.954955  \n",
            "2                0  0.2       2       0.918605  0.934579  \n",
            "3                0  0.2       3       0.872093  0.895238  \n",
            "4                0  0.2       4       0.872093  0.893204  \n",
            "5                0  0.2       5       0.872093  0.899083  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, pathlib\n",
        "csv = pathlib.Path(\"/content/drive/MyDrive/FFNN_Healing_Thesis/results/damage_immediate_drop.csv\")\n",
        "print(\"Log path exists:\", csv.exists())\n",
        "df = pd.read_csv(csv)\n",
        "print(\"Last 8 rows:\")\n",
        "print(df.tail(8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHo946-uiHV_",
        "outputId": "7f6fca40-179c-461a-f09a-340b8c9c769c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repeat 0: MAE=0.3626, RMSE=0.5223\n",
            "Repeat 1: MAE=0.3980, RMSE=0.5500\n",
            "Repeat 2: MAE=0.4254, RMSE=0.5833\n",
            "Repeat 3: MAE=0.5298, RMSE=0.7730\n",
            "Repeat 4: MAE=0.6466, RMSE=0.9256\n",
            "Repeat 5: MAE=0.7139, RMSE=1.0377\n",
            "Logged to: /content/drive/MyDrive/FFNN_Healing_Thesis/results/damage_immediate_drop.csv\n"
          ]
        }
      ],
      "source": [
        "# === SANITY: regression, 20% neurons in first hidden, 5 repeats ===\n",
        "set_seed(42)\n",
        "\n",
        "FIRST_LINEAR_REG = summarize_linear_layers(clone_from_checkpoint(reg_ctor, REG_CKPT))[0][\"seq_index\"]\n",
        "\n",
        "rows_to_log = []\n",
        "res = eval_task(\n",
        "    model_ctor=reg_ctor,\n",
        "    ckpt_path=REG_CKPT,\n",
        "    task=\"reg\",\n",
        "    data_dir=reg_dir,\n",
        "    damage_fn=damage_neurons_in_layer,\n",
        "    repeats=5,\n",
        "    layer_lin_seq_index=FIRST_LINEAR_REG,\n",
        "    pct_neurons=0.20\n",
        ")\n",
        "\n",
        "for step in res:\n",
        "    r = step[\"repeat\"]; m = step[\"metrics\"]\n",
        "    print(f\"Repeat {r}: MAE={m['mae']:.4f}, RMSE={m['rmse']:.4f}\")\n",
        "    rows_to_log.append({\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"damage_immediate\",\n",
        "        \"dataset\": \"california_housing\",\n",
        "        \"task\": \"reg\",\n",
        "        \"damage_type\": \"neurons\",\n",
        "        \"layer_seq_index\": FIRST_LINEAR_REG,\n",
        "        \"pct\": 0.20,\n",
        "        \"repeat\": r,\n",
        "        \"test_mae\": m[\"mae\"],\n",
        "        \"test_rmse\": m[\"rmse\"]\n",
        "    })\n",
        "\n",
        "csv_path = save_experiment_log(rows_to_log, \"damage_immediate_drop.csv\")\n",
        "print(\"Logged to:\", csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbFz8O9Bildr"
      },
      "outputs": [],
      "source": [
        "#rerun\n",
        "# === 4.4 EXPERIMENT RUNNER (logs tidy rows for each repeat) ===\n",
        "from copy import deepcopy\n",
        "\n",
        "def layer_seq_indices(model_ctor):\n",
        "    m = model_ctor().to(DEVICE)\n",
        "    return [d[\"seq_index\"] for d in summarize_linear_layers(m)]\n",
        "\n",
        "def run_damage_experiment(\n",
        "    model_ctor, ckpt_path, task, data_dir,\n",
        "    layer_seq_index, pct, repeats_list,\n",
        "    damage_type=\"neurons\", weight_mode=\"random\",\n",
        "    rng_seed=12345, csv_name=\"damage_immediate_drop.csv\"\n",
        "):\n",
        "    \"\"\"\n",
        "    For each repeats in repeats_list:\n",
        "      - reload clean checkpoint\n",
        "      - apply progressive damage (repeats times) on given layer\n",
        "      - evaluate after each repeat\n",
        "      - log baseline + each step, with deltas vs baseline\n",
        "    \"\"\"\n",
        "    assert damage_type in (\"neurons\",\"weights\")\n",
        "    rows = []\n",
        "    for repeats in repeats_list:\n",
        "        # run a fresh progressive-damage sequence starting from the clean checkpoint\n",
        "        if damage_type == \"neurons\":\n",
        "            res = eval_task(\n",
        "                model_ctor=model_ctor, ckpt_path=ckpt_path, task=task, data_dir=data_dir,\n",
        "                damage_fn=damage_neurons_in_layer, repeats=repeats,\n",
        "                layer_lin_seq_index=layer_seq_index, pct_neurons=pct\n",
        "            )\n",
        "        else:\n",
        "            res = eval_task(\n",
        "                model_ctor=model_ctor, ckpt_path=ckpt_path, task=task, data_dir=data_dir,\n",
        "                damage_fn=damage_weights_in_layer, repeats=repeats,\n",
        "                layer_lin_seq_index=layer_seq_index, pct_weights=pct, mode=weight_mode\n",
        "            )\n",
        "\n",
        "        # baseline metrics for delta\n",
        "        base = res[0][\"metrics\"]\n",
        "        for step in res:\n",
        "            r = step[\"repeat\"]; m = step[\"metrics\"]\n",
        "            row = {\n",
        "                \"timestamp\": timestamp(),\n",
        "                \"phase\": \"damage_immediate\",\n",
        "                \"task\": task,\n",
        "                \"damage_type\": damage_type,\n",
        "                \"layer_seq_index\": int(layer_seq_index),\n",
        "                \"pct\": float(pct),\n",
        "                \"repeats_total\": int(repeats),\n",
        "                \"repeat_eval\": int(r),\n",
        "                \"ckpt\": ckpt_path\n",
        "            }\n",
        "            if task == \"clf\":\n",
        "                row.update({\n",
        "                    \"metric\": \"accuracy\",\n",
        "                    \"baseline_metric\": float(base[\"accuracy\"]),\n",
        "                    \"value\": float(m[\"accuracy\"]),\n",
        "                    \"delta_from_baseline\": float(base[\"accuracy\"] - m[\"accuracy\"]),\n",
        "                    \"f1\": float(m[\"f1\"])\n",
        "                })\n",
        "            else:\n",
        "                row.update({\n",
        "                    \"metric\": \"rmse\",\n",
        "                    \"baseline_metric\": float(base[\"rmse\"]),\n",
        "                    \"value\": float(m[\"rmse\"]),\n",
        "                    \"delta_from_baseline\": float(m[\"rmse\"] - base[\"rmse\"]),\n",
        "                    \"mae\": float(m[\"mae\"])\n",
        "                })\n",
        "            rows.append(row)\n",
        "\n",
        "    # append to CSV\n",
        "    for row in rows:\n",
        "        log_result(row, csv_name=csv_name)\n",
        "    print(f\"Logged {len(rows)} rows to {PROJECT_ROOT/'results'/csv_name}\")\n",
        "    return rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sWmZ8FHjDxh",
        "outputId": "10614aa4-8364-4cb4-c038-bcc4f4503276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged 6 rows to /content/drive/MyDrive/FFNN_Healing_Thesis/results/damage_immediate_drop.csv\n",
            "          timestamp             phase dataset task damage_type  \\\n",
            "12  20250812-091235  damage_immediate     NaN  clf     neurons   \n",
            "13  20250812-091235  damage_immediate     NaN  clf     neurons   \n",
            "14  20250812-091235  damage_immediate     NaN  clf     neurons   \n",
            "15  20250812-091235  damage_immediate     NaN  clf     neurons   \n",
            "16  20250812-091235  damage_immediate     NaN  clf     neurons   \n",
            "17  20250812-091235  damage_immediate     NaN  clf     neurons   \n",
            "\n",
            "    layer_seq_index  pct  repeat  test_accuracy  test_f1  test_mae  test_rmse  \\\n",
            "12                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "13                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "14                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "15                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "16                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "17                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "\n",
            "    repeats_total  repeat_eval  \\\n",
            "12            5.0          0.0   \n",
            "13            5.0          1.0   \n",
            "14            5.0          2.0   \n",
            "15            5.0          3.0   \n",
            "16            5.0          4.0   \n",
            "17            5.0          5.0   \n",
            "\n",
            "                                                 ckpt    metric  \\\n",
            "12  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "13  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "14  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "15  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "16  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "17  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "\n",
            "    baseline_metric     value  delta_from_baseline        f1  \n",
            "12         0.953488  0.953488             0.000000  0.962963  \n",
            "13         0.953488  0.941860             0.011628  0.954955  \n",
            "14         0.953488  0.918605             0.034884  0.934579  \n",
            "15         0.953488  0.872093             0.081395  0.895238  \n",
            "16         0.953488  0.872093             0.081395  0.893204  \n",
            "17         0.953488  0.872093             0.081395  0.899083  \n"
          ]
        }
      ],
      "source": [
        "# CLASSIFICATION: first hidden layer (seq 0), 20% neurons, repeats = [5]\n",
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]  # should be 0\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=clf_dir,\n",
        "    layer_seq_index=first_lin_clf, pct=0.20, repeats_list=[5],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_immediate_drop.csv\"\n",
        ")\n",
        "\n",
        "# quick peek\n",
        "import pandas as pd\n",
        "df = pd.read_csv(PROJECT_ROOT/\"results/damage_immediate_drop.csv\")\n",
        "print(df.tail(6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf5EV8NVjZAH"
      },
      "outputs": [],
      "source": [
        "#rerun\n",
        "# === Patch: experiment runner with dataset_name + separate CSV ===\n",
        "def run_damage_experiment(\n",
        "    model_ctor, ckpt_path, task, data_dir,\n",
        "    layer_seq_index, pct, repeats_list,\n",
        "    damage_type=\"neurons\", weight_mode=\"random\",\n",
        "    rng_seed=12345, csv_name=\"damage_runs.csv\",\n",
        "    dataset_name=None\n",
        "):\n",
        "    \"\"\"\n",
        "    For each repeats in repeats_list:\n",
        "      - reload clean checkpoint\n",
        "      - apply progressive damage (repeats times) on given layer\n",
        "      - evaluate after each repeat\n",
        "      - log baseline + each step with deltas\n",
        "    \"\"\"\n",
        "    assert damage_type in (\"neurons\",\"weights\")\n",
        "    rows = []\n",
        "    for repeats in repeats_list:\n",
        "        if damage_type == \"neurons\":\n",
        "            res = eval_task(\n",
        "                model_ctor=model_ctor, ckpt_path=ckpt_path, task=task, data_dir=data_dir,\n",
        "                damage_fn=damage_neurons_in_layer, repeats=repeats,\n",
        "                layer_lin_seq_index=layer_seq_index, pct_neurons=pct\n",
        "            )\n",
        "        else:\n",
        "            res = eval_task(\n",
        "                model_ctor=model_ctor, ckpt_path=ckpt_path, task=task, data_dir=data_dir,\n",
        "                damage_fn=damage_weights_in_layer, repeats=repeats,\n",
        "                layer_lin_seq_index=layer_seq_index, pct_weights=pct, mode=weight_mode\n",
        "            )\n",
        "\n",
        "        base = res[0][\"metrics\"]\n",
        "        for step in res:\n",
        "            r = step[\"repeat\"]; m = step[\"metrics\"]\n",
        "            row = {\n",
        "                \"timestamp\": timestamp(),\n",
        "                \"phase\": \"damage_immediate\",\n",
        "                \"dataset\": dataset_name,\n",
        "                \"task\": task,\n",
        "                \"damage_type\": damage_type,\n",
        "                \"layer_seq_index\": int(layer_seq_index),\n",
        "                \"pct\": float(pct),\n",
        "                \"repeats_total\": int(repeats),\n",
        "                \"repeat_eval\": int(r),\n",
        "                \"ckpt\": ckpt_path\n",
        "            }\n",
        "            if task == \"clf\":\n",
        "                row.update({\n",
        "                    \"metric\": \"accuracy\",\n",
        "                    \"baseline_metric\": float(base[\"accuracy\"]),\n",
        "                    \"value\": float(m[\"accuracy\"]),\n",
        "                    \"delta_from_baseline\": float(base[\"accuracy\"] - m[\"accuracy\"]),\n",
        "                    \"f1\": float(m[\"f1\"])\n",
        "                })\n",
        "            else:\n",
        "                row.update({\n",
        "                    \"metric\": \"rmse\",\n",
        "                    \"baseline_metric\": float(base[\"rmse\"]),\n",
        "                    \"value\": float(m[\"rmse\"]),\n",
        "                    \"delta_from_baseline\": float(m[\"rmse\"] - base[\"rmse\"]),\n",
        "                    \"mae\": float(m[\"mae\"])\n",
        "                })\n",
        "            rows.append(row)\n",
        "\n",
        "    for row in rows:\n",
        "        log_result(row, csv_name=csv_name)\n",
        "    print(f\"Logged {len(rows)} rows to {PROJECT_ROOT/'results'/csv_name}\")\n",
        "    return rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf1iESRgkUfr",
        "outputId": "b0c7cd0b-1f91-4b42-c008-553e24a1aa69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged 6 rows to /content/drive/MyDrive/FFNN_Healing_Thesis/results/damage_runs.csv\n",
            "         timestamp             phase        dataset task damage_type  \\\n",
            "0  20250812-091732  damage_immediate  breast_cancer  clf     neurons   \n",
            "1  20250812-091732  damage_immediate  breast_cancer  clf     neurons   \n",
            "2  20250812-091732  damage_immediate  breast_cancer  clf     neurons   \n",
            "3  20250812-091732  damage_immediate  breast_cancer  clf     neurons   \n",
            "4  20250812-091732  damage_immediate  breast_cancer  clf     neurons   \n",
            "5  20250812-091732  damage_immediate  breast_cancer  clf     neurons   \n",
            "\n",
            "   layer_seq_index  pct  repeats_total  repeat_eval  \\\n",
            "0                0  0.2              5            0   \n",
            "1                0  0.2              5            1   \n",
            "2                0  0.2              5            2   \n",
            "3                0  0.2              5            3   \n",
            "4                0  0.2              5            4   \n",
            "5                0  0.2              5            5   \n",
            "\n",
            "                                                ckpt    metric  \\\n",
            "0  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "1  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "2  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "3  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "4  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "5  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "\n",
            "   baseline_metric     value  delta_from_baseline        f1  \n",
            "0         0.953488  0.953488             0.000000  0.962963  \n",
            "1         0.953488  0.941860             0.011628  0.954955  \n",
            "2         0.953488  0.918605             0.034884  0.934579  \n",
            "3         0.953488  0.872093             0.081395  0.895238  \n",
            "4         0.953488  0.872093             0.081395  0.893204  \n",
            "5         0.953488  0.872093             0.081395  0.899083  \n"
          ]
        }
      ],
      "source": [
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]  # seq index 0\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=clf_dir,\n",
        "    layer_seq_index=first_lin_clf, pct=0.20, repeats_list=[5],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\",\n",
        "    dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "df_new = pd.read_csv(PROJECT_ROOT/\"results/damage_runs.csv\")\n",
        "print(df_new.tail(6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o90w_UxTkhhf",
        "outputId": "26e3fa7b-a91a-40b3-b344-705a02029eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged 17 rows to /content/drive/MyDrive/FFNN_Healing_Thesis/results/damage_runs.csv\n",
            "Logged 17 rows to /content/drive/MyDrive/FFNN_Healing_Thesis/results/damage_runs.csv\n",
            "          timestamp             phase        dataset task damage_type  \\\n",
            "28  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "29  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "30  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "31  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "32  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "33  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "34  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "35  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "36  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "37  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "38  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "39  20250812-091904  damage_immediate  breast_cancer  clf     neurons   \n",
            "\n",
            "    layer_seq_index  pct  repeats_total  repeat_eval  \\\n",
            "28                0  0.4              5            5   \n",
            "29                0  0.4             10            0   \n",
            "30                0  0.4             10            1   \n",
            "31                0  0.4             10            2   \n",
            "32                0  0.4             10            3   \n",
            "33                0  0.4             10            4   \n",
            "34                0  0.4             10            5   \n",
            "35                0  0.4             10            6   \n",
            "36                0  0.4             10            7   \n",
            "37                0  0.4             10            8   \n",
            "38                0  0.4             10            9   \n",
            "39                0  0.4             10           10   \n",
            "\n",
            "                                                 ckpt    metric  \\\n",
            "28  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "29  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "30  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "31  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "32  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "33  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "34  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "35  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "36  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "37  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "38  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "39  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "\n",
            "    baseline_metric     value  delta_from_baseline        f1  \n",
            "28         0.953488  0.802326             0.151163  0.828283  \n",
            "29         0.953488  0.953488             0.000000  0.962963  \n",
            "30         0.953488  0.930233             0.023256  0.947368  \n",
            "31         0.953488  0.906977             0.046512  0.931034  \n",
            "32         0.953488  0.941860             0.011628  0.955752  \n",
            "33         0.953488  0.813953             0.139535  0.826087  \n",
            "34         0.953488  0.802326             0.151163  0.828283  \n",
            "35         0.953488  0.372093             0.581395  0.000000  \n",
            "36         0.953488  0.372093             0.581395  0.000000  \n",
            "37         0.953488  0.372093             0.581395  0.000000  \n",
            "38         0.953488  0.372093             0.581395  0.000000  \n",
            "39         0.953488  0.372093             0.581395  0.000000  \n"
          ]
        }
      ],
      "source": [
        "# === 4.5 SMALL BATCH: clf · first hidden · neurons · 20% & 40% · repeats [5, 10]\n",
        "set_seed(42)\n",
        "\n",
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]  # seq index 0\n",
        "\n",
        "# 20% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=clf_dir,\n",
        "    layer_seq_index=first_lin_clf, pct=0.20, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# 40% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=clf_dir,\n",
        "    layer_seq_index=first_lin_clf, pct=0.40, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# quick peek of just these rows\n",
        "import pandas as pd\n",
        "df = pd.read_csv(PROJECT_ROOT/\"results/damage_runs.csv\")\n",
        "mask = (df[\"dataset\"]==\"breast_cancer\") & (df[\"task\"]==\"clf\") & (df[\"damage_type\"]==\"neurons\") \\\n",
        "       & (df[\"layer_seq_index\"]==first_lin_clf) & (df[\"pct\"].isin([0.2, 0.4])) \\\n",
        "       & (df[\"repeats_total\"].isin([5,10]))\n",
        "print(df[mask].tail(12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWWhXjJWk3tx",
        "outputId": "0ebfddce-d86c-4b16-8a37-f23902448716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged 17 rows to /content/drive/MyDrive/FFNN_Healing_Thesis/results/damage_runs.csv\n",
            "Logged 17 rows to /content/drive/MyDrive/FFNN_Healing_Thesis/results/damage_runs.csv\n",
            "          timestamp             phase        dataset task damage_type  \\\n",
            "62  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "63  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "64  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "65  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "66  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "67  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "68  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "69  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "70  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "71  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "72  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "73  20250812-092048  damage_immediate  breast_cancer  clf     neurons   \n",
            "\n",
            "    layer_seq_index  pct  repeats_total  repeat_eval  \\\n",
            "62                9  0.4              5            5   \n",
            "63                9  0.4             10            0   \n",
            "64                9  0.4             10            1   \n",
            "65                9  0.4             10            2   \n",
            "66                9  0.4             10            3   \n",
            "67                9  0.4             10            4   \n",
            "68                9  0.4             10            5   \n",
            "69                9  0.4             10            6   \n",
            "70                9  0.4             10            7   \n",
            "71                9  0.4             10            8   \n",
            "72                9  0.4             10            9   \n",
            "73                9  0.4             10           10   \n",
            "\n",
            "                                                 ckpt    metric  \\\n",
            "62  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "63  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "64  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "65  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "66  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "67  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "68  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "69  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "70  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "71  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "72  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "73  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...  accuracy   \n",
            "\n",
            "    baseline_metric     value  delta_from_baseline        f1  \n",
            "62         0.953488  0.372093             0.581395  0.000000  \n",
            "63         0.953488  0.953488             0.000000  0.962963  \n",
            "64         0.953488  0.965116            -0.011628  0.972477  \n",
            "65         0.953488  0.372093             0.581395  0.000000  \n",
            "66         0.953488  0.372093             0.581395  0.000000  \n",
            "67         0.953488  0.372093             0.581395  0.000000  \n",
            "68         0.953488  0.372093             0.581395  0.000000  \n",
            "69         0.953488  0.372093             0.581395  0.000000  \n",
            "70         0.953488  0.372093             0.581395  0.000000  \n",
            "71         0.953488  0.372093             0.581395  0.000000  \n",
            "72         0.953488  0.372093             0.581395  0.000000  \n",
            "73         0.953488  0.372093             0.581395  0.000000  \n"
          ]
        }
      ],
      "source": [
        "# === 4.6: clf · OUTPUT layer · neurons · 20% & 40% · repeats [5, 10]\n",
        "set_seed(42)\n",
        "\n",
        "last_lin_clf = layer_seq_indices(clf_ctor)[-1]  # seq index of output Linear\n",
        "\n",
        "# 20% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=clf_dir,\n",
        "    layer_seq_index=last_lin_clf, pct=0.20, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# 40% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=clf_dir,\n",
        "    layer_seq_index=last_lin_clf, pct=0.40, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# Quick peek of these specific rows\n",
        "import pandas as pd\n",
        "df = pd.read_csv(PROJECT_ROOT/\"results/damage_runs.csv\")\n",
        "mask = (df[\"dataset\"]==\"breast_cancer\") & (df[\"task\"]==\"clf\") & (df[\"damage_type\"]==\"neurons\") \\\n",
        "       & (df[\"layer_seq_index\"]==last_lin_clf) & (df[\"pct\"].isin([0.2, 0.4])) \\\n",
        "       & (df[\"repeats_total\"].isin([5,10]))\n",
        "print(df[mask].tail(12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmY7a62TlROD",
        "outputId": "9470d3cc-95ce-4817-9532-9b69df201670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged 17 rows to /content/drive/MyDrive/FFNN_Healing_Thesis/results/damage_runs.csv\n",
            "Logged 17 rows to /content/drive/MyDrive/FFNN_Healing_Thesis/results/damage_runs.csv\n",
            "           timestamp             phase             dataset task damage_type  \\\n",
            "96   20250812-092313  damage_immediate  california_housing  reg     neurons   \n",
            "97   20250812-092314  damage_immediate  california_housing  reg     neurons   \n",
            "98   20250812-092314  damage_immediate  california_housing  reg     neurons   \n",
            "99   20250812-092314  damage_immediate  california_housing  reg     neurons   \n",
            "100  20250812-092314  damage_immediate  california_housing  reg     neurons   \n",
            "101  20250812-092314  damage_immediate  california_housing  reg     neurons   \n",
            "102  20250812-092314  damage_immediate  california_housing  reg     neurons   \n",
            "103  20250812-092314  damage_immediate  california_housing  reg     neurons   \n",
            "104  20250812-092314  damage_immediate  california_housing  reg     neurons   \n",
            "105  20250812-092314  damage_immediate  california_housing  reg     neurons   \n",
            "106  20250812-092314  damage_immediate  california_housing  reg     neurons   \n",
            "107  20250812-092314  damage_immediate  california_housing  reg     neurons   \n",
            "\n",
            "     layer_seq_index  pct  repeats_total  repeat_eval  \\\n",
            "96                 0  0.4              5            5   \n",
            "97                 0  0.4             10            0   \n",
            "98                 0  0.4             10            1   \n",
            "99                 0  0.4             10            2   \n",
            "100                0  0.4             10            3   \n",
            "101                0  0.4             10            4   \n",
            "102                0  0.4             10            5   \n",
            "103                0  0.4             10            6   \n",
            "104                0  0.4             10            7   \n",
            "105                0  0.4             10            8   \n",
            "106                0  0.4             10            9   \n",
            "107                0  0.4             10           10   \n",
            "\n",
            "                                                  ckpt metric  \\\n",
            "96   /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "97   /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "98   /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "99   /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "100  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "101  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "102  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "103  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "104  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "105  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "106  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "107  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   rmse   \n",
            "\n",
            "     baseline_metric     value  delta_from_baseline  f1       mae  \n",
            "96          0.522273  1.103114             0.580841 NaN  0.817469  \n",
            "97          0.522273  0.522273             0.000000 NaN  0.362560  \n",
            "98          0.522273  0.637477             0.115204 NaN  0.456024  \n",
            "99          0.522273  0.860813             0.338539 NaN  0.614335  \n",
            "100         0.522273  0.964255             0.441981 NaN  0.693093  \n",
            "101         0.522273  1.038430             0.516157 NaN  0.757623  \n",
            "102         0.522273  1.103114             0.580841 NaN  0.817469  \n",
            "103         0.522273  1.212306             0.690033 NaN  0.874546  \n",
            "104         0.522273  1.248145             0.725872 NaN  0.897995  \n",
            "105         0.522273  1.292039             0.769765 NaN  0.924615  \n",
            "106         0.522273  1.292039             0.769765 NaN  0.924615  \n",
            "107         0.522273  1.292039             0.769765 NaN  0.924615  \n"
          ]
        }
      ],
      "source": [
        "# === 4.7 SMALL BATCH: reg · first hidden · neurons · 20% & 40% · repeats [5, 10]\n",
        "set_seed(42)\n",
        "\n",
        "first_lin_reg = layer_seq_indices(reg_ctor)[0]  # seq index 0\n",
        "\n",
        "# 20% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=reg_dir,\n",
        "    layer_seq_index=first_lin_reg, pct=0.20, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"california_housing\"\n",
        ")\n",
        "\n",
        "# 40% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=reg_dir,\n",
        "    layer_seq_index=first_lin_reg, pct=0.40, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"california_housing\"\n",
        ")\n",
        "\n",
        "# Quick peek of just these rows\n",
        "import pandas as pd\n",
        "df = pd.read_csv(PROJECT_ROOT/\"results/damage_runs.csv\")\n",
        "mask = (df[\"dataset\"]==\"california_housing\") & (df[\"task\"]==\"reg\") & (df[\"damage_type\"]==\"neurons\") \\\n",
        "       & (df[\"layer_seq_index\"]==first_lin_reg) & (df[\"pct\"].isin([0.2, 0.4])) \\\n",
        "       & (df[\"repeats_total\"].isin([5,10]))\n",
        "print(df[mask].tail(12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5smUJZOlz40"
      },
      "outputs": [],
      "source": [
        "#rereun\n",
        "# === PHASE 5.0a: Healing scaffolding (gradient masks + masked damage) ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Reuse get_linear_layers, summarize_linear_layers from earlier.\n",
        "\n",
        "def _ensure_mask(t, fill=1.0):\n",
        "    \"\"\"Create a float mask same shape as tensor t, filled with fill (1.0 keeps grads, 0.0 freezes).\"\"\"\n",
        "    return torch.full_like(t, float(fill))\n",
        "\n",
        "def combine_mask_(base_mask: torch.Tensor, new_mask: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Combine two masks in-place: keep = base OR new (i.e., param is trainable if either says 1).\n",
        "    We want to FREEZE damaged entries, so we combine with logical-AND instead:\n",
        "    \"\"\"\n",
        "    # IMPORTANT: masks are 1=keep/trainable, 0=freeze; after new damage, final keep = base_keep * new_keep\n",
        "    base_mask.mul_(new_mask)\n",
        "    return base_mask\n",
        "\n",
        "def register_grad_mask_hooks(mask_dict):\n",
        "    \"\"\"\n",
        "    Given {param: mask}, register a backward hook that multiplies incoming gradients by the mask.\n",
        "    Returns list of hook handles; keep them alive during training, then remove() after.\n",
        "    \"\"\"\n",
        "    hooks = []\n",
        "    for p, m in mask_dict.items():\n",
        "        # make sure mask is on same device/dtype\n",
        "        mm = m.to(p.device).to(p.dtype)\n",
        "        h = p.register_hook(lambda g, _mm=mm: g * _mm)\n",
        "        hooks.append(h)\n",
        "    return hooks\n",
        "\n",
        "def init_full_keep_masks_for_model(model: nn.Module):\n",
        "    \"\"\"\n",
        "    Build an initial mask dict of ones (keep all trainable) for each Linear weight/bias we might touch.\n",
        "    We will AND (multiply) zeros into these masks as we damage more.\n",
        "    \"\"\"\n",
        "    mask_dict = {}\n",
        "    for _, m in model.named_modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            mask_dict[m.weight] = _ensure_mask(m.weight, fill=1.0)\n",
        "            if m.bias is not None:\n",
        "                mask_dict[m.bias] = _ensure_mask(m.bias, fill=1.0)\n",
        "    return mask_dict\n",
        "\n",
        "def damage_neurons_in_layer_with_masks(model: nn.Module, layer_lin_seq_index: int, pct_neurons: float, rng: np.random.Generator, mask_dict=None):\n",
        "    \"\"\"\n",
        "    Same as damage_neurons_in_layer, but also returns/updates a mask_dict so damaged entries are frozen.\n",
        "    - Zero rows in target Linear (incoming to damaged neuron) + its bias entries.\n",
        "    - Zero columns in the next Linear (outgoing from damaged neuron).\n",
        "    - Update mask_dict so these entries get gradient=0 during healing.\n",
        "    Returns: (damaged_neuron_indices)\n",
        "    \"\"\"\n",
        "    linear_layers = get_linear_layers(model)\n",
        "    # find target & next linear layers (by sequential index in model.net)\n",
        "    target_pos = None\n",
        "    for k, (seq_i, lin) in enumerate(linear_layers):\n",
        "        if seq_i == layer_lin_seq_index:\n",
        "            target_pos = k\n",
        "            break\n",
        "    assert target_pos is not None, f\"No Linear layer at seq index {layer_lin_seq_index}\"\n",
        "    target_seq_i, target_lin = linear_layers[target_pos]\n",
        "    next_lin = linear_layers[target_pos + 1][1] if (target_pos + 1) < len(linear_layers) else None\n",
        "\n",
        "    out_feats = target_lin.out_features\n",
        "    n_dmg = max(1, int(round(pct_neurons * out_feats)))\n",
        "    dmg_neurons = rng.choice(out_feats, size=n_dmg, replace=False)\n",
        "\n",
        "    # Initialize mask dict if needed (1 = keep/train, 0 = freeze)\n",
        "    if mask_dict is None:\n",
        "        mask_dict = init_full_keep_masks_for_model(model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Zero rows in target weight + target bias at damaged neurons\n",
        "        W = target_lin.weight  # [out, in]\n",
        "        W[dmg_neurons, :] = 0.0\n",
        "        maskW = mask_dict[W] if W in mask_dict else mask_dict.setdefault(W, _ensure_mask(W, 1.0))\n",
        "        maskW[dmg_neurons, :] = 0.0  # freeze those rows\n",
        "\n",
        "        if target_lin.bias is not None:\n",
        "            b = target_lin.bias\n",
        "            b[dmg_neurons] = 0.0\n",
        "            maskb = mask_dict[b] if b in mask_dict else mask_dict.setdefault(b, _ensure_mask(b, 1.0))\n",
        "            maskb[dmg_neurons] = 0.0\n",
        "\n",
        "        # Zero columns in next layer's weight (outgoing from damaged neurons)\n",
        "        if next_lin is not None:\n",
        "            Wn = next_lin.weight  # [next_out, next_in], next_in == out_feats\n",
        "            Wn[:, dmg_neurons] = 0.0\n",
        "            maskWn = mask_dict[Wn] if Wn in mask_dict else mask_dict.setdefault(Wn, _ensure_mask(Wn, 1.0))\n",
        "            maskWn[:, dmg_neurons] = 0.0\n",
        "\n",
        "    return dmg_neurons.tolist(), mask_dict\n",
        "\n",
        "def apply_progressive_neuron_damage_with_masks(model_ctor, ckpt_path, task, data_dir,\n",
        "                                               layer_lin_seq_index, pct_neurons, repeats, seed=12345):\n",
        "    \"\"\"\n",
        "    Load clean model, apply progressive neuron damage (repeats times), accumulating both damage and masks.\n",
        "    Return: model_damaged, mask_dict, metrics_before, metrics_after\n",
        "    \"\"\"\n",
        "    # load data (for evaluation)\n",
        "    if task == \"clf\":\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"clf\", batch_size=256)\n",
        "    else:\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"reg\", batch_size=512)\n",
        "\n",
        "    # fresh model from checkpoint\n",
        "    model = clone_from_checkpoint(model_ctor, ckpt_path)\n",
        "    metrics_before = evaluate_model(model, test_loader, task)\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    mask_dict = init_full_keep_masks_for_model(model)\n",
        "\n",
        "    for _ in range(repeats):\n",
        "        _, mask_dict = damage_neurons_in_layer_with_masks(\n",
        "            model, layer_lin_seq_index=layer_lin_seq_index, pct_neurons=pct_neurons, rng=rng, mask_dict=mask_dict\n",
        "        )\n",
        "\n",
        "    metrics_after = evaluate_model(model, test_loader, task)\n",
        "    return model, mask_dict, train_loader, val_loader, test_loader, metrics_before, metrics_after\n",
        "\n",
        "def healing_train_constrained(model, mask_dict, train_loader, val_loader, task,\n",
        "                              max_epochs=50, lr=1e-3, weight_decay=1e-4, patience=10, run_name=\"heal\"):\n",
        "    \"\"\"\n",
        "    Retrain while freezing masked entries (0=freeze). We enforce freezing by multiplying grads by mask each step.\n",
        "    Returns: history dict; best state_dict path.\n",
        "    \"\"\"\n",
        "    model = model.to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss() if task == \"clf\" else nn.MSELoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5, verbose=False)\n",
        "\n",
        "    # register hooks\n",
        "    hooks = register_grad_mask_hooks(mask_dict)\n",
        "\n",
        "    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"val_metric\": []}\n",
        "    best_val, best_path, epochs_no_improve = float(\"inf\"), MODEL_DIR / f\"{run_name}_{timestamp()}_best.pt\", 0\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        model.train(); train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb if task==\"clf\" else yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_losses.append(loss.item())\n",
        "        train_loss = float(np.mean(train_losses)) if train_losses else 0.0\n",
        "\n",
        "        # validate\n",
        "        model.eval(); val_losses = []; y_true_list, y_pred_list = [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                out = model(xb)\n",
        "                loss = criterion(out, yb if task==\"clf\" else yb)\n",
        "                val_losses.append(loss.item())\n",
        "                if task==\"clf\":\n",
        "                    y_pred_list.append(out.argmax(dim=1).cpu().numpy()); y_true_list.append(yb.cpu().numpy())\n",
        "                else:\n",
        "                    y_pred_list.append(out.cpu().numpy().squeeze()); y_true_list.append(yb.cpu().numpy().squeeze())\n",
        "        val_loss = float(np.mean(val_losses)) if val_losses else 0.0\n",
        "        if task==\"clf\":\n",
        "            import numpy as _np\n",
        "            y_true = _np.concatenate(y_true_list); y_pred = _np.concatenate(y_pred_list)\n",
        "            val_metric = float(accuracy_score(y_true, y_pred))\n",
        "        else:\n",
        "            import numpy as _np, math as _math\n",
        "            y_true = _np.array(_np.concatenate([_np.atleast_1d(a) for a in y_true_list]))\n",
        "            y_pred = _np.array(_np.concatenate([_np.atleast_1d(a) for a in y_pred_list]))\n",
        "            val_metric = float(_math.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history[\"epoch\"].append(epoch); history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss); history[\"val_metric\"].append(val_metric)\n",
        "\n",
        "        # save best by val_loss\n",
        "        if val_loss < best_val - 1e-8:\n",
        "            best_val = val_loss; epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), best_path); mark=\"*\"\n",
        "        else:\n",
        "            epochs_no_improve += 1; mark=\"\"\n",
        "        if epoch % 10 == 0 or mark == \"*\":\n",
        "            print(f\"[{run_name}] epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | metric {val_metric:.4f} {mark}\")\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"[{run_name}] Early stopping at epoch {epoch}. Best val_loss={best_val:.4f}\")\n",
        "            break\n",
        "\n",
        "    # remove hooks\n",
        "    for h in hooks: h.remove()\n",
        "    return best_path, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nlrjimSmjZt",
        "outputId": "909371e8-bfab-4678-cc41-29f31f51476b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASELINE: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "POST-DAMAGE: {'accuracy': 0.872093023255814, 'f1': 0.8990825688073395}\n"
          ]
        }
      ],
      "source": [
        "#rerun\n",
        "#create a damaged model + masks (clf, first hidden, 20%, 5 repeats)\n",
        "set_seed(42)\n",
        "\n",
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]  # seq index 0\n",
        "\n",
        "damaged_model, mask_dict, train_loader, val_loader, test_loader, m_before, m_after = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=clf_dir,\n",
        "        layer_lin_seq_index=first_lin_clf,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "print(\"BASELINE:\", m_before)\n",
        "print(\"POST-DAMAGE:\", m_after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8ifyS2gm3mc",
        "outputId": "a38d5833-2afc-4014-9146-38acb1d6d2ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_clf_firstHidden_p20_r5_short] epoch 001 | train 0.1347 | val 0.1654 | metric 0.9412 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 002 | train 0.1150 | val 0.1425 | metric 0.9529 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 003 | train 0.1020 | val 0.1250 | metric 0.9647 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 004 | train 0.0892 | val 0.1114 | metric 0.9647 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 005 | train 0.0732 | val 0.1008 | metric 0.9647 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 006 | train 0.0744 | val 0.0922 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 007 | train 0.0715 | val 0.0859 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 008 | train 0.0601 | val 0.0809 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 009 | train 0.0611 | val 0.0770 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 010 | train 0.0648 | val 0.0747 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 011 | train 0.0532 | val 0.0734 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 012 | train 0.0537 | val 0.0733 | metric 0.9765 *\n",
            "SHORT HEAL (≈15 epochs) TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9622641509433962}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3378603677.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  m_short.load_state_dict(torch.load(best_short_path, map_location=DEVICE))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_clf_firstHidden_p20_r5_long] epoch 001 | train 0.1274 | val 0.1652 | metric 0.9412 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 002 | train 0.1071 | val 0.1418 | metric 0.9529 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 003 | train 0.0976 | val 0.1235 | metric 0.9647 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 004 | train 0.0898 | val 0.1100 | metric 0.9647 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 005 | train 0.0760 | val 0.0994 | metric 0.9647 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 006 | train 0.0644 | val 0.0910 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 007 | train 0.0616 | val 0.0848 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 008 | train 0.0621 | val 0.0801 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 009 | train 0.0567 | val 0.0773 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 010 | train 0.0500 | val 0.0756 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 011 | train 0.0519 | val 0.0750 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 012 | train 0.0478 | val 0.0742 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 013 | train 0.0529 | val 0.0737 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 014 | train 0.0420 | val 0.0728 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 015 | train 0.0444 | val 0.0719 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 020 | train 0.0353 | val 0.0760 | metric 0.9765 \n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 030 | train 0.0312 | val 0.0776 | metric 0.9765 \n",
            "[heal_clf_firstHidden_p20_r5_long] Early stopping at epoch 35. Best val_loss=0.0719\n",
            "LONG HEAL (≈150 epochs) TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9622641509433962}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3378603677.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  m_long.load_state_dict(torch.load(best_long_path, map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged 4 rows to /content/drive/MyDrive/FFNN_Healing_Thesis/results/healing_runs.csv\n",
            "Short heal plots: /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_firstHidden_p20_r5_short_loss.png | /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_firstHidden_p20_r5_short_accuracy.png\n",
            "Long heal plots: /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_firstHidden_p20_r5_long_loss.png | /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_firstHidden_p20_r5_long_accuracy.png\n"
          ]
        }
      ],
      "source": [
        "#rerun\n",
        "# heal (short vs long), evaluate, save plots, and log\n",
        "\n",
        "# ---- SHORT HEAL (≈15 epochs) ----\n",
        "short_name = \"heal_clf_firstHidden_p20_r5_short\"\n",
        "best_short_path, hist_short = healing_train_constrained(\n",
        "    model=damaged_model,\n",
        "    mask_dict=mask_dict,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    task=\"clf\",\n",
        "    max_epochs=15,          # quick heal\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=5,\n",
        "    run_name=short_name\n",
        ")\n",
        "\n",
        "# Load the best short-healed weights into a fresh model and evaluate on TEST\n",
        "m_short = clf_ctor().to(DEVICE)\n",
        "m_short.load_state_dict(torch.load(best_short_path, map_location=DEVICE))\n",
        "metrics_short = evaluate_model(m_short, test_loader, task=\"clf\")\n",
        "print(\"SHORT HEAL (≈15 epochs) TEST:\", metrics_short)\n",
        "\n",
        "# Plot short-heal curves\n",
        "fig_loss_s, fig_metric_s = plot_history(hist_short, short_name, task=\"clf\")\n",
        "\n",
        "# ---- LONG HEAL (≈150 epochs) ----\n",
        "# IMPORTANT: start from the same post-damage state again (not from the short-healed model)\n",
        "damaged_model2, mask_dict2, train_loader2, val_loader2, test_loader2, _, _ = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=clf_dir,\n",
        "        layer_lin_seq_index=first_lin_clf,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "long_name = \"heal_clf_firstHidden_p20_r5_long\"\n",
        "best_long_path, hist_long = healing_train_constrained(\n",
        "    model=damaged_model2,\n",
        "    mask_dict=mask_dict2,\n",
        "    train_loader=train_loader2,\n",
        "    val_loader=val_loader2,\n",
        "    task=\"clf\",\n",
        "    max_epochs=150,         # longer heal\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=20,\n",
        "    run_name=long_name\n",
        ")\n",
        "\n",
        "m_long = clf_ctor().to(DEVICE)\n",
        "m_long.load_state_dict(torch.load(best_long_path, map_location=DEVICE))\n",
        "metrics_long = evaluate_model(m_long, test_loader2, task=\"clf\")\n",
        "print(\"LONG HEAL (≈150 epochs) TEST:\", metrics_long)\n",
        "\n",
        "# Plot long-heal curves\n",
        "fig_loss_l, fig_metric_l = plot_history(hist_long, long_name, task=\"clf\")\n",
        "\n",
        "# ---- LOG everything to a separate healing CSV ----\n",
        "rows = [\n",
        "    {\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"healing\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"stage\": \"baseline\",\n",
        "        \"layer_seq_index\": int(first_lin_clf),\n",
        "        \"pct\": 0.20,\n",
        "        \"repeats\": 5,\n",
        "        \"accuracy\": float(m_before[\"accuracy\"]),\n",
        "        \"f1\": float(m_before[\"f1\"])\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"healing\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"stage\": \"post_damage\",\n",
        "        \"layer_seq_index\": int(first_lin_clf),\n",
        "        \"pct\": 0.20,\n",
        "        \"repeats\": 5,\n",
        "        \"accuracy\": float(m_after[\"accuracy\"]),\n",
        "        \"f1\": float(m_after[\"f1\"])\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"healing\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"stage\": \"healed_short\",\n",
        "        \"layer_seq_index\": int(first_lin_clf),\n",
        "        \"pct\": 0.20,\n",
        "        \"repeats\": 5,\n",
        "        \"epochs\": len(hist_short[\"epoch\"]),\n",
        "        \"accuracy\": float(metrics_short[\"accuracy\"]),\n",
        "        \"f1\": float(metrics_short[\"f1\"]),\n",
        "        \"best_ckpt\": str(best_short_path),\n",
        "        \"fig_loss\": fig_loss_s,\n",
        "        \"fig_metric\": fig_metric_s\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"healing\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"stage\": \"healed_long\",\n",
        "        \"layer_seq_index\": int(first_lin_clf),\n",
        "        \"pct\": 0.20,\n",
        "        \"repeats\": 5,\n",
        "        \"epochs\": len(hist_long[\"epoch\"]),\n",
        "        \"accuracy\": float(metrics_long[\"accuracy\"]),\n",
        "        \"f1\": float(metrics_long[\"f1\"]),\n",
        "        \"best_ckpt\": str(best_long_path),\n",
        "        \"fig_loss\": fig_loss_l,\n",
        "        \"fig_metric\": fig_metric_l\n",
        "    }\n",
        "]\n",
        "\n",
        "for r in rows:\n",
        "    log_result(r, csv_name=\"healing_runs.csv\")\n",
        "\n",
        "print(\"Logged 4 rows to\", PROJECT_ROOT / \"results\" / \"healing_runs.csv\")\n",
        "print(\"Short heal plots:\", fig_loss_s, \"|\", fig_metric_s)\n",
        "print(\"Long heal plots:\", fig_loss_l, \"|\", fig_metric_l)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX1EKhTBnSRN",
        "outputId": "3bdd75d4-6a3e-45a9-a769-4611774e5d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         timestamp    phase        dataset task         stage  \\\n",
            "0  20250812-092939  healing  breast_cancer  clf      baseline   \n",
            "1  20250812-092939  healing  breast_cancer  clf   post_damage   \n",
            "2  20250812-092939  healing  breast_cancer  clf  healed_short   \n",
            "3  20250812-092939  healing  breast_cancer  clf   healed_long   \n",
            "\n",
            "   layer_seq_index  pct  repeats  accuracy        f1  epochs  \\\n",
            "0                0  0.2        5  0.953488  0.962963     NaN   \n",
            "1                0  0.2        5  0.872093  0.899083     NaN   \n",
            "2                0  0.2        5  0.953488  0.962264    15.0   \n",
            "3                0  0.2        5  0.953488  0.962264    35.0   \n",
            "\n",
            "                                           best_ckpt  \\\n",
            "0                                                NaN   \n",
            "1                                                NaN   \n",
            "2  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   \n",
            "3  /content/drive/MyDrive/FFNN_Healing_Thesis/mod...   \n",
            "\n",
            "                                            fig_loss  \\\n",
            "0                                                NaN   \n",
            "1                                                NaN   \n",
            "2  /content/drive/MyDrive/FFNN_Healing_Thesis/fig...   \n",
            "3  /content/drive/MyDrive/FFNN_Healing_Thesis/fig...   \n",
            "\n",
            "                                          fig_metric  \n",
            "0                                                NaN  \n",
            "1                                                NaN  \n",
            "2  /content/drive/MyDrive/FFNN_Healing_Thesis/fig...  \n",
            "3  /content/drive/MyDrive/FFNN_Healing_Thesis/fig...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "dfh = pd.read_csv(PROJECT_ROOT/\"results/healing_runs.csv\")\n",
        "print(dfh.tail(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yh2-BFnn8tg"
      },
      "source": [
        "We’ll run two tiny healing experiments:\n",
        "\n",
        "Case A (likely recoverable): output layer · 20% neurons · repeats=1\n",
        "\n",
        "Case B (likely non-recoverable): output layer · 20% neurons · repeats=5 (often kills both logits across repeats → model collapses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czkFZiWGn5kC",
        "outputId": "f9ba31ef-93ef-4540-b93b-52641f7a7d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A — BASELINE: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "A — POST-DAMAGE: {'accuracy': 0.9651162790697675, 'f1': 0.9724770642201835}\n"
          ]
        }
      ],
      "source": [
        "# A1 — damage: output layer, 20%, repeats=1\n",
        "set_seed(42)\n",
        "\n",
        "last_lin_clf = layer_seq_indices(clf_ctor)[-1]  # seq index of output Linear\n",
        "\n",
        "damaged_A, mask_A, train_A, val_A, test_A, mA_before, mA_after = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=clf_dir,\n",
        "        layer_lin_seq_index=last_lin_clf,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=1,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "print(\"A — BASELINE:\", mA_before)\n",
        "print(\"A — POST-DAMAGE:\", mA_after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlWOWf9Doi6O",
        "outputId": "474b041b-62ec-4d03-8a45-f5920dc6d062"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_clf_OUTPUT_p20_r1_short] epoch 001 | train 0.0825 | val 0.0741 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 002 | train 0.0695 | val 0.0633 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 003 | train 0.0603 | val 0.0573 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 004 | train 0.0518 | val 0.0539 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 005 | train 0.0435 | val 0.0514 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 006 | train 0.0445 | val 0.0494 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 007 | train 0.0414 | val 0.0484 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 008 | train 0.0344 | val 0.0479 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 009 | train 0.0356 | val 0.0475 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 010 | train 0.0375 | val 0.0477 | metric 0.9765 \n",
            "[heal_clf_OUTPUT_p20_r1_short] Early stopping at epoch 14. Best val_loss=0.0475\n",
            "A — SHORT HEAL TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-592423115.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mA_s.load_state_dict(torch.load(bestA_s, map_location=DEVICE))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_clf_OUTPUT_p20_r1_long] epoch 001 | train 0.0797 | val 0.0735 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 002 | train 0.0652 | val 0.0628 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 003 | train 0.0564 | val 0.0562 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 004 | train 0.0510 | val 0.0525 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 005 | train 0.0497 | val 0.0509 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 006 | train 0.0406 | val 0.0501 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 007 | train 0.0355 | val 0.0496 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 008 | train 0.0340 | val 0.0494 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 009 | train 0.0359 | val 0.0491 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 010 | train 0.0331 | val 0.0497 | metric 0.9765 \n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 020 | train 0.0214 | val 0.0557 | metric 0.9765 \n",
            "[heal_clf_OUTPUT_p20_r1_long] Early stopping at epoch 29. Best val_loss=0.0491\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-592423115.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mA_l.load_state_dict(torch.load(bestA_l, map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A — LONG HEAL TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "A — Logged 4 rows to /content/drive/MyDrive/FFNN_Healing_Thesis/results/healing_runs.csv\n",
            "A — Plots: /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_OUTPUT_p20_r1_short_loss.png /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_OUTPUT_p20_r1_short_accuracy.png | /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_OUTPUT_p20_r1_long_loss.png /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_OUTPUT_p20_r1_long_accuracy.png\n"
          ]
        }
      ],
      "source": [
        "# A2 — heal short (15) & long (150), evaluate, log\n",
        "A_short = \"heal_clf_OUTPUT_p20_r1_short\"\n",
        "bestA_s, histA_s = healing_train_constrained(\n",
        "    model=damaged_A, mask_dict=mask_A,\n",
        "    train_loader=train_A, val_loader=val_A, task=\"clf\",\n",
        "    max_epochs=15, lr=1e-3, weight_decay=1e-4, patience=5,\n",
        "    run_name=A_short\n",
        ")\n",
        "mA_s = clf_ctor().to(DEVICE)\n",
        "mA_s.load_state_dict(torch.load(bestA_s, map_location=DEVICE))\n",
        "metricsA_s = evaluate_model(mA_s, test_A, task=\"clf\")\n",
        "print(\"A — SHORT HEAL TEST:\", metricsA_s)\n",
        "figA_s_loss, figA_s_metric = plot_history(histA_s, A_short, task=\"clf\")\n",
        "\n",
        "# Recreate the SAME post-damage state for a clean long run\n",
        "damaged_A2, mask_A2, train_A2, val_A2, test_A2, _, _ = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=clf_dir,\n",
        "        layer_lin_seq_index=last_lin_clf,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=1,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "A_long = \"heal_clf_OUTPUT_p20_r1_long\"\n",
        "bestA_l, histA_l = healing_train_constrained(\n",
        "    model=damaged_A2, mask_dict=mask_A2,\n",
        "    train_loader=train_A2, val_loader=val_A2, task=\"clf\",\n",
        "    max_epochs=150, lr=1e-3, weight_decay=1e-4, patience=20,\n",
        "    run_name=A_long\n",
        ")\n",
        "mA_l = clf_ctor().to(DEVICE)\n",
        "mA_l.load_state_dict(torch.load(bestA_l, map_location=DEVICE))\n",
        "metricsA_l = evaluate_model(mA_l, test_A2, task=\"clf\")\n",
        "print(\"A — LONG HEAL TEST:\", metricsA_l)\n",
        "figA_l_loss, figA_l_metric = plot_history(histA_l, A_long, task=\"clf\")\n",
        "\n",
        "# Log\n",
        "rowsA = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"baseline\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 1,\n",
        "     \"accuracy\": float(mA_before[\"accuracy\"]), \"f1\": float(mA_before[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"post_damage\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 1,\n",
        "     \"accuracy\": float(mA_after[\"accuracy\"]), \"f1\": float(mA_after[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_short\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 1,\n",
        "     \"epochs\": len(histA_s[\"epoch\"]), \"accuracy\": float(metricsA_s[\"accuracy\"]), \"f1\": float(metricsA_s[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestA_s), \"fig_loss\": figA_s_loss, \"fig_metric\": figA_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_long\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 1,\n",
        "     \"epochs\": len(histA_l[\"epoch\"]), \"accuracy\": float(metricsA_l[\"accuracy\"]), \"f1\": float(metricsA_l[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestA_l), \"fig_loss\": figA_l_loss, \"fig_metric\": figA_l_metric},\n",
        "]\n",
        "for r in rowsA: log_result(r, csv_name=\"healing_runs.csv\")\n",
        "print(\"A — Logged 4 rows to\", PROJECT_ROOT/\"results/healing_runs.csv\")\n",
        "print(\"A — Plots:\", figA_s_loss, figA_s_metric, \"|\", figA_l_loss, figA_l_metric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkCT0WkCokpv",
        "outputId": "40230d3f-b9a7-44b6-e09d-efc5496e995f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B — BASELINE: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "B — POST-DAMAGE: {'accuracy': 0.37209302325581395, 'f1': 0.0}\n"
          ]
        }
      ],
      "source": [
        "# B1 — damage: output layer, 20%, repeats=5\n",
        "set_seed(42)\n",
        "\n",
        "damaged_B, mask_B, train_B, val_B, test_B, mB_before, mB_after = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=clf_dir,\n",
        "        layer_lin_seq_index=last_lin_clf,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "print(\"B — BASELINE:\", mB_before)\n",
        "print(\"B — POST-DAMAGE:\", mB_after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh8QgifFom0m",
        "outputId": "8f19830d-a54f-4afb-c32d-7fa053011e2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1130996581.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mB_s.load_state_dict(torch.load(bestB_s, map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_clf_OUTPUT_p20_r5_short] epoch 001 | train 0.6931 | val 0.6931 | metric 0.3765 *\n",
            "[heal_clf_OUTPUT_p20_r5_short] Early stopping at epoch 6. Best val_loss=0.6931\n",
            "B — SHORT HEAL TEST: {'accuracy': 0.37209302325581395, 'f1': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_clf_OUTPUT_p20_r5_long] epoch 001 | train 0.6931 | val 0.6931 | metric 0.3765 *\n",
            "[heal_clf_OUTPUT_p20_r5_long] epoch 010 | train 0.6931 | val 0.6931 | metric 0.3765 \n",
            "[heal_clf_OUTPUT_p20_r5_long] epoch 020 | train 0.6931 | val 0.6931 | metric 0.3765 \n",
            "[heal_clf_OUTPUT_p20_r5_long] Early stopping at epoch 21. Best val_loss=0.6931\n",
            "B — LONG HEAL TEST: {'accuracy': 0.37209302325581395, 'f1': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1130996581.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mB_l.load_state_dict(torch.load(bestB_l, map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B — Logged 4 rows to /content/drive/MyDrive/FFNN_Healing_Thesis/results/healing_runs.csv\n",
            "B — Plots: /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_OUTPUT_p20_r5_short_loss.png /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_OUTPUT_p20_r5_short_accuracy.png | /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_OUTPUT_p20_r5_long_loss.png /content/drive/MyDrive/FFNN_Healing_Thesis/figures/heal_clf_OUTPUT_p20_r5_long_accuracy.png\n"
          ]
        }
      ],
      "source": [
        "# B2 — heal short (15) & long (150), evaluate, log\n",
        "B_short = \"heal_clf_OUTPUT_p20_r5_short\"\n",
        "bestB_s, histB_s = healing_train_constrained(\n",
        "    model=damaged_B, mask_dict=mask_B,\n",
        "    train_loader=train_B, val_loader=val_B, task=\"clf\",\n",
        "    max_epochs=15, lr=1e-3, weight_decay=1e-4, patience=5,\n",
        "    run_name=B_short\n",
        ")\n",
        "mB_s = clf_ctor().to(DEVICE)\n",
        "mB_s.load_state_dict(torch.load(bestB_s, map_location=DEVICE))\n",
        "metricsB_s = evaluate_model(mB_s, test_B, task=\"clf\")\n",
        "print(\"B — SHORT HEAL TEST:\", metricsB_s)\n",
        "figB_s_loss, figB_s_metric = plot_history(histB_s, B_short, task=\"clf\")\n",
        "\n",
        "# Fresh same-damage state for long run\n",
        "damaged_B2, mask_B2, train_B2, val_B2, test_B2, _, _ = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=clf_dir,\n",
        "        layer_lin_seq_index=last_lin_clf,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "B_long = \"heal_clf_OUTPUT_p20_r5_long\"\n",
        "bestB_l, histB_l = healing_train_constrained(\n",
        "    model=damaged_B2, mask_dict=mask_B2,\n",
        "    train_loader=train_B2, val_loader=val_B2, task=\"clf\",\n",
        "    max_epochs=150, lr=1e-3, weight_decay=1e-4, patience=20,\n",
        "    run_name=B_long\n",
        ")\n",
        "mB_l = clf_ctor().to(DEVICE)\n",
        "mB_l.load_state_dict(torch.load(bestB_l, map_location=DEVICE))\n",
        "metricsB_l = evaluate_model(mB_l, test_B2, task=\"clf\")\n",
        "print(\"B — LONG HEAL TEST:\", metricsB_l)\n",
        "figB_l_loss, figB_l_metric = plot_history(histB_l, B_long, task=\"clf\")\n",
        "\n",
        "# Log\n",
        "rowsB = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"baseline\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"accuracy\": float(mB_before[\"accuracy\"]), \"f1\": float(mB_before[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"post_damage\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"accuracy\": float(mB_after[\"accuracy\"]), \"f1\": float(mB_after[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_short\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"epochs\": len(histB_s[\"epoch\"]), \"accuracy\": float(metricsB_s[\"accuracy\"]), \"f1\": float(metricsB_s[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestB_s), \"fig_loss\": figB_s_loss, \"fig_metric\": figB_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_long\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"epochs\": len(histB_l[\"epoch\"]), \"accuracy\": float(metricsB_l[\"accuracy\"]), \"f1\": float(metricsB_l[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestB_l), \"fig_loss\": figB_l_loss, \"fig_metric\": figB_l_metric},\n",
        "]\n",
        "for r in rowsB: log_result(r, csv_name=\"healing_runs.csv\")\n",
        "print(\"B — Logged 4 rows to\", PROJECT_ROOT/\"results/healing_runs.csv\")\n",
        "print(\"B — Plots:\", figB_s_loss, figB_s_metric, \"|\", figB_l_loss, figB_l_metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-2K8IMDorpg"
      },
      "source": [
        "Output layer, light damage (r=1) → no real harm; healing returns to baseline\n",
        "\n",
        "Output layer, heavy damage (r=5) → collapses to chance; healing fails with frozen logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfmVnTg3ooZV",
        "outputId": "a4ef0ef7-549a-482f-cd0b-528215cfc76c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REG — BASELINE: {'mae': 0.36256006360054016, 'rmse': 0.5222732064061352}\n",
            "REG — POST-DAMAGE: {'mae': 0.7138549089431763, 'rmse': 1.0377403968912586}\n"
          ]
        }
      ],
      "source": [
        "# regression: damage first hidden layer, 20%, repeats=5\n",
        "set_seed(42)\n",
        "\n",
        "first_lin_reg = layer_seq_indices(reg_ctor)[0]  # seq index 0 for reg model\n",
        "\n",
        "damaged_R, mask_R, train_R, val_R, test_R, mR_before, mR_after = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=reg_ctor,\n",
        "        ckpt_path=REG_CKPT,\n",
        "        task=\"reg\",\n",
        "        data_dir=reg_dir,\n",
        "        layer_lin_seq_index=first_lin_reg,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "print(\"REG — BASELINE:\", mR_before)\n",
        "print(\"REG — POST-DAMAGE:\", mR_after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQmGH6ZGo9g2",
        "outputId": "8b79ace3-1777-4bed-90d9-045ee7c0046f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_reg_firstHidden_p20_r5_short] epoch 001 | train 0.5881 | val 0.4757 | metric 0.6585 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 002 | train 0.3803 | val 0.3932 | metric 0.6114 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 003 | train 0.3478 | val 0.3756 | metric 0.5989 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 004 | train 0.3339 | val 0.3614 | metric 0.5906 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 005 | train 0.3254 | val 0.3501 | metric 0.5831 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 006 | train 0.3168 | val 0.3432 | metric 0.5781 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 007 | train 0.3167 | val 0.3392 | metric 0.5751 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 008 | train 0.3041 | val 0.3342 | metric 0.5714 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 009 | train 0.3054 | val 0.3293 | metric 0.5689 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 010 | train 0.2974 | val 0.3236 | metric 0.5643 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 013 | train 0.2887 | val 0.3174 | metric 0.5590 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 015 | train 0.2823 | val 0.3145 | metric 0.5561 *\n",
            "REG — SHORT HEAL TEST: {'mae': 0.37126675248146057, 'rmse': 0.5261521333589038}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-232092828.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mR_s.load_state_dict(torch.load(bestR_s, map_location=DEVICE))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_reg_firstHidden_p20_r5_long] epoch 001 | train 0.5834 | val 0.4756 | metric 0.6574 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 002 | train 0.3850 | val 0.3968 | metric 0.6115 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 003 | train 0.3500 | val 0.3756 | metric 0.5981 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 004 | train 0.3319 | val 0.3618 | metric 0.5901 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 005 | train 0.3234 | val 0.3520 | metric 0.5834 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 006 | train 0.3267 | val 0.3444 | metric 0.5790 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 007 | train 0.3104 | val 0.3403 | metric 0.5754 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 008 | train 0.3055 | val 0.3328 | metric 0.5699 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 009 | train 0.2999 | val 0.3306 | metric 0.5674 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 010 | train 0.3002 | val 0.3269 | metric 0.5654 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 011 | train 0.2947 | val 0.3247 | metric 0.5639 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 012 | train 0.2886 | val 0.3219 | metric 0.5609 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 013 | train 0.2858 | val 0.3187 | metric 0.5601 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 014 | train 0.2840 | val 0.3174 | metric 0.5595 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 015 | train 0.2831 | val 0.3172 | metric 0.5577 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 016 | train 0.2804 | val 0.3152 | metric 0.5565 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 017 | train 0.2797 | val 0.3110 | metric 0.5550 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 020 | train 0.2744 | val 0.3053 | metric 0.5503 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 025 | train 0.2690 | val 0.3023 | metric 0.5456 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 026 | train 0.2669 | val 0.3013 | metric 0.5458 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 027 | train 0.2668 | val 0.2971 | metric 0.5444 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 029 | train 0.2636 | val 0.2966 | metric 0.5429 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 030 | train 0.2638 | val 0.2982 | metric 0.5424 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 033 | train 0.2561 | val 0.2966 | metric 0.5416 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 035 | train 0.2569 | val 0.2913 | metric 0.5390 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 038 | train 0.2511 | val 0.2887 | metric 0.5378 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 040 | train 0.2578 | val 0.2947 | metric 0.5408 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 048 | train 0.2434 | val 0.2878 | metric 0.5361 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 050 | train 0.2454 | val 0.2890 | metric 0.5360 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 055 | train 0.2412 | val 0.2874 | metric 0.5353 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 058 | train 0.2395 | val 0.2868 | metric 0.5355 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 060 | train 0.2377 | val 0.2869 | metric 0.5349 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 062 | train 0.2381 | val 0.2867 | metric 0.5351 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 063 | train 0.2405 | val 0.2856 | metric 0.5349 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 070 | train 0.2364 | val 0.2866 | metric 0.5347 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 074 | train 0.2349 | val 0.2855 | metric 0.5343 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 076 | train 0.2391 | val 0.2848 | metric 0.5341 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 080 | train 0.2392 | val 0.2861 | metric 0.5343 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 090 | train 0.2352 | val 0.2851 | metric 0.5340 \n"
          ]
        }
      ],
      "source": [
        "# R2 — regression healing (short vs long), evaluate, log\n",
        "\n",
        "# ---- SHORT HEAL (≈15 epochs) ----\n",
        "R_short = \"heal_reg_firstHidden_p20_r5_short\"\n",
        "bestR_s, histR_s = healing_train_constrained(\n",
        "    model=damaged_R,\n",
        "    mask_dict=mask_R,\n",
        "    train_loader=train_R,\n",
        "    val_loader=val_R,\n",
        "    task=\"reg\",\n",
        "    max_epochs=15,          # quick heal\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=5,\n",
        "    run_name=R_short\n",
        ")\n",
        "\n",
        "mR_s = reg_ctor().to(DEVICE)\n",
        "mR_s.load_state_dict(torch.load(bestR_s, map_location=DEVICE))\n",
        "metricsR_s = evaluate_model(mR_s, test_R, task=\"reg\")\n",
        "print(\"REG — SHORT HEAL TEST:\", metricsR_s)\n",
        "figR_s_loss, figR_s_metric = plot_history(histR_s, R_short, task=\"reg\")\n",
        "\n",
        "# ---- LONG HEAL (≤150 epochs) ----\n",
        "# Recreate the SAME post-damage state so long run starts from identical damage\n",
        "damaged_R2, mask_R2, train_R2, val_R2, test_R2, _, _ = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=reg_ctor,\n",
        "        ckpt_path=REG_CKPT,\n",
        "        task=\"reg\",\n",
        "        data_dir=reg_dir,\n",
        "        layer_lin_seq_index=first_lin_reg,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "R_long = \"heal_reg_firstHidden_p20_r5_long\"\n",
        "bestR_l, histR_l = healing_train_constrained(\n",
        "    model=damaged_R2,\n",
        "    mask_dict=mask_R2,\n",
        "    train_loader=train_R2,\n",
        "    val_loader=val_R2,\n",
        "    task=\"reg\",\n",
        "    max_epochs=150,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=20,\n",
        "    run_name=R_long\n",
        ")\n",
        "\n",
        "mR_l = reg_ctor().to(DEVICE)\n",
        "mR_l.load_state_dict(torch.load(bestR_l, map_location=DEVICE))\n",
        "metricsR_l = evaluate_model(mR_l, test_R2, task=\"reg\")\n",
        "print(\"REG — LONG HEAL TEST:\", metricsR_l)\n",
        "figR_l_loss, figR_l_metric = plot_history(histR_l, R_long, task=\"reg\")\n",
        "\n",
        "# ---- LOG to healing_runs.csv (mae & rmse) ----\n",
        "rowsR = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"baseline\", \"layer_seq_index\": int(first_lin_reg), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"mae\": float(mR_before[\"mae\"]), \"rmse\": float(mR_before[\"rmse\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"post_damage\", \"layer_seq_index\": int(first_lin_reg), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"mae\": float(mR_after[\"mae\"]), \"rmse\": float(mR_after[\"rmse\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"healed_short\", \"layer_seq_index\": int(first_lin_reg), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"epochs\": len(histR_s[\"epoch\"]),\n",
        "     \"mae\": float(metricsR_s[\"mae\"]), \"rmse\": float(metricsR_s[\"rmse\"]),\n",
        "     \"best_ckpt\": str(bestR_s), \"fig_loss\": figR_s_loss, \"fig_metric\": figR_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"healed_long\", \"layer_seq_index\": int(first_lin_reg), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"epochs\": len(histR_l[\"epoch\"]),\n",
        "     \"mae\": float(metricsR_l[\"mae\"]), \"rmse\": float(metricsR_l[\"rmse\"]),\n",
        "     \"best_ckpt\": str(bestR_l), \"fig_loss\": figR_l_loss, \"fig_metric\": figR_l_metric},\n",
        "]\n",
        "for r in rowsR:\n",
        "    log_result(r, csv_name=\"healing_runs.csv\")\n",
        "\n",
        "print(\"REG — Logged 4 rows to\", PROJECT_ROOT / \"results\" / \"healing_runs.csv\")\n",
        "print(\"REG — Plots:\", figR_s_loss, figR_s_metric, \"|\", figR_l_loss, figR_l_metric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkv2QCcMpeQY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ffnn-healing",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
