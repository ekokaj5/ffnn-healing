{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFhVPoTySbdt",
        "outputId": "ac472e54-c0a0-4e28-d9bf-0fdd81dce833"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.8.0+cpu | CUDA available: False\n",
            "Device: cpu | Name: CPU\n"
          ]
        }
      ],
      "source": [
        "# GPU sanity check  (rerun after every restart)\n",
        "import torch\n",
        "\n",
        "gpu_available = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if gpu_available else \"cpu\")\n",
        "gpu_name = torch.cuda.get_device_name(0) if gpu_available else \"CPU\"\n",
        "\n",
        "print(f\"Torch: {torch.__version__} | CUDA available: {gpu_available}\")\n",
        "print(f\"Device: {device} | Name: {gpu_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tytVS_2CSiyh",
        "outputId": "98c61774-775e-4d39-b74e-cbbf1f2c9b39"
      },
      "source": [
        "# CELL 2 — Pin working versions (rerun once after a fresh session)\n",
        "# PyTorch 2.4.0 (CUDA 12.1 build) + matching libs\n",
        "!pip -q install --force-reinstall --no-cache-dir \\\n",
        "  torch==2.4.0+cu121 torchvision==0.19.0+cu121 torchaudio==2.4.0+cu121 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip -q install --force-reinstall --no-cache-dir \\\n",
        "  numpy==2.0.2 pandas==2.2.2 scikit-learn==1.6.1 matplotlib==3.10.0 seaborn==0.13.2 \\\n",
        "  tqdm==4.66.4 tabulate==0.9.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrSlu9azUZZY",
        "outputId": "c5616720-281d-43ae-9c73-86a50213f1fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: c:\\Users\\Admin\\Desktop\\ffnn-healing\n",
            "Data dir    : c:\\Users\\Admin\\Desktop\\ffnn-healing\\data\n",
            "Models dir  : c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\n",
            "Runs dir    : c:\\Users\\Admin\\Desktop\\ffnn-healing\\experiments\\runs\n"
          ]
        }
      ],
      "source": [
        "#  Project paths RERUN\n",
        "from pathlib import Path\n",
        "\n",
        "def find_project_root(start: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Walk up from 'start' until we find a .git folder (repo root).\n",
        "    If not found, fall back to current working directory.\n",
        "    \"\"\"\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / \".git\").exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_project_root(Path.cwd())\n",
        "\n",
        "\n",
        "DIRS = [\n",
        "    \"data\",             \n",
        "    \"logs\",             \n",
        "    \"models\",           \n",
        "    \"figures\",          \n",
        "    \"results\",          \n",
        "    \"notebooks\",        \n",
        "    \"experiments/configs\",\n",
        "    \"experiments/runs\", \n",
        "    \"outputs\"          \n",
        "]\n",
        "\n",
        "for d in DIRS:\n",
        "    (PROJECT_ROOT / d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "DATA_DIR    = PROJECT_ROOT / \"data\"\n",
        "MODELS_DIR  = PROJECT_ROOT / \"models\"\n",
        "RUNS_DIR    = PROJECT_ROOT / \"experiments\" / \"runs\"\n",
        "FIG_DIR     = PROJECT_ROOT / \"figures\"\n",
        "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
        "\n",
        "print(\"Project root:\", PROJECT_ROOT)\n",
        "print(\"Data dir    :\", DATA_DIR)\n",
        "print(\"Models dir  :\", MODELS_DIR)\n",
        "print(\"Runs dir    :\", RUNS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE0CsIHpYLY_",
        "outputId": "b4667485-a9df-47f9-818d-f87a999cb1bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Seed set. Log file will be: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\experiments_log.csv\n"
          ]
        }
      ],
      "source": [
        "# Utilities, seed, and experiment logging  RERUN\n",
        "import os, random, time\n",
        "from pathlib import Path\n",
        "import torch, numpy as np, pandas as pd\n",
        "\n",
        "# We expect PROJECT_ROOT from the previous \"Project paths\" cell.\n",
        "assert 'PROJECT_ROOT' in globals(), \"Run the Project paths cell first.\"\n",
        "\n",
        "# Device: reuse the 'device' from the GPU sanity cell if present; otherwise detect now\n",
        "DEVICE = globals().get('device', torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def timestamp() -> str:\n",
        "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "def exp_path(kind: str = \"results\") -> Path:\n",
        "    p = PROJECT_ROOT / kind\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def log_result(row: dict, csv_name: str = \"experiments_log.csv\") -> str:\n",
        "    csv_file = exp_path(\"results\") / csv_name\n",
        "    if csv_file.exists():\n",
        "        df0 = pd.read_csv(csv_file)\n",
        "        df = pd.concat([df0, pd.DataFrame([row])], ignore_index=True)\n",
        "    else:\n",
        "        df = pd.DataFrame([row])\n",
        "    df.to_csv(csv_file, index=False)\n",
        "    return str(csv_file)\n",
        "\n",
        "set_seed(123)\n",
        "print(\"Seed set. Log file will be:\", exp_path('results') / 'experiments_log.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKj8Cq8bZjmx",
        "outputId": "574ef937-ced5-4415-cee7-a61bd27b208a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV exists: True | path: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\experiments_log.csv\n",
            "         timestamp   phase         note\n",
            "0  20250813-102110  sanity  logger test\n"
          ]
        }
      ],
      "source": [
        "# Logger sanity test   OPTIONAL \n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "row = {\n",
        "    \"timestamp\": timestamp(),  \n",
        "    \"phase\": \"sanity\",\n",
        "    \"note\": \"logger test\"\n",
        "}\n",
        "csv_file = log_result(row)      \n",
        "\n",
        "csv_path = Path(csv_file)\n",
        "print(\"CSV exists:\", csv_path.exists(), \"| path:\", csv_path)\n",
        "print(pd.read_csv(csv_path).tail(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMeE3MAfZzhC",
        "outputId": "88673cce-26a8-46af-872c-4b97a88af420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[breast_cancer] Saved. features=30 | splits={'train': 398, 'val': 85, 'test': 86}\n",
            "  Class balance (train/val/test): {'train': {1: 250, 0: 148}, 'val': {1: 53, 0: 32}, 'test': {1: 54, 0: 32}}\n",
            "[california_housing] Saved. features=8 | splits={'train': 14448, 'val': 3096, 'test': 3096}\n",
            "\n",
            "Saved folders:\n",
            "c:\\Users\\Admin\\Desktop\\ffnn-healing\\data\n",
            "├─ breast_cancer\n",
            "│  └─ meta.json\n",
            "│  └─ scaler.joblib\n",
            "│  └─ X_test.npy\n",
            "│  └─ X_train.npy\n",
            "│  └─ X_val.npy\n",
            "│  └─ y_test.npy\n",
            "│  └─ y_train.npy\n",
            "│  └─ y_val.npy\n",
            "├─ california_housing\n",
            "│  └─ meta.json\n",
            "│  └─ scaler.joblib\n",
            "│  └─ X_test.npy\n",
            "│  └─ X_train.npy\n",
            "│  └─ X_val.npy\n",
            "│  └─ y_test.npy\n",
            "│  └─ y_train.npy\n",
            "│  └─ y_val.npy\n"
          ]
        }
      ],
      "source": [
        "# DATASET PREP (Option A)\n",
        "# RUN-WHEN-CHANGING-CONFIG \n",
        "\n",
        "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np, os, json, joblib, pandas as pd, collections\n",
        "from pathlib import Path\n",
        "\n",
        "set_seed(42)  # from our utilities cell\n",
        "\n",
        "# Use project-local data directory (created earlier)\n",
        "DATA_ROOT = DATA_DIR  # from the Project paths cell\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def split_scale_save(X, y, name, task_type, feature_names=None):\n",
        "    \"\"\"\n",
        "    Splits (70/15/15), scales features (fit on train), saves arrays+scaler+meta to disk.\n",
        "    task_type: 'clf' or 'reg'\n",
        "    \"\"\"\n",
        "    # Split\n",
        "    strat = y if task_type == \"clf\" else None\n",
        "    X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
        "        X, y, test_size=0.30, random_state=42, stratify=strat\n",
        "    )\n",
        "    strat_tmp = y_tmp if task_type == \"clf\" else None\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_tmp, y_tmp, test_size=0.50, random_state=42, stratify=strat_tmp\n",
        "    )\n",
        "\n",
        "    # Scale features using train stats only\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train = scaler.transform(X_train).astype(np.float32)\n",
        "    X_val   = scaler.transform(X_val).astype(np.float32)\n",
        "    X_test  = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "    # Ensure y dtypes\n",
        "    if task_type == \"clf\":\n",
        "        y_train = y_train.astype(np.int64); y_val = y_val.astype(np.int64); y_test = y_test.astype(np.int64)\n",
        "    else:\n",
        "        y_train = y_train.astype(np.float32); y_val = y_val.astype(np.float32); y_test = y_test.astype(np.float32)\n",
        "\n",
        "    # Save\n",
        "    OUT = DATA_ROOT / name\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "    np.save(OUT / \"X_train.npy\", X_train); np.save(OUT / \"y_train.npy\", y_train)\n",
        "    np.save(OUT / \"X_val.npy\",   X_val);   np.save(OUT / \"y_val.npy\",   y_val)\n",
        "    np.save(OUT / \"X_test.npy\",  X_test);  np.save(OUT / \"y_test.npy\",  y_test)\n",
        "    joblib.dump(scaler, OUT / \"scaler.joblib\")\n",
        "\n",
        "    # Metadata\n",
        "    meta = {\n",
        "        \"name\": name,\n",
        "        \"task\": task_type,\n",
        "        \"n_features\": int(X_train.shape[1]),\n",
        "        \"splits\": {\"train\": int(len(y_train)), \"val\": int(len(y_val)), \"test\": int(len(y_test))},\n",
        "        \"feature_names\": list(feature_names) if feature_names is not None else None\n",
        "    }\n",
        "    if task_type == \"clf\":\n",
        "        meta[\"class_counts\"] = {\n",
        "            \"train\": {int(k): int(v) for k,v in collections.Counter(y_train).items()},\n",
        "            \"val\":   {int(k): int(v) for k,v in collections.Counter(y_val).items()},\n",
        "            \"test\":  {int(k): int(v) for k,v in collections.Counter(y_test).items()},\n",
        "        }\n",
        "\n",
        "    with open(OUT / \"meta.json\", \"w\") as f:\n",
        "        f.write(json.dumps(meta, indent=2))\n",
        "\n",
        "    # Log a one-line summary to our experiments log\n",
        "    log_result({\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"data_prep\",\n",
        "        \"dataset\": name,\n",
        "        \"task\": task_type,\n",
        "        \"n_features\": meta[\"n_features\"],\n",
        "        \"train\": meta[\"splits\"][\"train\"],\n",
        "        \"val\": meta[\"splits\"][\"val\"],\n",
        "        \"test\": meta[\"splits\"][\"test\"]\n",
        "    })\n",
        "\n",
        "    print(f\"[{name}] Saved. features={meta['n_features']} | splits={meta['splits']}\")\n",
        "    if task_type == \"clf\":\n",
        "        print(f\"  Class balance (train/val/test): {meta['class_counts']}\")\n",
        "    return meta\n",
        "\n",
        "# ---- Classification: Breast Cancer (binary) ----\n",
        "bc = load_breast_cancer()\n",
        "meta_bc = split_scale_save(\n",
        "    bc.data, bc.target, name=\"breast_cancer\", task_type=\"clf\", feature_names=bc.feature_names\n",
        ")\n",
        "\n",
        "# ---- Regression: California Housing ----\n",
        "cal = fetch_california_housing()\n",
        "meta_cal = split_scale_save(\n",
        "    cal.data, cal.target, name=\"california_housing\", task_type=\"reg\", feature_names=cal.feature_names\n",
        ")\n",
        "\n",
        "def print_tree(root: Path, max_depth=2, prefix=\"\"):\n",
        "    root = Path(root)\n",
        "    def _walk(p: Path, depth: int, pref: str):\n",
        "        if depth > max_depth: \n",
        "            return\n",
        "        for child in sorted(p.iterdir()):\n",
        "            print(pref + (\"└─ \" if child.is_file() else \"├─ \") + child.name)\n",
        "            if child.is_dir():\n",
        "                _walk(child, depth+1, pref + \"│  \")\n",
        "    print(str(root))\n",
        "    _walk(root, 0, \"\")\n",
        "    \n",
        "print(\"\\nSaved folders:\")\n",
        "print_tree(DATA_ROOT, max_depth=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGc544Hgc8Io",
        "outputId": "c7dc8c94-97fa-4a64-ff31-3cacb5bac646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Ready. Torch: 2.4.0+cu121 | Device: cuda | Root: /content/drive/MyDrive/FFNN_Healing_Thesis\n"
          ]
        }
      ],
      "source": [
        "# === MINIMAL BOOTSTRAP (run after every restart) ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, random, time, torch, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/FFNN_Healing_Thesis\")\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def timestamp(): return time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "def exp_path(kind=\"results\"):\n",
        "    p = PROJECT_ROOT / kind\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def log_result(row: dict, csv_name=\"experiments_log.csv\"):\n",
        "    csv_file = exp_path(\"results\") / csv_name\n",
        "    df = pd.DataFrame([row])\n",
        "    if csv_file.exists():\n",
        "        df0 = pd.read_csv(csv_file); df = pd.concat([df0, df], ignore_index=True)\n",
        "    df.to_csv(csv_file, index=False)\n",
        "    return str(csv_file)\n",
        "\n",
        "print(\"Ready. Torch:\", torch.__version__, \"| Device:\", DEVICE, \"| Root:\", PROJECT_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ugwlz1C-d2P9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Commons ready ✓  | DEVICE: cpu\n"
          ]
        }
      ],
      "source": [
        "# COMMONS \n",
        "# RERUN-AFTER-RESTART\n",
        "\n",
        "import os, json, math, time\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Reuse what we already set earlier\n",
        "assert 'PROJECT_ROOT' in globals(), \"Run the Project paths cell first.\"\n",
        "assert 'FIG_DIR' in globals() and 'MODELS_DIR' in globals() and 'RESULTS_DIR' in globals(), \"Project dirs not set.\"\n",
        "DEVICE = globals().get('device', torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "set_seed(42)\n",
        "\n",
        "# Ensure dirs exist (safe if they already do)\n",
        "for d in [FIG_DIR, MODELS_DIR, RESULTS_DIR]:\n",
        "    Path(d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, X_path, y_path, task):\n",
        "        self.X = np.load(X_path).astype(np.float32)\n",
        "        self.y = np.load(y_path)\n",
        "        self.task = task\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, i):\n",
        "        x = torch.from_numpy(self.X[i])\n",
        "        if self.task == \"clf\":\n",
        "            y = torch.tensor(int(self.y[i]), dtype=torch.long)\n",
        "        else:\n",
        "            y = torch.tensor(float(self.y[i]), dtype=torch.float32).unsqueeze(0)\n",
        "        return x, y\n",
        "\n",
        "def make_loaders(data_dir, task, batch_size=128):\n",
        "    data_dir = Path(data_dir)\n",
        "    ds_train = NumpyDataset(data_dir/\"X_train.npy\", data_dir/\"y_train.npy\", task)\n",
        "    ds_val   = NumpyDataset(data_dir/\"X_val.npy\",   data_dir/\"y_val.npy\",   task)\n",
        "    ds_test  = NumpyDataset(data_dir/\"X_test.npy\",  data_dir/\"y_test.npy\",  task)\n",
        "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    val_loader   = DataLoader(ds_val,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "    test_loader  = DataLoader(ds_test,  batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "    n_features = ds_train.X.shape[1]\n",
        "    return train_loader, val_loader, test_loader, n_features\n",
        "\n",
        "def init_kaiming(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "class FFNN_Classifier(nn.Module):\n",
        "    def __init__(self, n_in, hidden=[64, 64, 32], n_out=2, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, prev = [], n_in\n",
        "        for h in hidden:\n",
        "            layers += [nn.Linear(prev, h), nn.ReLU(), nn.Dropout(p_drop)]\n",
        "            prev = h\n",
        "        layers += [nn.Linear(prev, n_out)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.apply(init_kaiming)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class FFNN_Regression(nn.Module):\n",
        "    def __init__(self, n_in, hidden=[64, 64, 32], p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, prev = [], n_in\n",
        "        for h in hidden:\n",
        "            layers += [nn.Linear(prev, h), nn.ReLU(), nn.Dropout(p_drop)]\n",
        "            prev = h\n",
        "        layers += [nn.Linear(prev, 1)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.apply(init_kaiming)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, task, max_epochs=200, lr=1e-3, weight_decay=1e-4, patience=20, run_name=\"run\"):\n",
        "    model = model.to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss() if task==\"clf\" else nn.MSELoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "\n",
        "    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"val_metric\": []}\n",
        "    best_val, best_path, epochs_no_improve = float(\"inf\"), Path(MODELS_DIR) / f\"{run_name}_{timestamp()}_best.pt\", 0\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        model.train(); train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb if task==\"clf\" else yb)\n",
        "            loss.backward(); opt.step()\n",
        "            train_losses.append(loss.item())\n",
        "        train_loss = float(np.mean(train_losses)) if train_losses else 0.0\n",
        "\n",
        "        model.eval(); val_losses = []; y_true_list, y_pred_list = [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                out = model(xb)\n",
        "                loss = criterion(out, yb if task==\"clf\" else yb)\n",
        "                val_losses.append(loss.item())\n",
        "                if task == \"clf\":\n",
        "                    preds = out.argmax(dim=1).detach().cpu().numpy()\n",
        "                    y_true_list.append(yb.detach().cpu().numpy())\n",
        "                    y_pred_list.append(preds)\n",
        "                else:\n",
        "                    y_true_list.append(yb.detach().cpu().numpy().squeeze())\n",
        "                    y_pred_list.append(out.detach().cpu().numpy().squeeze())\n",
        "        val_loss = float(np.mean(val_losses)) if val_losses else 0.0\n",
        "\n",
        "        if task == \"clf\":\n",
        "            y_true = np.concatenate(y_true_list); y_pred = np.concatenate(y_pred_list)\n",
        "            val_metric = float(accuracy_score(y_true, y_pred))\n",
        "        else:\n",
        "            y_true = np.array(np.concatenate([np.atleast_1d(a) for a in y_true_list]))\n",
        "            y_pred = np.array(np.concatenate([np.atleast_1d(a) for a in y_pred_list]))\n",
        "            val_metric = float(math.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history[\"epoch\"].append(epoch); history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss); history[\"val_metric\"].append(val_metric)\n",
        "\n",
        "        if val_loss < best_val - 1e-8:\n",
        "            best_val = val_loss; epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), best_path); best_mark=\"*\"\n",
        "        else:\n",
        "            epochs_no_improve += 1; best_mark=\"\"\n",
        "        if epoch % 10 == 0 or best_mark == \"*\":\n",
        "            print(f\"[{run_name}] epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | metric {val_metric:.4f} {best_mark}\")\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"[{run_name}] Early stopping at epoch {epoch}. Best val_loss={best_val:.4f}\")\n",
        "            break\n",
        "    return best_path, history\n",
        "\n",
        "def evaluate_model(model, loader, task):\n",
        "    model.eval(); y_true_list, y_pred_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            out = model(xb)\n",
        "            if task == \"clf\":\n",
        "                preds = out.argmax(dim=1)\n",
        "                y_true_list.append(yb.cpu().numpy()); y_pred_list.append(preds.cpu().numpy())\n",
        "            else:\n",
        "                y_true_list.append(yb.cpu().numpy().squeeze()); y_pred_list.append(out.cpu().numpy().squeeze())\n",
        "    if task == \"clf\":\n",
        "        y_true = np.concatenate(y_true_list); y_pred = np.concatenate(y_pred_list)\n",
        "        return {\"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "                \"f1\": float(f1_score(y_true, y_pred, zero_division=0))}\n",
        "    else:\n",
        "        y_true = np.array(np.concatenate([np.atleast_1d(a) for a in y_true_list]))\n",
        "        y_pred = np.array(np.concatenate([np.atleast_1d(a) for a in y_pred_list]))\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        return {\"mae\": float(mae), \"rmse\": float(rmse)}\n",
        "\n",
        "def plot_history(history, run_name, task):\n",
        "    fig = plt.figure(figsize=(6,4))\n",
        "    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train_loss\")\n",
        "    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.title(f\"{run_name} loss\")\n",
        "    fig_path1 = Path(FIG_DIR) / f\"{run_name}_loss.png\"\n",
        "    plt.savefig(fig_path1, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "    fig = plt.figure(figsize=(6,4))\n",
        "    ylabel = \"accuracy\" if task==\"clf\" else \"RMSE\"\n",
        "    plt.plot(history[\"epoch\"], history[\"val_metric\"], label=f\"val_{ylabel}\")\n",
        "    plt.xlabel(\"epoch\"); plt.ylabel(ylabel); plt.legend(); plt.title(f\"{run_name} {ylabel}\")\n",
        "    fig_path2 = Path(FIG_DIR) / f\"{run_name}_{ylabel}.png\"\n",
        "    plt.savefig(fig_path2, bbox_inches=\"tight\"); plt.close()\n",
        "    return str(fig_path1), str(fig_path2)\n",
        "\n",
        "print(\"Commons ready ✓  | DEVICE:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgNzxzUKeYaA",
        "outputId": "d912bf95-ce56-4c0b-f185-82baeebc408f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_features: 30\n",
            "[clf_breast_cancer_baseline] epoch 001 | train 0.6813 | val 0.4313 | metric 0.9059 *\n",
            "[clf_breast_cancer_baseline] epoch 002 | train 0.4232 | val 0.3212 | metric 0.9412 *\n",
            "[clf_breast_cancer_baseline] epoch 003 | train 0.3494 | val 0.2498 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 004 | train 0.2894 | val 0.2014 | metric 0.9529 *\n",
            "[clf_breast_cancer_baseline] epoch 005 | train 0.2403 | val 0.1667 | metric 0.9529 *\n",
            "[clf_breast_cancer_baseline] epoch 006 | train 0.1997 | val 0.1404 | metric 0.9529 *\n",
            "[clf_breast_cancer_baseline] epoch 007 | train 0.2006 | val 0.1206 | metric 0.9647 *\n",
            "[clf_breast_cancer_baseline] epoch 008 | train 0.2253 | val 0.1066 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 009 | train 0.1667 | val 0.0989 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 010 | train 0.1386 | val 0.0924 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 011 | train 0.1131 | val 0.0865 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 012 | train 0.1322 | val 0.0810 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 013 | train 0.2014 | val 0.0754 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 014 | train 0.0980 | val 0.0734 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 015 | train 0.1482 | val 0.0715 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 016 | train 0.1037 | val 0.0700 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 017 | train 0.0899 | val 0.0684 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 018 | train 0.1079 | val 0.0663 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 019 | train 0.0737 | val 0.0636 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 020 | train 0.0837 | val 0.0611 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 021 | train 0.0781 | val 0.0587 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 022 | train 0.0897 | val 0.0560 | metric 0.9882 *\n",
            "[clf_breast_cancer_baseline] epoch 023 | train 0.0729 | val 0.0545 | metric 0.9882 *\n",
            "[clf_breast_cancer_baseline] epoch 024 | train 0.0820 | val 0.0541 | metric 0.9882 *\n",
            "[clf_breast_cancer_baseline] epoch 027 | train 0.1012 | val 0.0541 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 030 | train 0.0578 | val 0.0533 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 031 | train 0.0717 | val 0.0520 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 032 | train 0.0476 | val 0.0505 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 033 | train 0.0746 | val 0.0498 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 034 | train 0.0469 | val 0.0481 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 035 | train 0.0524 | val 0.0476 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 036 | train 0.0660 | val 0.0472 | metric 0.9765 *\n",
            "[clf_breast_cancer_baseline] epoch 040 | train 0.0376 | val 0.0629 | metric 0.9765 \n",
            "[clf_breast_cancer_baseline] epoch 050 | train 0.0424 | val 0.0658 | metric 0.9765 \n",
            "[clf_breast_cancer_baseline] Early stopping at epoch 56. Best val_loss=0.0472\n",
            "TEST metrics: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "Saved plots: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\clf_breast_cancer_baseline_loss.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\clf_breast_cancer_baseline_accuracy.png\n",
            "Best model path: c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf_breast_cancer_baseline_20250813-103737_best.pt\n",
            "Metrics saved to: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\clf_breast_cancer_baseline_20250813-103739_metrics.json\n"
          ]
        }
      ],
      "source": [
        "# Baseline training — Breast Cancer \n",
        "from pathlib import Path\n",
        "import json, torch\n",
        "\n",
        "\n",
        "BREAST_DIR = DATA_DIR / \"breast_cancer\"\n",
        "assert (BREAST_DIR / \"X_train.npy\").exists(), \"Run the dataset prep cell first to create local splits.\"\n",
        "\n",
        "train_loader, val_loader, test_loader, n_features = make_loaders(BREAST_DIR, task=\"clf\", batch_size=128)\n",
        "print(\"n_features:\", n_features)\n",
        "\n",
        "run_name = \"clf_breast_cancer_baseline\"\n",
        "model_clf = FFNN_Classifier(n_in=n_features, hidden=[64,64,32], n_out=2, p_drop=0.1)\n",
        "\n",
        "best_path, hist = train_model(\n",
        "    model_clf, train_loader, val_loader, task=\"clf\",\n",
        "    max_epochs=200, lr=1e-3, weight_decay=1e-4, patience=20, run_name=run_name\n",
        ")\n",
        "\n",
        "# Evaluate best checkpoint on test\n",
        "best_model = FFNN_Classifier(n_in=n_features, hidden=[64,64,32], n_out=2, p_drop=0.1).to(DEVICE)\n",
        "best_model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
        "metrics_test = evaluate_model(best_model, test_loader, task=\"clf\")\n",
        "print(\"TEST metrics:\", metrics_test)\n",
        "\n",
        "# Plots + metrics\n",
        "fig_loss, fig_metric = plot_history(hist, run_name, task=\"clf\")\n",
        "metrics_path = Path(RESULTS_DIR) / f\"{run_name}_{timestamp()}_metrics.json\"\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump({\"test\": metrics_test}, f, indent=2)\n",
        "\n",
        "# Log run summary to CSV\n",
        "log_result({\n",
        "    \"timestamp\": timestamp(),\n",
        "    \"phase\": \"baseline\",\n",
        "    \"dataset\": \"breast_cancer\",\n",
        "    \"task\": \"clf\",\n",
        "    \"model\": \"FFNN_Classifier[64,64,32]\",\n",
        "    \"test_accuracy\": metrics_test[\"accuracy\"],\n",
        "    \"test_f1\": metrics_test[\"f1\"],\n",
        "    \"best_ckpt\": str(best_path),\n",
        "    \"fig_loss\": fig_loss,\n",
        "    \"fig_metric\": fig_metric\n",
        "})\n",
        "\n",
        "print(\"Saved plots:\", fig_loss, \"|\", fig_metric)\n",
        "print(\"Best model path:\", best_path)\n",
        "print(\"Metrics saved to:\", metrics_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGPSf8qFe1Qd",
        "outputId": "f0d3c6fa-198f-4f5e-c262-ea72722cf0a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_features: 8\n",
            "[reg_california_baseline] epoch 001 | train 2.2803 | val 0.9713 | metric 0.9606 *\n",
            "[reg_california_baseline] epoch 002 | train 1.0828 | val 0.7501 | metric 0.8398 *\n",
            "[reg_california_baseline] epoch 003 | train 0.8886 | val 0.6839 | metric 0.7993 *\n",
            "[reg_california_baseline] epoch 004 | train 0.8055 | val 0.5888 | metric 0.7460 *\n",
            "[reg_california_baseline] epoch 005 | train 0.7231 | val 0.5782 | metric 0.7338 *\n",
            "[reg_california_baseline] epoch 006 | train 0.8544 | val 0.5396 | metric 0.6977 *\n",
            "[reg_california_baseline] epoch 007 | train 0.6485 | val 0.5157 | metric 0.6987 *\n",
            "[reg_california_baseline] epoch 008 | train 0.6164 | val 0.4713 | metric 0.6667 *\n",
            "[reg_california_baseline] epoch 009 | train 0.5646 | val 0.4703 | metric 0.6645 *\n",
            "[reg_california_baseline] epoch 010 | train 0.7211 | val 0.4758 | metric 0.6679 \n",
            "[reg_california_baseline] epoch 011 | train 0.5638 | val 0.4542 | metric 0.6617 *\n",
            "[reg_california_baseline] epoch 012 | train 0.5220 | val 0.4320 | metric 0.6394 *\n",
            "[reg_california_baseline] epoch 017 | train 0.4884 | val 0.4102 | metric 0.6267 *\n",
            "[reg_california_baseline] epoch 019 | train 0.4387 | val 0.4006 | metric 0.6181 *\n",
            "[reg_california_baseline] epoch 020 | train 0.4470 | val 0.3884 | metric 0.6117 *\n",
            "[reg_california_baseline] epoch 025 | train 0.4212 | val 0.3817 | metric 0.6075 *\n",
            "[reg_california_baseline] epoch 026 | train 0.4275 | val 0.3794 | metric 0.6083 *\n",
            "[reg_california_baseline] epoch 027 | train 0.4123 | val 0.3744 | metric 0.6047 *\n",
            "[reg_california_baseline] epoch 030 | train 0.4012 | val 0.3911 | metric 0.6120 \n",
            "[reg_california_baseline] epoch 034 | train 0.3881 | val 0.3725 | metric 0.5962 *\n",
            "[reg_california_baseline] epoch 036 | train 0.3882 | val 0.3684 | metric 0.5929 *\n",
            "[reg_california_baseline] epoch 037 | train 0.3811 | val 0.3606 | metric 0.5867 *\n",
            "[reg_california_baseline] epoch 040 | train 0.3720 | val 0.3481 | metric 0.5817 *\n",
            "[reg_california_baseline] epoch 044 | train 0.3734 | val 0.3463 | metric 0.5805 *\n",
            "[reg_california_baseline] epoch 050 | train 0.3946 | val 0.3540 | metric 0.5845 \n",
            "[reg_california_baseline] epoch 058 | train 0.3590 | val 0.3437 | metric 0.5776 *\n",
            "[reg_california_baseline] epoch 060 | train 0.3577 | val 0.3478 | metric 0.5812 \n",
            "[reg_california_baseline] epoch 061 | train 0.3586 | val 0.3433 | metric 0.5774 *\n",
            "[reg_california_baseline] epoch 062 | train 0.3590 | val 0.3428 | metric 0.5766 *\n",
            "[reg_california_baseline] epoch 070 | train 0.3604 | val 0.3482 | metric 0.5806 \n",
            "[reg_california_baseline] epoch 080 | train 0.3567 | val 0.3467 | metric 0.5794 \n",
            "[reg_california_baseline] Early stopping at epoch 87. Best val_loss=0.3428\n",
            "TEST metrics: {'mae': 0.37485018372535706, 'rmse': 0.5373355181330539}\n",
            "Saved plots: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\reg_california_baseline_loss.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\reg_california_baseline_RMSE.png\n",
            "Best model path: c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg_california_baseline_20250813-104035_best.pt\n",
            "Metrics saved to: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\reg_california_baseline_20250813-104136_metrics.json\n"
          ]
        }
      ],
      "source": [
        "# === REGRESSION BASELINE (California Housing) \n",
        "from pathlib import Path\n",
        "import json, torch\n",
        "\n",
        "CAL_DIR = DATA_DIR / \"california_housing\"   \n",
        "assert (CAL_DIR / \"X_train.npy\").exists(), \"Run the dataset prep cell first to create local splits.\"\n",
        "\n",
        "train_loader, val_loader, test_loader, n_features = make_loaders(CAL_DIR, task=\"reg\", batch_size=256)\n",
        "print(\"n_features:\", n_features)\n",
        "\n",
        "run_name = \"reg_california_baseline\"\n",
        "model_reg = FFNN_Regression(n_in=n_features, hidden=[128,64,32], p_drop=0.1)\n",
        "\n",
        "best_path, hist = train_model(\n",
        "    model_reg, train_loader, val_loader, task=\"reg\",\n",
        "    max_epochs=250, lr=1e-3, weight_decay=1e-4, patience=25, run_name=run_name\n",
        ")\n",
        "\n",
        "# Load best checkpoint and evaluate on test\n",
        "best_model = FFNN_Regression(n_in=n_features, hidden=[128,64,32], p_drop=0.1).to(DEVICE)\n",
        "state = torch.load(best_path, map_location=DEVICE)   \n",
        "best_model.load_state_dict(state)\n",
        "\n",
        "metrics_test = evaluate_model(best_model, test_loader, task=\"reg\")\n",
        "print(\"TEST metrics:\", metrics_test) \n",
        "\n",
        "# Plots + metrics file\n",
        "fig_loss, fig_metric = plot_history(hist, run_name, task=\"reg\")\n",
        "metrics_path = Path(RESULTS_DIR) / f\"{run_name}_{timestamp()}_metrics.json\"\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump({\"test\": metrics_test}, f, indent=2)\n",
        "\n",
        "# Log run summary to CSV\n",
        "log_result({\n",
        "    \"timestamp\": timestamp(),\n",
        "    \"phase\": \"baseline\",\n",
        "    \"dataset\": \"california_housing\",\n",
        "    \"task\": \"reg\",\n",
        "    \"model\": \"FFNN_Regression[128,64,32]\",\n",
        "    \"test_mae\": metrics_test[\"mae\"],\n",
        "    \"test_rmse\": metrics_test[\"rmse\"],\n",
        "    \"best_ckpt\": str(best_path),\n",
        "    \"fig_loss\": fig_loss,\n",
        "    \"fig_metric\": fig_metric\n",
        "})\n",
        "\n",
        "print(\"Saved plots:\", fig_loss, \"|\", fig_metric)\n",
        "print(\"Best model path:\", best_path)\n",
        "print(\"Metrics saved to:\", metrics_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NK9Gxp8-fZej"
      },
      "outputs": [],
      "source": [
        "#  DAMAGE ENGINE\n",
        "# RERUN-AFTER-RESTART\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "\n",
        "# Re-use: DEVICE, set_seed, log_result, evaluate_model, make_loaders, timestamp,\n",
        "# PROJECT_ROOT, MODELS_DIR, RESULTS_DIR already defined earlier.\n",
        "assert 'PROJECT_ROOT' in globals(), \"Run the Project paths cell first.\"\n",
        "assert 'RESULTS_DIR' in globals(), \"Run the Project paths cell first (defines RESULTS_DIR).\"\n",
        "\n",
        "def get_linear_layers(model: nn.Module):\n",
        "    \"\"\"Return list of (seq_index_in_model_net, linear_module) for nn.Linear layers in forward order.\"\"\"\n",
        "    layers = []\n",
        "    for i, m in enumerate(model.net):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            layers.append((i, m))\n",
        "    return layers\n",
        "\n",
        "def summarize_linear_layers(model):\n",
        "    \"\"\"Return a list of dicts describing each Linear layer (useful to pick which layer to damage).\"\"\"\n",
        "    info = []\n",
        "    for li, lin in get_linear_layers(model):\n",
        "        info.append({\"seq_index\": li, \"in_features\": lin.in_features, \"out_features\": lin.out_features})\n",
        "    return info\n",
        "\n",
        "def clone_from_checkpoint(model_ctor, ckpt_path: str):\n",
        "    \"\"\"\n",
        "    Build a fresh model via `model_ctor()` and load weights from `ckpt_path`.\n",
        "    `model_ctor` should be a zero-arg function returning an *already-constructed* model with the same architecture.\n",
        "    \"\"\"\n",
        "    m = model_ctor().to(DEVICE)\n",
        "    try:\n",
        "        state = torch.load(ckpt_path, map_location=DEVICE, weights_only=True)  # newer torch\n",
        "    except TypeError:\n",
        "        state = torch.load(ckpt_path, map_location=DEVICE)                     # older torch\n",
        "    m.load_state_dict(state)\n",
        "    return m\n",
        "\n",
        "def damage_neurons_in_layer(model: nn.Module, layer_lin_seq_index: int, pct_neurons: float, rng: np.random.Generator):\n",
        "    \"\"\"\n",
        "    Damage a percentage of *neurons* in a given Linear layer:\n",
        "    - Zero the corresponding rows of this layer's weight (incoming weights) and its bias.\n",
        "    - Also zero the corresponding *columns* in the *next* Linear layer (outgoing connections),\n",
        "      if a next Linear layer exists.\n",
        "    Returns list of damaged neuron indices (relative to that layer).\n",
        "    \"\"\"\n",
        "    linear_layers = get_linear_layers(model)\n",
        "    # find target linear by its seq index inside model.net\n",
        "    target_pos = None\n",
        "    for pos, (seq_i, _) in enumerate(linear_layers):\n",
        "        if seq_i == layer_lin_seq_index:\n",
        "            target_pos = pos\n",
        "            break\n",
        "    assert target_pos is not None, f\"No nn.Linear at seq index {layer_lin_seq_index} in model.net\"\n",
        "\n",
        "    target_seq_i, target_lin = linear_layers[target_pos]\n",
        "    out_feats = target_lin.out_features\n",
        "    n_dmg = max(1, int(round(pct_neurons * out_feats)))\n",
        "    dmg_neurons = rng.choice(out_feats, size=n_dmg, replace=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # zero incoming weights + bias for damaged neurons\n",
        "        W = target_lin.weight  # [out_features, in_features]\n",
        "        W[dmg_neurons, :] = 0.0\n",
        "        if target_lin.bias is not None:\n",
        "            target_lin.bias[dmg_neurons] = 0.0\n",
        "\n",
        "        # zero outgoing connections in the next linear (columns)\n",
        "        if target_pos + 1 < len(linear_layers):\n",
        "            _, next_lin = linear_layers[target_pos + 1]\n",
        "            next_lin.weight[:, dmg_neurons] = 0.0\n",
        "\n",
        "    return dmg_neurons.tolist()\n",
        "\n",
        "def damage_weights_in_layer(model: nn.Module, layer_lin_seq_index: int, pct_weights: float, rng: np.random.Generator, mode=\"random\"):\n",
        "    \"\"\"\n",
        "    Damage a percentage of *weights* in a given Linear layer:\n",
        "    - mode='random': independent random mask over all weights\n",
        "    - mode='block' : zero a contiguous rectangular block (for 'specific area' effect)\n",
        "    Returns number of weights zeroed.\n",
        "    \"\"\"\n",
        "    target_lin = None\n",
        "    for seq_i, lin in get_linear_layers(model):\n",
        "        if seq_i == layer_lin_seq_index:\n",
        "            target_lin = lin\n",
        "            break\n",
        "    assert target_lin is not None, f\"No nn.Linear at seq index {layer_lin_seq_index} in model.net\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        W = target_lin.weight  # [out_features, in_features]\n",
        "        H, K = W.shape\n",
        "        total = H * K\n",
        "        n_dmg = max(1, int(round(pct_weights * total)))\n",
        "\n",
        "        if mode == \"random\":\n",
        "            idx = rng.choice(total, size=n_dmg, replace=False)\n",
        "            rows = (idx // K).astype(int)\n",
        "            cols = (idx % K).astype(int)\n",
        "            W[rows, cols] = 0.0\n",
        "        elif mode == \"block\":\n",
        "            # choose a block whose area ≈ n_dmg\n",
        "            h = max(1, int(round(np.sqrt(n_dmg))))\n",
        "            k = max(1, int(round(n_dmg / h)))\n",
        "            r0 = int(rng.integers(0, max(1, H - h + 1)))\n",
        "            c0 = int(rng.integers(0, max(1, K - k + 1)))\n",
        "            W[r0:r0+h, c0:c0+k] = 0.0\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'random' or 'block'\")\n",
        "\n",
        "    return n_dmg\n",
        "\n",
        "def eval_task(model_ctor, ckpt_path, task, data_dir, damage_fn=None, repeats=1, **damage_kwargs):\n",
        "    \"\"\"\n",
        "    Load a fresh model from checkpoint, optionally apply damage cumulatively, and evaluate after each step.\n",
        "    Returns: list of dicts with keys {'repeat': int, 'metrics': {...}} where repeat=0 is the baseline.\n",
        "    \"\"\"\n",
        "    set_seed(42)\n",
        "    if task == \"clf\":\n",
        "        _, _, test_loader, _ = make_loaders(data_dir, \"clf\", batch_size=256)\n",
        "    else:\n",
        "        _, _, test_loader, _ = make_loaders(data_dir, \"reg\", batch_size=512)\n",
        "\n",
        "    # baseline\n",
        "    model = clone_from_checkpoint(model_ctor, ckpt_path)\n",
        "    out = [{\"repeat\": 0, \"metrics\": evaluate_model(model, test_loader, task)}]\n",
        "\n",
        "    if damage_fn is None or repeats <= 0:\n",
        "        return out\n",
        "\n",
        "    rng = np.random.default_rng(12345)\n",
        "    # progressive (cumulative) damage\n",
        "    for r in range(1, repeats + 1):\n",
        "        damage_fn(model, **damage_kwargs, rng=rng)\n",
        "        m = evaluate_model(model, test_loader, task)\n",
        "        out.append({\"repeat\": r, \"metrics\": m})\n",
        "    return out\n",
        "\n",
        "def save_experiment_log(rows, csv_name=\"damage_immediate_drop.csv\"):\n",
        "    \"\"\"\n",
        "    Append rows to a results CSV in RESULTS_DIR.\n",
        "    Each row should already be a flat dict (add your own metadata before calling).\n",
        "    \"\"\"\n",
        "    for row in rows:\n",
        "        log_result(row, csv_name=csv_name)  # uses RESULTS_DIR internally\n",
        "    return str(Path(RESULTS_DIR) / csv_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi2TA-LEgHBi",
        "outputId": "925b91f1-fa9a-499e-9c99-c3b0c52cd584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CLF_CKPT: c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf_breast_cancer_baseline_20250813-103737_best.pt\n",
            "REG_CKPT: c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg_california_baseline_20250813-104035_best.pt\n",
            "Classifier linear layers: [{'seq_index': 0, 'in_features': 30, 'out_features': 64}, {'seq_index': 3, 'in_features': 64, 'out_features': 64}, {'seq_index': 6, 'in_features': 64, 'out_features': 32}, {'seq_index': 9, 'in_features': 32, 'out_features': 2}]\n",
            "Regression linear layers: [{'seq_index': 0, 'in_features': 8, 'out_features': 128}, {'seq_index': 3, 'in_features': 128, 'out_features': 64}, {'seq_index': 6, 'in_features': 64, 'out_features': 32}, {'seq_index': 9, 'in_features': 32, 'out_features': 1}]\n"
          ]
        }
      ],
      "source": [
        "# === CONFIG: checkpoint paths & model-ctor lambdas \n",
        "# RERUN-AFTER-RESTART\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# Use project-local data and models dirs defined earlier\n",
        "BREAST_DIR = DATA_DIR / \"breast_cancer\"\n",
        "CAL_DIR    = DATA_DIR / \"california_housing\"\n",
        "\n",
        "def latest_ckpt(pattern: str) -> Path:\n",
        "    \"\"\"Pick the most recently modified checkpoint matching a pattern.\"\"\"\n",
        "    candidates = sorted(Path(MODELS_DIR).glob(pattern))\n",
        "    assert candidates, f\"No checkpoints matching pattern: {pattern}\"\n",
        "    # choose by modification time\n",
        "    return max(candidates, key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "# Automatically pick the latest baseline checkpoints \n",
        "CLF_CKPT = latest_ckpt(\"clf_breast_cancer_baseline_*_best.pt\")\n",
        "REG_CKPT = latest_ckpt(\"reg_california_baseline_*_best.pt\")\n",
        "\n",
        "# Infer input dims from saved arrays\n",
        "n_in_clf = np.load(BREAST_DIR / \"X_train.npy\").shape[1]\n",
        "n_in_reg = np.load(CAL_DIR    / \"X_train.npy\").shape[1]\n",
        "\n",
        "# Recreate model-ctor lambdas (match the baseline architectures you trained)\n",
        "clf_ctor = lambda: FFNN_Classifier(n_in=n_in_clf, hidden=[64,64,32], n_out=2, p_drop=0.1)\n",
        "reg_ctor = lambda: FFNN_Regression(n_in=n_in_reg, hidden=[128,64,32], p_drop=0.1)\n",
        "\n",
        "print(\"CLF_CKPT:\", CLF_CKPT)\n",
        "print(\"REG_CKPT:\", REG_CKPT)\n",
        "\n",
        "# Inspect linear layers so we know which seq_index to target for damage\n",
        "print(\"Classifier linear layers:\", summarize_linear_layers(clf_ctor()))\n",
        "print(\"Regression linear layers:\", summarize_linear_layers(reg_ctor()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWUd9KR5hJDN",
        "outputId": "e900a15f-3b16-4bd2-caa4-5f8f775ea836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repeat 0: ACC=0.9535, F1=0.9630\n",
            "Repeat 1: ACC=0.9419, F1=0.9550\n",
            "Repeat 2: ACC=0.9070, F1=0.9259\n",
            "Repeat 3: ACC=0.8837, F1=0.9057\n",
            "Repeat 4: ACC=0.8837, F1=0.9038\n",
            "Repeat 5: ACC=0.8721, F1=0.8991\n",
            "Logged to: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_immediate_drop.csv\n"
          ]
        }
      ],
      "source": [
        "#rerun when needed\n",
        "# SANITY EXPERIMENT: classification, 20% neurons in first hidden, 5 repeats ===\n",
        "set_seed(42)\n",
        "\n",
        "# first Linear layer's seq index (from your summary, it's 0)\n",
        "FIRST_LINEAR = summarize_linear_layers(clone_from_checkpoint(clf_ctor, CLF_CKPT))[0][\"seq_index\"]\n",
        "\n",
        "rows_to_log = []\n",
        "res = eval_task(\n",
        "    model_ctor=clf_ctor,\n",
        "    ckpt_path=CLF_CKPT,\n",
        "    task=\"clf\",\n",
        "    data_dir=BREAST_DIR,            # ← use local data dir\n",
        "    damage_fn=damage_neurons_in_layer,\n",
        "    repeats=5,                      # progressive (cumulative) damage\n",
        "    layer_lin_seq_index=FIRST_LINEAR,\n",
        "    pct_neurons=0.20                # 20% neurons\n",
        ")\n",
        "\n",
        "for step in res:\n",
        "    r = step[\"repeat\"]; m = step[\"metrics\"]\n",
        "    print(f\"Repeat {r}: ACC={m['accuracy']:.4f}, F1={m['f1']:.4f}\")\n",
        "    rows_to_log.append({\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"damage_immediate\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"damage_type\": \"neurons\",\n",
        "        \"layer_seq_index\": FIRST_LINEAR,\n",
        "        \"pct\": 0.20,\n",
        "        \"repeat\": r,\n",
        "        \"test_accuracy\": m[\"accuracy\"],\n",
        "        \"test_f1\": m[\"f1\"]\n",
        "    })\n",
        "\n",
        "csv_path = save_experiment_log(rows_to_log, \"damage_immediate_drop.csv\")\n",
        "print(\"Logged to:\", csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Go_3K0XhoMJ",
        "outputId": "957124f5-b02c-4c19-e2b9-b6fd0ac7d72f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log path exists: True | path: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_immediate_drop.csv\n",
            "Last 8 rows:\n",
            "         timestamp             phase        dataset task damage_type  \\\n",
            "0  20250813-104930  damage_immediate  breast_cancer  clf     neurons   \n",
            "1  20250813-104930  damage_immediate  breast_cancer  clf     neurons   \n",
            "2  20250813-104930  damage_immediate  breast_cancer  clf     neurons   \n",
            "3  20250813-104930  damage_immediate  breast_cancer  clf     neurons   \n",
            "4  20250813-104930  damage_immediate  breast_cancer  clf     neurons   \n",
            "5  20250813-104930  damage_immediate  breast_cancer  clf     neurons   \n",
            "\n",
            "   layer_seq_index  pct  repeat  test_accuracy   test_f1  \n",
            "0                0  0.2       0       0.953488  0.962963  \n",
            "1                0  0.2       1       0.941860  0.954955  \n",
            "2                0  0.2       2       0.906977  0.925926  \n",
            "3                0  0.2       3       0.883721  0.905660  \n",
            "4                0  0.2       4       0.883721  0.903846  \n",
            "5                0  0.2       5       0.872093  0.899083  \n"
          ]
        }
      ],
      "source": [
        "# View damage log \n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "csv = Path(RESULTS_DIR) / \"damage_immediate_drop.csv\"\n",
        "print(\"Log path exists:\", csv.exists(), \"| path:\", csv)\n",
        "df = pd.read_csv(csv)\n",
        "print(\"Last 8 rows:\")\n",
        "print(df.tail(8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHo946-uiHV_",
        "outputId": "7f6fca40-179c-461a-f09a-340b8c9c769c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repeat 0: MAE=0.3749, RMSE=0.5373\n",
            "Repeat 1: MAE=0.5098, RMSE=0.7588\n",
            "Repeat 2: MAE=0.5256, RMSE=0.7507\n",
            "Repeat 3: MAE=0.5723, RMSE=0.8086\n",
            "Repeat 4: MAE=0.5954, RMSE=0.8313\n",
            "Repeat 5: MAE=0.6137, RMSE=0.9004\n",
            "Logged to: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_immediate_drop.csv\n"
          ]
        }
      ],
      "source": [
        "# === SANITY: regression, 20% neurons in first hidden, 5 repeats \n",
        "set_seed(42)\n",
        "\n",
        "FIRST_LINEAR_REG = summarize_linear_layers(clone_from_checkpoint(reg_ctor, REG_CKPT))[0][\"seq_index\"]\n",
        "\n",
        "rows_to_log = []\n",
        "res = eval_task(\n",
        "    model_ctor=reg_ctor,\n",
        "    ckpt_path=REG_CKPT,\n",
        "    task=\"reg\",\n",
        "    data_dir=CAL_DIR,                 # ← use local data dir\n",
        "    damage_fn=damage_neurons_in_layer,\n",
        "    repeats=5,\n",
        "    layer_lin_seq_index=FIRST_LINEAR_REG,\n",
        "    pct_neurons=0.20\n",
        ")\n",
        "\n",
        "for step in res:\n",
        "    r = step[\"repeat\"]; m = step[\"metrics\"]\n",
        "    print(f\"Repeat {r}: MAE={m['mae']:.4f}, RMSE={m['rmse']:.4f}\")\n",
        "    rows_to_log.append({\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"damage_immediate\",\n",
        "        \"dataset\": \"california_housing\",\n",
        "        \"task\": \"reg\",\n",
        "        \"damage_type\": \"neurons\",\n",
        "        \"layer_seq_index\": FIRST_LINEAR_REG,\n",
        "        \"pct\": 0.20,\n",
        "        \"repeat\": r,\n",
        "        \"test_mae\": m[\"mae\"],\n",
        "        \"test_rmse\": m[\"rmse\"]\n",
        "    })\n",
        "\n",
        "csv_path = save_experiment_log(rows_to_log, \"damage_immediate_drop.csv\")\n",
        "print(\"Logged to:\", csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BbFz8O9Bildr"
      },
      "outputs": [],
      "source": [
        "# === 4.4 EXPERIMENT RUNNER (logs tidy rows for each repeat) ===\n",
        "# Tag: RERUN-AFTER-RESTART\n",
        "\n",
        "def layer_seq_indices(model_ctor):\n",
        "    m = model_ctor().to(DEVICE)\n",
        "    return [d[\"seq_index\"] for d in summarize_linear_layers(m)]\n",
        "\n",
        "def run_damage_experiment(\n",
        "    model_ctor, ckpt_path, task, data_dir,\n",
        "    layer_seq_index, pct, repeats_list,\n",
        "    damage_type=\"neurons\", weight_mode=\"random\",\n",
        "    rng_seed=12345, csv_name=\"damage_immediate_drop.csv\"\n",
        "):\n",
        "    \"\"\"\n",
        "    For each repeats in repeats_list:\n",
        "      - reload clean checkpoint\n",
        "      - apply progressive damage (repeats times) on given layer\n",
        "      - evaluate after each repeat\n",
        "      - log baseline + each step, with deltas vs baseline\n",
        "    \"\"\"\n",
        "    assert damage_type in (\"neurons\",\"weights\")\n",
        "    rows = []\n",
        "    for repeats in repeats_list:\n",
        "        if damage_type == \"neurons\":\n",
        "            res = eval_task(\n",
        "                model_ctor=model_ctor, ckpt_path=ckpt_path, task=task, data_dir=data_dir,\n",
        "                damage_fn=damage_neurons_in_layer, repeats=repeats,\n",
        "                layer_lin_seq_index=layer_seq_index, pct_neurons=pct\n",
        "            )\n",
        "        else:\n",
        "            res = eval_task(\n",
        "                model_ctor=model_ctor, ckpt_path=ckpt_path, task=task, data_dir=data_dir,\n",
        "                damage_fn=damage_weights_in_layer, repeats=repeats,\n",
        "                layer_lin_seq_index=layer_seq_index, pct_weights=pct, mode=weight_mode\n",
        "            )\n",
        "\n",
        "        base = res[0][\"metrics\"]\n",
        "        for step in res:\n",
        "            r = step[\"repeat\"]; m = step[\"metrics\"]\n",
        "            row = {\n",
        "                \"timestamp\": timestamp(),\n",
        "                \"phase\": \"damage_immediate\",\n",
        "                \"task\": task,\n",
        "                \"damage_type\": damage_type,\n",
        "                \"layer_seq_index\": int(layer_seq_index),\n",
        "                \"pct\": float(pct),\n",
        "                \"repeats_total\": int(repeats),\n",
        "                \"repeat_eval\": int(r),\n",
        "                \"ckpt\": str(ckpt_path),\n",
        "            }\n",
        "            if task == \"clf\":\n",
        "                row.update({\n",
        "                    \"metric\": \"accuracy\",\n",
        "                    \"baseline_metric\": float(base[\"accuracy\"]),\n",
        "                    \"value\": float(m[\"accuracy\"]),\n",
        "                    \"delta_from_baseline\": float(base[\"accuracy\"] - m[\"accuracy\"]),\n",
        "                    \"f1\": float(m[\"f1\"]),\n",
        "                })\n",
        "            else:\n",
        "                row.update({\n",
        "                    \"metric\": \"rmse\",\n",
        "                    \"baseline_metric\": float(base[\"rmse\"]),\n",
        "                    \"value\": float(m[\"rmse\"]),\n",
        "                    \"delta_from_baseline\": float(m[\"rmse\"] - base[\"rmse\"]),\n",
        "                    \"mae\": float(m[\"mae\"]),\n",
        "                })\n",
        "            rows.append(row)\n",
        "\n",
        "    for row in rows:\n",
        "        log_result(row, csv_name=csv_name)\n",
        "    print(f\"Logged {len(rows)} rows to {RESULTS_DIR / csv_name}\")\n",
        "    return rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sWmZ8FHjDxh",
        "outputId": "10614aa4-8364-4cb4-c038-bcc4f4503276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged 6 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_immediate_drop.csv\n",
            "          timestamp             phase dataset task damage_type  \\\n",
            "12  20250813-105456  damage_immediate     NaN  clf     neurons   \n",
            "13  20250813-105456  damage_immediate     NaN  clf     neurons   \n",
            "14  20250813-105456  damage_immediate     NaN  clf     neurons   \n",
            "15  20250813-105456  damage_immediate     NaN  clf     neurons   \n",
            "16  20250813-105456  damage_immediate     NaN  clf     neurons   \n",
            "17  20250813-105456  damage_immediate     NaN  clf     neurons   \n",
            "\n",
            "    layer_seq_index  pct  repeat  test_accuracy  test_f1  test_mae  test_rmse  \\\n",
            "12                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "13                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "14                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "15                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "16                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "17                0  0.2     NaN            NaN      NaN       NaN        NaN   \n",
            "\n",
            "    repeats_total  repeat_eval  \\\n",
            "12            5.0          0.0   \n",
            "13            5.0          1.0   \n",
            "14            5.0          2.0   \n",
            "15            5.0          3.0   \n",
            "16            5.0          4.0   \n",
            "17            5.0          5.0   \n",
            "\n",
            "                                                 ckpt    metric  \\\n",
            "12  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "13  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "14  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "15  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "16  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "17  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "\n",
            "    baseline_metric     value  delta_from_baseline        f1  \n",
            "12         0.953488  0.953488             0.000000  0.962963  \n",
            "13         0.953488  0.941860             0.011628  0.954955  \n",
            "14         0.953488  0.906977             0.046512  0.925926  \n",
            "15         0.953488  0.883721             0.069767  0.905660  \n",
            "16         0.953488  0.883721             0.069767  0.903846  \n",
            "17         0.953488  0.872093             0.081395  0.899083  \n"
          ]
        }
      ],
      "source": [
        "# CLASSIFICATION: first hidden layer (seq 0), 20% neurons, repeats = [5]\n",
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]  # should be 0\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=first_lin_clf, pct=0.20, repeats_list=[5],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_immediate_drop.csv\"\n",
        ")\n",
        "\n",
        "# quick peek\n",
        "import pandas as pd\n",
        "df = pd.read_csv(RESULTS_DIR / \"damage_immediate_drop.csv\")\n",
        "print(df.tail(6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jf5EV8NVjZAH"
      },
      "outputs": [],
      "source": [
        "# === 4.4 EXPERIMENT RUNNER (with dataset_name + selectable CSV) — VS Code version ===\n",
        "# Tag: RERUN-AFTER-RESTART\n",
        "\n",
        "def run_damage_experiment(\n",
        "    model_ctor, ckpt_path, task, data_dir,\n",
        "    layer_seq_index, pct, repeats_list,\n",
        "    damage_type=\"neurons\", weight_mode=\"random\",\n",
        "    rng_seed=12345, csv_name=\"damage_runs.csv\",\n",
        "    dataset_name=None\n",
        "):\n",
        "    \"\"\"\n",
        "    For each 'repeats' in repeats_list:\n",
        "      - reload clean checkpoint\n",
        "      - apply progressive damage (repeats times) on given layer\n",
        "      - evaluate after each repeat\n",
        "      - log baseline + each step with deltas\n",
        "    \"\"\"\n",
        "    assert damage_type in (\"neurons\", \"weights\")\n",
        "    rows = []\n",
        "    for repeats in repeats_list:\n",
        "        if damage_type == \"neurons\":\n",
        "            res = eval_task(\n",
        "                model_ctor=model_ctor, ckpt_path=ckpt_path, task=task, data_dir=data_dir,\n",
        "                damage_fn=damage_neurons_in_layer, repeats=repeats,\n",
        "                layer_lin_seq_index=layer_seq_index, pct_neurons=pct\n",
        "            )\n",
        "        else:\n",
        "            res = eval_task(\n",
        "                model_ctor=model_ctor, ckpt_path=ckpt_path, task=task, data_dir=data_dir,\n",
        "                damage_fn=damage_weights_in_layer, repeats=repeats,\n",
        "                layer_lin_seq_index=layer_seq_index, pct_weights=pct, mode=weight_mode\n",
        "            )\n",
        "\n",
        "        base = res[0][\"metrics\"]\n",
        "        for step in res:\n",
        "            r = step[\"repeat\"]; m = step[\"metrics\"]\n",
        "            row = {\n",
        "                \"timestamp\": timestamp(),\n",
        "                \"phase\": \"damage_immediate\",\n",
        "                \"dataset\": dataset_name,\n",
        "                \"task\": task,\n",
        "                \"damage_type\": damage_type,\n",
        "                \"layer_seq_index\": int(layer_seq_index),\n",
        "                \"pct\": float(pct),\n",
        "                \"repeats_total\": int(repeats),\n",
        "                \"repeat_eval\": int(r),\n",
        "                \"ckpt\": str(ckpt_path)\n",
        "            }\n",
        "            if task == \"clf\":\n",
        "                row.update({\n",
        "                    \"metric\": \"accuracy\",\n",
        "                    \"baseline_metric\": float(base[\"accuracy\"]),\n",
        "                    \"value\": float(m[\"accuracy\"]),\n",
        "                    \"delta_from_baseline\": float(base[\"accuracy\"] - m[\"accuracy\"]),\n",
        "                    \"f1\": float(m[\"f1\"])\n",
        "                })\n",
        "            else:\n",
        "                row.update({\n",
        "                    \"metric\": \"rmse\",\n",
        "                    \"baseline_metric\": float(base[\"rmse\"]),\n",
        "                    \"value\": float(m[\"rmse\"]),\n",
        "                    \"delta_from_baseline\": float(m[\"rmse\"] - base[\"rmse\"]),\n",
        "                    \"mae\": float(m[\"mae\"])\n",
        "                })\n",
        "            rows.append(row)\n",
        "\n",
        "    for row in rows:\n",
        "        log_result(row, csv_name=csv_name)\n",
        "    print(f\"Logged {len(rows)} rows to {RESULTS_DIR / csv_name}\")\n",
        "    return rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf1iESRgkUfr",
        "outputId": "b0c7cd0b-1f91-4b42-c008-553e24a1aa69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged 6 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "         timestamp             phase        dataset task damage_type  \\\n",
            "0  20250813-110039  damage_immediate  breast_cancer  clf     neurons   \n",
            "1  20250813-110039  damage_immediate  breast_cancer  clf     neurons   \n",
            "2  20250813-110039  damage_immediate  breast_cancer  clf     neurons   \n",
            "3  20250813-110039  damage_immediate  breast_cancer  clf     neurons   \n",
            "4  20250813-110039  damage_immediate  breast_cancer  clf     neurons   \n",
            "5  20250813-110039  damage_immediate  breast_cancer  clf     neurons   \n",
            "\n",
            "   layer_seq_index  pct  repeats_total  repeat_eval  \\\n",
            "0                0  0.2              5            0   \n",
            "1                0  0.2              5            1   \n",
            "2                0  0.2              5            2   \n",
            "3                0  0.2              5            3   \n",
            "4                0  0.2              5            4   \n",
            "5                0  0.2              5            5   \n",
            "\n",
            "                                                ckpt    metric  \\\n",
            "0  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "1  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "2  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "3  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "4  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "5  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "\n",
            "   baseline_metric     value  delta_from_baseline        f1  \n",
            "0         0.953488  0.953488             0.000000  0.962963  \n",
            "1         0.953488  0.941860             0.011628  0.954955  \n",
            "2         0.953488  0.906977             0.046512  0.925926  \n",
            "3         0.953488  0.883721             0.069767  0.905660  \n",
            "4         0.953488  0.883721             0.069767  0.903846  \n",
            "5         0.953488  0.872093             0.081395  0.899083  \n"
          ]
        }
      ],
      "source": [
        "# CLASSIFICATION: first hidden layer (seq 0), 20% neurons, repeats=[5]\n",
        "#run on demand\n",
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]  # seq index 0\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=first_lin_clf, pct=0.20, repeats_list=[5],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\",\n",
        "    dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# quick peek of the appended rows\n",
        "import pandas as pd\n",
        "df_new = pd.read_csv(RESULTS_DIR / \"damage_runs.csv\")\n",
        "print(df_new.tail(6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o90w_UxTkhhf",
        "outputId": "26e3fa7b-a91a-40b3-b344-705a02029eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "          timestamp             phase        dataset task damage_type  \\\n",
            "28  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "29  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "30  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "31  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "32  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "33  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "34  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "35  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "36  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "37  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "38  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "39  20250813-110315  damage_immediate  breast_cancer  clf     neurons   \n",
            "\n",
            "    layer_seq_index  pct  repeats_total  repeat_eval  \\\n",
            "28                0  0.4              5            5   \n",
            "29                0  0.4             10            0   \n",
            "30                0  0.4             10            1   \n",
            "31                0  0.4             10            2   \n",
            "32                0  0.4             10            3   \n",
            "33                0  0.4             10            4   \n",
            "34                0  0.4             10            5   \n",
            "35                0  0.4             10            6   \n",
            "36                0  0.4             10            7   \n",
            "37                0  0.4             10            8   \n",
            "38                0  0.4             10            9   \n",
            "39                0  0.4             10           10   \n",
            "\n",
            "                                                 ckpt    metric  \\\n",
            "28  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "29  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "30  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "31  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "32  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "33  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "34  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "35  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "36  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "37  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "38  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "39  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "\n",
            "    baseline_metric     value  delta_from_baseline        f1  \n",
            "28         0.953488  0.790698             0.162791  0.826923  \n",
            "29         0.953488  0.953488             0.000000  0.962963  \n",
            "30         0.953488  0.930233             0.023256  0.947368  \n",
            "31         0.953488  0.895349             0.058140  0.923077  \n",
            "32         0.953488  0.906977             0.046512  0.931034  \n",
            "33         0.953488  0.802326             0.151163  0.838095  \n",
            "34         0.953488  0.790698             0.162791  0.826923  \n",
            "35         0.953488  0.372093             0.581395  0.000000  \n",
            "36         0.953488  0.372093             0.581395  0.000000  \n",
            "37         0.953488  0.372093             0.581395  0.000000  \n",
            "38         0.953488  0.372093             0.581395  0.000000  \n",
            "39         0.953488  0.372093             0.581395  0.000000  \n"
          ]
        }
      ],
      "source": [
        "#  SMALL BATCH: clf · first hidden · neurons · 20% & 40% · repeats [5, 10]\n",
        "set_seed(42)\n",
        "\n",
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]  # seq index 0\n",
        "\n",
        "# 20% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=first_lin_clf, pct=0.20, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# 40% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=first_lin_clf, pct=0.40, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# quick peek of just these rows\n",
        "import pandas as pd\n",
        "df = pd.read_csv(RESULTS_DIR / \"damage_runs.csv\")\n",
        "mask = (\n",
        "    (df[\"dataset\"] == \"breast_cancer\") &\n",
        "    (df[\"task\"] == \"clf\") &\n",
        "    (df[\"damage_type\"] == \"neurons\") &\n",
        "    (df[\"layer_seq_index\"] == first_lin_clf) &\n",
        "    (df[\"pct\"].isin([0.2, 0.4])) &\n",
        "    (df[\"repeats_total\"].isin([5, 10]))\n",
        ")\n",
        "print(df[mask].tail(12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWWhXjJWk3tx",
        "outputId": "0ebfddce-d86c-4b16-8a37-f23902448716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "          timestamp             phase        dataset task damage_type  \\\n",
            "62  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "63  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "64  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "65  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "66  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "67  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "68  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "69  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "70  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "71  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "72  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "73  20250813-110450  damage_immediate  breast_cancer  clf     neurons   \n",
            "\n",
            "    layer_seq_index  pct  repeats_total  repeat_eval  \\\n",
            "62                9  0.4              5            5   \n",
            "63                9  0.4             10            0   \n",
            "64                9  0.4             10            1   \n",
            "65                9  0.4             10            2   \n",
            "66                9  0.4             10            3   \n",
            "67                9  0.4             10            4   \n",
            "68                9  0.4             10            5   \n",
            "69                9  0.4             10            6   \n",
            "70                9  0.4             10            7   \n",
            "71                9  0.4             10            8   \n",
            "72                9  0.4             10            9   \n",
            "73                9  0.4             10           10   \n",
            "\n",
            "                                                 ckpt    metric  \\\n",
            "62  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "63  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "64  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "65  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "66  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "67  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "68  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "69  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "70  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "71  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "72  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "73  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "\n",
            "    baseline_metric     value  delta_from_baseline        f1  \n",
            "62         0.953488  0.372093             0.581395  0.000000  \n",
            "63         0.953488  0.953488             0.000000  0.962963  \n",
            "64         0.953488  0.965116            -0.011628  0.972973  \n",
            "65         0.953488  0.372093             0.581395  0.000000  \n",
            "66         0.953488  0.372093             0.581395  0.000000  \n",
            "67         0.953488  0.372093             0.581395  0.000000  \n",
            "68         0.953488  0.372093             0.581395  0.000000  \n",
            "69         0.953488  0.372093             0.581395  0.000000  \n",
            "70         0.953488  0.372093             0.581395  0.000000  \n",
            "71         0.953488  0.372093             0.581395  0.000000  \n",
            "72         0.953488  0.372093             0.581395  0.000000  \n",
            "73         0.953488  0.372093             0.581395  0.000000  \n"
          ]
        }
      ],
      "source": [
        "# clf · OUTPUT layer · neurons · 20% & 40% · repeats [5, 10]\n",
        "set_seed(42)\n",
        "\n",
        "last_lin_clf = layer_seq_indices(clf_ctor)[-1]  # seq index of output Linear\n",
        "\n",
        "# 20% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=last_lin_clf, pct=0.20, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# 40% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=last_lin_clf, pct=0.40, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# Quick peek of these specific rows\n",
        "import pandas as pd\n",
        "df = pd.read_csv(RESULTS_DIR / \"damage_runs.csv\")\n",
        "mask = (\n",
        "    (df[\"dataset\"] == \"breast_cancer\") &\n",
        "    (df[\"task\"] == \"clf\") &\n",
        "    (df[\"damage_type\"] == \"neurons\") &\n",
        "    (df[\"layer_seq_index\"] == last_lin_clf) &\n",
        "    (df[\"pct\"].isin([0.2, 0.4])) &\n",
        "    (df[\"repeats_total\"].isin([5, 10]))\n",
        ")\n",
        "print(df[mask].tail(12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmY7a62TlROD",
        "outputId": "9470d3cc-95ce-4817-9532-9b69df201670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "           timestamp             phase             dataset task damage_type  \\\n",
            "96   20250813-110723  damage_immediate  california_housing  reg     neurons   \n",
            "97   20250813-110724  damage_immediate  california_housing  reg     neurons   \n",
            "98   20250813-110724  damage_immediate  california_housing  reg     neurons   \n",
            "99   20250813-110724  damage_immediate  california_housing  reg     neurons   \n",
            "100  20250813-110724  damage_immediate  california_housing  reg     neurons   \n",
            "101  20250813-110724  damage_immediate  california_housing  reg     neurons   \n",
            "102  20250813-110724  damage_immediate  california_housing  reg     neurons   \n",
            "103  20250813-110724  damage_immediate  california_housing  reg     neurons   \n",
            "104  20250813-110724  damage_immediate  california_housing  reg     neurons   \n",
            "105  20250813-110724  damage_immediate  california_housing  reg     neurons   \n",
            "106  20250813-110724  damage_immediate  california_housing  reg     neurons   \n",
            "107  20250813-110724  damage_immediate  california_housing  reg     neurons   \n",
            "\n",
            "     layer_seq_index  pct  repeats_total  repeat_eval  \\\n",
            "96                 0  0.4              5            5   \n",
            "97                 0  0.4             10            0   \n",
            "98                 0  0.4             10            1   \n",
            "99                 0  0.4             10            2   \n",
            "100                0  0.4             10            3   \n",
            "101                0  0.4             10            4   \n",
            "102                0  0.4             10            5   \n",
            "103                0  0.4             10            6   \n",
            "104                0  0.4             10            7   \n",
            "105                0  0.4             10            8   \n",
            "106                0  0.4             10            9   \n",
            "107                0  0.4             10           10   \n",
            "\n",
            "                                                  ckpt metric  \\\n",
            "96   c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "97   c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "98   c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "99   c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "100  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "101  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "102  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "103  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "104  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "105  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "106  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "107  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\reg...   rmse   \n",
            "\n",
            "     baseline_metric     value  delta_from_baseline  f1       mae  \n",
            "96          0.537336  1.257674             0.720338 NaN  0.905047  \n",
            "97          0.537336  0.537336             0.000000 NaN  0.374850  \n",
            "98          0.537336  0.644867             0.107531 NaN  0.482532  \n",
            "99          0.537336  0.859450             0.322114 NaN  0.591337  \n",
            "100         0.537336  1.071479             0.534144 NaN  0.765762  \n",
            "101         0.537336  1.176146             0.638811 NaN  0.840240  \n",
            "102         0.537336  1.257674             0.720338 NaN  0.905047  \n",
            "103         0.537336  1.247198             0.709863 NaN  0.899043  \n",
            "104         0.537336  1.239897             0.702562 NaN  0.901800  \n",
            "105         0.537336  1.253172             0.715837 NaN  0.913375  \n",
            "106         0.537336  1.253172             0.715837 NaN  0.913375  \n",
            "107         0.537336  1.253172             0.715837 NaN  0.913375  \n"
          ]
        }
      ],
      "source": [
        "# SMALL BATCH: reg · first hidden · neurons · 20% & 40% · repeats [5, 10]\n",
        "set_seed(42)\n",
        "\n",
        "first_lin_reg = layer_seq_indices(reg_ctor)[0]  # seq index 0\n",
        "\n",
        "# 20% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=CAL_DIR,\n",
        "    layer_seq_index=first_lin_reg, pct=0.20, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"california_housing\"\n",
        ")\n",
        "\n",
        "# 40% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=CAL_DIR,\n",
        "    layer_seq_index=first_lin_reg, pct=0.40, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"california_housing\"\n",
        ")\n",
        "\n",
        "# Quick peek of just these rows\n",
        "import pandas as pd\n",
        "df = pd.read_csv(RESULTS_DIR / \"damage_runs.csv\")\n",
        "mask = (\n",
        "    (df[\"dataset\"] == \"california_housing\") &\n",
        "    (df[\"task\"] == \"reg\") &\n",
        "    (df[\"damage_type\"] == \"neurons\") &\n",
        "    (df[\"layer_seq_index\"] == first_lin_reg) &\n",
        "    (df[\"pct\"].isin([0.2, 0.4])) &\n",
        "    (df[\"repeats_total\"].isin([5, 10]))\n",
        ")\n",
        "print(df[mask].tail(12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "s5smUJZOlz40"
      },
      "outputs": [],
      "source": [
        "#  Healing scaffolding (gradient masks + masked damage) \n",
        "#rerun\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Reuse: get_linear_layers, summarize_linear_layers, clone_from_checkpoint,\n",
        "# make_loaders, evaluate_model, set_seed, DEVICE, MODELS_DIR already defined.\n",
        "\n",
        "def _ensure_mask(t, fill=1.0):\n",
        "    \"\"\"Create a float mask same shape as tensor t, filled with fill (1.0 = trainable, 0.0 = freeze).\"\"\"\n",
        "    return torch.full_like(t, float(fill))\n",
        "\n",
        "def combine_mask_(base_mask: torch.Tensor, new_mask: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Combine two masks in-place using AND semantics for freezing:\n",
        "    final_keep = base_keep * new_keep\n",
        "    \"\"\"\n",
        "    base_mask.mul_(new_mask)\n",
        "    return base_mask\n",
        "\n",
        "def register_grad_mask_hooks(mask_dict):\n",
        "    \"\"\"\n",
        "    Given {param_tensor: mask_tensor}, register a backward hook that multiplies incoming gradients by the mask.\n",
        "    Keep the returned handles alive; call handle.remove() when done.\n",
        "    \"\"\"\n",
        "    hooks = []\n",
        "    for p, m in mask_dict.items():\n",
        "        mm = m.to(p.device).to(p.dtype)\n",
        "        h = p.register_hook(lambda g, _mm=mm: g * _mm)\n",
        "        hooks.append(h)\n",
        "    return hooks\n",
        "\n",
        "def init_full_keep_masks_for_model(model: nn.Module):\n",
        "    \"\"\"\n",
        "    Build an initial mask dict of ones (keep/train) for each Linear weight/bias.\n",
        "    We will AND (multiply) zeros into these masks as we damage more.\n",
        "    \"\"\"\n",
        "    mask_dict = {}\n",
        "    for _, m in model.named_modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            mask_dict[m.weight] = _ensure_mask(m.weight, fill=1.0)\n",
        "            if m.bias is not None:\n",
        "                mask_dict[m.bias] = _ensure_mask(m.bias, fill=1.0)\n",
        "    return mask_dict\n",
        "\n",
        "def damage_neurons_in_layer_with_masks(model: nn.Module, layer_lin_seq_index: int, pct_neurons: float,\n",
        "                                       rng: np.random.Generator, mask_dict=None):\n",
        "    \"\"\"\n",
        "    Same as damage_neurons_in_layer, but also updates a mask_dict so damaged entries are frozen.\n",
        "    - Zero rows in target Linear weight + corresponding bias entries.\n",
        "    - Zero columns in the next Linear weight (outgoing).\n",
        "    Returns: (damaged_neuron_indices, mask_dict)\n",
        "    \"\"\"\n",
        "    linear_layers = get_linear_layers(model)\n",
        "    target_pos = None\n",
        "    for k, (seq_i, lin) in enumerate(linear_layers):\n",
        "        if seq_i == layer_lin_seq_index:\n",
        "            target_pos = k\n",
        "            break\n",
        "    assert target_pos is not None, f\"No Linear layer at seq index {layer_lin_seq_index}\"\n",
        "    _, target_lin = linear_layers[target_pos]\n",
        "    next_lin = linear_layers[target_pos + 1][1] if (target_pos + 1) < len(linear_layers) else None\n",
        "\n",
        "    out_feats = target_lin.out_features\n",
        "    n_dmg = max(1, int(round(pct_neurons * out_feats)))\n",
        "    dmg_neurons = rng.choice(out_feats, size=n_dmg, replace=False)\n",
        "\n",
        "    if mask_dict is None:\n",
        "        mask_dict = init_full_keep_masks_for_model(model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Zero rows in target weight + target bias at damaged neurons; mark them frozen (mask=0)\n",
        "        W = target_lin.weight  # [out, in]\n",
        "        W[dmg_neurons, :] = 0.0\n",
        "        maskW = mask_dict.get(W, _ensure_mask(W, 1.0)); maskW[dmg_neurons, :] = 0.0; mask_dict[W] = maskW\n",
        "\n",
        "        if target_lin.bias is not None:\n",
        "            b = target_lin.bias\n",
        "            b[dmg_neurons] = 0.0\n",
        "            maskb = mask_dict.get(b, _ensure_mask(b, 1.0)); maskb[dmg_neurons] = 0.0; mask_dict[b] = maskb\n",
        "\n",
        "        # Zero outgoing columns in next layer's weight; mark them frozen\n",
        "        if next_lin is not None:\n",
        "            Wn = next_lin.weight  # [next_out, next_in]\n",
        "            Wn[:, dmg_neurons] = 0.0\n",
        "            maskWn = mask_dict.get(Wn, _ensure_mask(Wn, 1.0)); maskWn[:, dmg_neurons] = 0.0; mask_dict[Wn] = maskWn\n",
        "\n",
        "    return dmg_neurons.tolist(), mask_dict\n",
        "\n",
        "def apply_progressive_neuron_damage_with_masks(model_ctor, ckpt_path, task, data_dir,\n",
        "                                               layer_lin_seq_index, pct_neurons, repeats, seed=12345):\n",
        "    \"\"\"\n",
        "    Load clean model, apply progressive neuron damage (repeats times),\n",
        "    accumulating both damage and masks.\n",
        "    Returns: model_damaged, mask_dict, train_loader, val_loader, test_loader, metrics_before, metrics_after\n",
        "    \"\"\"\n",
        "    if task == \"clf\":\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"clf\", batch_size=256)\n",
        "    else:\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"reg\", batch_size=512)\n",
        "\n",
        "    model = clone_from_checkpoint(model_ctor, ckpt_path)\n",
        "    metrics_before = evaluate_model(model, test_loader, task)\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    mask_dict = init_full_keep_masks_for_model(model)\n",
        "\n",
        "    for _ in range(repeats):\n",
        "        _, mask_dict = damage_neurons_in_layer_with_masks(\n",
        "            model, layer_lin_seq_index=layer_lin_seq_index, pct_neurons=pct_neurons, rng=rng, mask_dict=mask_dict\n",
        "        )\n",
        "\n",
        "    metrics_after = evaluate_model(model, test_loader, task)\n",
        "    return model, mask_dict, train_loader, val_loader, test_loader, metrics_before, metrics_after\n",
        "\n",
        "def healing_train_constrained(model, mask_dict, train_loader, val_loader, task,\n",
        "                              max_epochs=50, lr=1e-3, weight_decay=1e-4, patience=10, run_name=\"heal\"):\n",
        "    \"\"\"\n",
        "    Retrain while freezing masked entries (0=freeze). We enforce freezing by:\n",
        "      1) multiplying grads by mask (hook), AND\n",
        "      2) post-step: p.data *= mask  (prevents weight_decay/optimizer from moving frozen params)\n",
        "    Returns: best_path, history\n",
        "    \"\"\"\n",
        "    model = model.to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss() if task == \"clf\" else nn.MSELoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    # register grad-mask hooks\n",
        "    hooks = register_grad_mask_hooks(mask_dict)\n",
        "\n",
        "    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"val_metric\": []}\n",
        "    best_val = float(\"inf\")\n",
        "    best_path = Path(MODELS_DIR) / f\"{run_name}_{timestamp()}_best.pt\"\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        model.train(); train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb if task == \"clf\" else yb)\n",
        "            loss.backward()\n",
        "\n",
        "            # standard optimizer step\n",
        "            opt.step()\n",
        "\n",
        "            # POST-STEP ENFORCEMENT: keep frozen entries at their masked values (0)\n",
        "            with torch.no_grad():\n",
        "                for p, m in mask_dict.items():\n",
        "                    p.data.mul_(m.to(p.device).to(p.dtype))\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "        train_loss = float(np.mean(train_losses)) if train_losses else 0.0\n",
        "\n",
        "        # validate\n",
        "        model.eval(); val_losses = []; y_true_list, y_pred_list = [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                out = model(xb)\n",
        "                vloss = criterion(out, yb if task == \"clf\" else yb)\n",
        "                val_losses.append(vloss.item())\n",
        "                if task == \"clf\":\n",
        "                    y_pred_list.append(out.argmax(dim=1).cpu().numpy()); y_true_list.append(yb.cpu().numpy())\n",
        "                else:\n",
        "                    y_pred_list.append(out.cpu().numpy().squeeze()); y_true_list.append(yb.cpu().numpy().squeeze())\n",
        "\n",
        "        val_loss = float(np.mean(val_losses)) if val_losses else 0.0\n",
        "        if task == \"clf\":\n",
        "            import numpy as _np\n",
        "            y_true = _np.concatenate(y_true_list); y_pred = _np.concatenate(y_pred_list)\n",
        "            from sklearn.metrics import accuracy_score as _acc\n",
        "            val_metric = float(_acc(y_true, y_pred))\n",
        "        else:\n",
        "            import numpy as _np, math as _math\n",
        "            from sklearn.metrics import mean_squared_error as _mse\n",
        "            y_true = _np.array(_np.concatenate([_np.atleast_1d(a) for a in y_true_list]))\n",
        "            y_pred = _np.array(_np.concatenate([_np.atleast_1d(a) for a in y_pred_list]))\n",
        "            val_metric = float(_math.sqrt(_mse(y_true, y_pred)))\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history[\"epoch\"].append(epoch); history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss); history[\"val_metric\"].append(val_metric)\n",
        "\n",
        "        # save best by val_loss\n",
        "        if val_loss < best_val - 1e-8:\n",
        "            best_val = val_loss; epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), best_path); mark = \"*\"\n",
        "        else:\n",
        "            epochs_no_improve += 1; mark = \"\"\n",
        "        if epoch % 10 == 0 or mark == \"*\":\n",
        "            print(f\"[{run_name}] epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | metric {val_metric:.4f} {mark}\")\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"[{run_name}] Early stopping at epoch {epoch}. Best val_loss={best_val:.4f}\")\n",
        "            break\n",
        "\n",
        "    # clean up hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "    return best_path, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nlrjimSmjZt",
        "outputId": "909371e8-bfab-4678-cc41-29f31f51476b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASELINE: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "POST-DAMAGE: {'accuracy': 0.872093023255814, 'f1': 0.8990825688073395}\n"
          ]
        }
      ],
      "source": [
        "#rerun\n",
        "# create a damaged model + masks (clf, first hidden, 20%, 5 repeats)\n",
        "set_seed(42)\n",
        "\n",
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]  # seq index 0\n",
        "\n",
        "damaged_model, mask_dict, train_loader, val_loader, test_loader, m_before, m_after = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=BREAST_DIR,          # ← local data dir\n",
        "        layer_lin_seq_index=first_lin_clf,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "print(\"BASELINE:\", m_before)\n",
        "print(\"POST-DAMAGE:\", m_after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8ifyS2gm3mc",
        "outputId": "a38d5833-2afc-4014-9146-38acb1d6d2ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_clf_firstHidden_p20_r5_short] epoch 001 | train 0.1583 | val 0.1570 | metric 0.9412 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 002 | train 0.1435 | val 0.1386 | metric 0.9529 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 003 | train 0.1475 | val 0.1244 | metric 0.9529 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 004 | train 0.1060 | val 0.1126 | metric 0.9647 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 005 | train 0.1099 | val 0.1030 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 006 | train 0.0790 | val 0.0950 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 007 | train 0.0938 | val 0.0895 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 008 | train 0.0886 | val 0.0849 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 009 | train 0.0716 | val 0.0820 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 010 | train 0.0671 | val 0.0788 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 011 | train 0.0817 | val 0.0766 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 012 | train 0.0675 | val 0.0754 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 013 | train 0.0651 | val 0.0747 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_short] epoch 014 | train 0.0744 | val 0.0741 | metric 0.9765 *\n",
            "SHORT HEAL (≈15 epochs) TEST: {'accuracy': 0.9302325581395349, 'f1': 0.9444444444444444}\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 001 | train 0.1274 | val 0.1551 | metric 0.9529 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 002 | train 0.1160 | val 0.1346 | metric 0.9529 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 003 | train 0.1370 | val 0.1195 | metric 0.9647 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 004 | train 0.1067 | val 0.1074 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 005 | train 0.1072 | val 0.0976 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 006 | train 0.0985 | val 0.0897 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 007 | train 0.0835 | val 0.0834 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 008 | train 0.0857 | val 0.0793 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 009 | train 0.0849 | val 0.0763 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 010 | train 0.0902 | val 0.0739 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 011 | train 0.0935 | val 0.0722 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 012 | train 0.0697 | val 0.0710 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 013 | train 0.0862 | val 0.0704 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 020 | train 0.0648 | val 0.0709 | metric 0.9765 \n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 021 | train 0.0636 | val 0.0701 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 022 | train 0.0711 | val 0.0694 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 023 | train 0.0502 | val 0.0692 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 024 | train 0.0601 | val 0.0690 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 025 | train 0.0658 | val 0.0687 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 026 | train 0.0719 | val 0.0686 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 030 | train 0.0699 | val 0.0700 | metric 0.9765 \n",
            "[heal_clf_firstHidden_p20_r5_long] epoch 040 | train 0.0480 | val 0.0731 | metric 0.9765 \n",
            "[heal_clf_firstHidden_p20_r5_long] Early stopping at epoch 46. Best val_loss=0.0686\n",
            "LONG HEAL (≈150 epochs) TEST: {'accuracy': 0.9651162790697675, 'f1': 0.9719626168224299}\n",
            "Logged 4 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n",
            "Short heal plots: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_firstHidden_p20_r5_short_loss.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_firstHidden_p20_r5_short_accuracy.png\n",
            "Long heal plots: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_firstHidden_p20_r5_long_loss.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_firstHidden_p20_r5_long_accuracy.png\n"
          ]
        }
      ],
      "source": [
        "#rerun\n",
        "# heal (short vs long), evaluate, save plots, and log\n",
        "\n",
        "# ---- SHORT HEAL (≈15 epochs) ----\n",
        "short_name = \"heal_clf_firstHidden_p20_r5_short\"\n",
        "best_short_path, hist_short = healing_train_constrained(\n",
        "    model=damaged_model,\n",
        "    mask_dict=mask_dict,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    task=\"clf\",\n",
        "    max_epochs=15,          # quick heal\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=5,\n",
        "    run_name=short_name\n",
        ")\n",
        "\n",
        "# Load the best short-healed weights into a fresh model and evaluate on TEST\n",
        "m_short = clf_ctor().to(DEVICE)\n",
        "m_short.load_state_dict(torch.load(best_short_path, map_location=DEVICE))\n",
        "metrics_short = evaluate_model(m_short, test_loader, task=\"clf\")\n",
        "print(\"SHORT HEAL (≈15 epochs) TEST:\", metrics_short)\n",
        "\n",
        "# Plot short-heal curves\n",
        "fig_loss_s, fig_metric_s = plot_history(hist_short, short_name, task=\"clf\")\n",
        "\n",
        "# ---- LONG HEAL (≈150 epochs) ----\n",
        "# IMPORTANT: start from the same post-damage state again (not from the short-healed model)\n",
        "damaged_model2, mask_dict2, train_loader2, val_loader2, test_loader2, _, _ = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=BREAST_DIR,          # ← local data dir\n",
        "        layer_lin_seq_index=first_lin_clf,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "long_name = \"heal_clf_firstHidden_p20_r5_long\"\n",
        "best_long_path, hist_long = healing_train_constrained(\n",
        "    model=damaged_model2,\n",
        "    mask_dict=mask_dict2,\n",
        "    train_loader=train_loader2,\n",
        "    val_loader=val_loader2,\n",
        "    task=\"clf\",\n",
        "    max_epochs=150,         # longer heal\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=20,\n",
        "    run_name=long_name\n",
        ")\n",
        "\n",
        "m_long = clf_ctor().to(DEVICE)\n",
        "m_long.load_state_dict(torch.load(best_long_path, map_location=DEVICE))\n",
        "metrics_long = evaluate_model(m_long, test_loader2, task=\"clf\")\n",
        "print(\"LONG HEAL (≈150 epochs) TEST:\", metrics_long)\n",
        "\n",
        "# Plot long-heal curves\n",
        "fig_loss_l, fig_metric_l = plot_history(hist_long, long_name, task=\"clf\")\n",
        "\n",
        "# ---- LOG everything to a separate healing CSV ----\n",
        "rows = [\n",
        "    {\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"healing\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"stage\": \"baseline\",\n",
        "        \"layer_seq_index\": int(first_lin_clf),\n",
        "        \"pct\": 0.20,\n",
        "        \"repeats\": 5,\n",
        "        \"accuracy\": float(m_before[\"accuracy\"]),\n",
        "        \"f1\": float(m_before[\"f1\"])\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"healing\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"stage\": \"post_damage\",\n",
        "        \"layer_seq_index\": int(first_lin_clf),\n",
        "        \"pct\": 0.20,\n",
        "        \"repeats\": 5,\n",
        "        \"accuracy\": float(m_after[\"accuracy\"]),\n",
        "        \"f1\": float(m_after[\"f1\"])\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"healing\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"stage\": \"healed_short\",\n",
        "        \"layer_seq_index\": int(first_lin_clf),\n",
        "        \"pct\": 0.20,\n",
        "        \"repeats\": 5,\n",
        "        \"epochs\": len(hist_short[\"epoch\"]),\n",
        "        \"accuracy\": float(metrics_short[\"accuracy\"]),\n",
        "        \"f1\": float(metrics_short[\"f1\"]),\n",
        "        \"best_ckpt\": str(best_short_path),\n",
        "        \"fig_loss\": fig_loss_s,\n",
        "        \"fig_metric\": fig_metric_s\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"healing\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"stage\": \"healed_long\",\n",
        "        \"layer_seq_index\": int(first_lin_clf),\n",
        "        \"pct\": 0.20,\n",
        "        \"repeats\": 5,\n",
        "        \"epochs\": len(hist_long[\"epoch\"]),\n",
        "        \"accuracy\": float(metrics_long[\"accuracy\"]),\n",
        "        \"f1\": float(metrics_long[\"f1\"]),\n",
        "        \"best_ckpt\": str(best_long_path),\n",
        "        \"fig_loss\": fig_loss_l,\n",
        "        \"fig_metric\": fig_metric_l\n",
        "    }\n",
        "]\n",
        "\n",
        "for r in rows:\n",
        "    log_result(r, csv_name=\"healing_runs.csv\")\n",
        "\n",
        "print(\"Logged 4 rows to\", RESULTS_DIR / \"healing_runs.csv\")\n",
        "print(\"Short heal plots:\", fig_loss_s, \"|\", fig_metric_s)\n",
        "print(\"Long heal plots:\", fig_loss_l, \"|\", fig_metric_l)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX1EKhTBnSRN",
        "outputId": "3bdd75d4-6a3e-45a9-a769-4611774e5d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exists: True | c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n",
            "         timestamp    phase        dataset task         stage  \\\n",
            "0  20250813-111348  healing  breast_cancer  clf      baseline   \n",
            "1  20250813-111348  healing  breast_cancer  clf   post_damage   \n",
            "2  20250813-111348  healing  breast_cancer  clf  healed_short   \n",
            "3  20250813-111348  healing  breast_cancer  clf   healed_long   \n",
            "\n",
            "   layer_seq_index  pct  repeats  accuracy        f1  epochs  \\\n",
            "0                0  0.2        5  0.953488  0.962963     NaN   \n",
            "1                0  0.2        5  0.872093  0.899083     NaN   \n",
            "2                0  0.2        5  0.930233  0.944444    15.0   \n",
            "3                0  0.2        5  0.965116  0.971963    46.0   \n",
            "\n",
            "                                           best_ckpt  \\\n",
            "0                                                NaN   \n",
            "1                                                NaN   \n",
            "2  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\hea...   \n",
            "3  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\hea...   \n",
            "\n",
            "                                            fig_loss  \\\n",
            "0                                                NaN   \n",
            "1                                                NaN   \n",
            "2  c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\he...   \n",
            "3  c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\he...   \n",
            "\n",
            "                                          fig_metric  \n",
            "0                                                NaN  \n",
            "1                                                NaN  \n",
            "2  c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\he...  \n",
            "3  c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\he...  \n"
          ]
        }
      ],
      "source": [
        "# View healing log \n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "csv = RESULTS_DIR / \"healing_runs.csv\"\n",
        "print(\"Exists:\", csv.exists(), \"|\", csv)\n",
        "dfh = pd.read_csv(csv)\n",
        "print(dfh.tail(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yh2-BFnn8tg"
      },
      "source": [
        "We’ll run two tiny healing experiments:\n",
        "\n",
        "Case A (likely recoverable): output layer · 20% neurons · repeats=1\n",
        "\n",
        "Case B (likely non-recoverable): output layer · 20% neurons · repeats=5 (often kills both logits across repeats → model collapses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czkFZiWGn5kC",
        "outputId": "f9ba31ef-93ef-4540-b93b-52641f7a7d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A — BASELINE: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "A — POST-DAMAGE: {'accuracy': 0.9651162790697675, 'f1': 0.972972972972973}\n"
          ]
        }
      ],
      "source": [
        "# A1 — damage: output layer, 20%, repeats=1 \n",
        "set_seed(42)\n",
        "\n",
        "last_lin_clf = layer_seq_indices(clf_ctor)[-1]  # seq index of output Linear\n",
        "\n",
        "damaged_A, mask_A, train_A, val_A, test_A, mA_before, mA_after = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=BREAST_DIR,        # ← local data dir\n",
        "        layer_lin_seq_index=last_lin_clf,\n",
        "        pct_neurons=0.20,           # with 2 output neurons, this will zero 1 of them\n",
        "        repeats=1,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "print(\"A — BASELINE:\", mA_before)\n",
        "print(\"A — POST-DAMAGE:\", mA_after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlWOWf9Doi6O",
        "outputId": "474b041b-62ec-4d03-8a45-f5920dc6d062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_clf_OUTPUT_p20_r1_short] epoch 001 | train 0.1133 | val 0.0721 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 002 | train 0.1014 | val 0.0674 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 003 | train 0.0803 | val 0.0629 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 004 | train 0.0840 | val 0.0600 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 005 | train 0.0777 | val 0.0585 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 006 | train 0.0578 | val 0.0576 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 008 | train 0.0711 | val 0.0572 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 009 | train 0.0548 | val 0.0569 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 010 | train 0.0542 | val 0.0554 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 011 | train 0.0520 | val 0.0539 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_short] epoch 012 | train 0.0494 | val 0.0535 | metric 0.9765 *\n",
            "A — SHORT HEAL TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 001 | train 0.0997 | val 0.0708 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 002 | train 0.0921 | val 0.0651 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 003 | train 0.0868 | val 0.0616 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 004 | train 0.0749 | val 0.0587 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 005 | train 0.0728 | val 0.0553 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 006 | train 0.0776 | val 0.0533 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 007 | train 0.0522 | val 0.0522 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 010 | train 0.0614 | val 0.0542 | metric 0.9765 \n",
            "[heal_clf_OUTPUT_p20_r1_long] epoch 020 | train 0.0439 | val 0.0565 | metric 0.9765 \n",
            "[heal_clf_OUTPUT_p20_r1_long] Early stopping at epoch 27. Best val_loss=0.0522\n",
            "A — LONG HEAL TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "A — Logged 4 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n",
            "A — Plots: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_p20_r1_short_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_p20_r1_short_accuracy.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_p20_r1_long_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_p20_r1_long_accuracy.png\n"
          ]
        }
      ],
      "source": [
        "# A2 — heal short (15) & long (150), evaluate, log\n",
        "A_short = \"heal_clf_OUTPUT_p20_r1_short\"\n",
        "bestA_s, histA_s = healing_train_constrained(\n",
        "    model=damaged_A, mask_dict=mask_A,\n",
        "    train_loader=train_A, val_loader=val_A, task=\"clf\",\n",
        "    max_epochs=15, lr=1e-3, weight_decay=1e-4, patience=5,\n",
        "    run_name=A_short\n",
        ")\n",
        "mA_s = clf_ctor().to(DEVICE)\n",
        "mA_s.load_state_dict(torch.load(bestA_s, map_location=DEVICE))\n",
        "metricsA_s = evaluate_model(mA_s, test_A, task=\"clf\")\n",
        "print(\"A — SHORT HEAL TEST:\", metricsA_s)\n",
        "figA_s_loss, figA_s_metric = plot_history(histA_s, A_short, task=\"clf\")\n",
        "\n",
        "# Recreate the SAME post-damage state for a clean long run\n",
        "damaged_A2, mask_A2, train_A2, val_A2, test_A2, _, _ = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=BREAST_DIR,      # ← local data dir (fixed)\n",
        "        layer_lin_seq_index=last_lin_clf,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=1,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "A_long = \"heal_clf_OUTPUT_p20_r1_long\"\n",
        "bestA_l, histA_l = healing_train_constrained(\n",
        "    model=damaged_A2, mask_dict=mask_A2,\n",
        "    train_loader=train_A2, val_loader=val_A2, task=\"clf\",\n",
        "    max_epochs=150, lr=1e-3, weight_decay=1e-4, patience=20,\n",
        "    run_name=A_long\n",
        ")\n",
        "mA_l = clf_ctor().to(DEVICE)\n",
        "mA_l.load_state_dict(torch.load(bestA_l, map_location=DEVICE))\n",
        "metricsA_l = evaluate_model(mA_l, test_A2, task=\"clf\")\n",
        "print(\"A — LONG HEAL TEST:\", metricsA_l)\n",
        "figA_l_loss, figA_l_metric = plot_history(histA_l, A_long, task=\"clf\")\n",
        "\n",
        "# Log\n",
        "rowsA = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"baseline\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 1,\n",
        "     \"accuracy\": float(mA_before[\"accuracy\"]), \"f1\": float(mA_before[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"post_damage\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 1,\n",
        "     \"accuracy\": float(mA_after[\"accuracy\"]), \"f1\": float(mA_after[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_short\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 1,\n",
        "     \"epochs\": len(histA_s[\"epoch\"]), \"accuracy\": float(metricsA_s[\"accuracy\"]), \"f1\": float(metricsA_s[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestA_s), \"fig_loss\": figA_s_loss, \"fig_metric\": figA_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_long\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 1,\n",
        "     \"epochs\": len(histA_l[\"epoch\"]), \"accuracy\": float(metricsA_l[\"accuracy\"]), \"f1\": float(metricsA_l[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestA_l), \"fig_loss\": figA_l_loss, \"fig_metric\": figA_l_metric},\n",
        "]\n",
        "for r in rowsA:\n",
        "    log_result(r, csv_name=\"healing_runs.csv\")\n",
        "\n",
        "print(\"A — Logged 4 rows to\", RESULTS_DIR / \"healing_runs.csv\")\n",
        "print(\"A — Plots:\", figA_s_loss, figA_s_metric, \"|\", figA_l_loss, figA_l_metric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkCT0WkCokpv",
        "outputId": "40230d3f-b9a7-44b6-e09d-efc5496e995f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B — BASELINE: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "B — POST-DAMAGE: {'accuracy': 0.37209302325581395, 'f1': 0.0}\n"
          ]
        }
      ],
      "source": [
        "# B1 — damage: output layer, 20%, repeats=5 \n",
        "set_seed(42)\n",
        "\n",
        "damaged_B, mask_B, train_B, val_B, test_B, mB_before, mB_after = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=BREAST_DIR,        # ← local data dir\n",
        "        layer_lin_seq_index=last_lin_clf,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "print(\"B — BASELINE:\", mB_before)\n",
        "print(\"B — POST-DAMAGE:\", mB_after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh8QgifFom0m",
        "outputId": "8f19830d-a54f-4afb-c32d-7fa053011e2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_clf_OUTPUT_p20_r5_short] epoch 001 | train 0.6931 | val 0.6931 | metric 0.3765 *\n",
            "[heal_clf_OUTPUT_p20_r5_short] Early stopping at epoch 6. Best val_loss=0.6931\n",
            "B — SHORT HEAL TEST: {'accuracy': 0.37209302325581395, 'f1': 0.0}\n",
            "[heal_clf_OUTPUT_p20_r5_long] epoch 001 | train 0.6931 | val 0.6931 | metric 0.3765 *\n",
            "[heal_clf_OUTPUT_p20_r5_long] epoch 010 | train 0.6931 | val 0.6931 | metric 0.3765 \n",
            "[heal_clf_OUTPUT_p20_r5_long] epoch 020 | train 0.6931 | val 0.6931 | metric 0.3765 \n",
            "[heal_clf_OUTPUT_p20_r5_long] Early stopping at epoch 21. Best val_loss=0.6931\n",
            "B — LONG HEAL TEST: {'accuracy': 0.37209302325581395, 'f1': 0.0}\n",
            "B — Logged 4 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n",
            "B — Plots: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_p20_r5_short_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_p20_r5_short_accuracy.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_p20_r5_long_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_p20_r5_long_accuracy.png\n"
          ]
        }
      ],
      "source": [
        "# B2 — heal short (15) & long (150), evaluate, log  \n",
        "B_short = \"heal_clf_OUTPUT_p20_r5_short\"\n",
        "bestB_s, histB_s = healing_train_constrained(\n",
        "    model=damaged_B, mask_dict=mask_B,\n",
        "    train_loader=train_B, val_loader=val_B, task=\"clf\",\n",
        "    max_epochs=15, lr=1e-3, weight_decay=1e-4, patience=5,\n",
        "    run_name=B_short\n",
        ")\n",
        "mB_s = clf_ctor().to(DEVICE)\n",
        "mB_s.load_state_dict(torch.load(bestB_s, map_location=DEVICE))\n",
        "metricsB_s = evaluate_model(mB_s, test_B, task=\"clf\")\n",
        "print(\"B — SHORT HEAL TEST:\", metricsB_s)\n",
        "figB_s_loss, figB_s_metric = plot_history(histB_s, B_short, task=\"clf\")\n",
        "\n",
        "# Fresh same-damage state for long run\n",
        "damaged_B2, mask_B2, train_B2, val_B2, test_B2, _, _ = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=clf_ctor,\n",
        "        ckpt_path=CLF_CKPT,\n",
        "        task=\"clf\",\n",
        "        data_dir=BREAST_DIR,     # ← FIXED\n",
        "        layer_lin_seq_index=last_lin_clf,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "B_long = \"heal_clf_OUTPUT_p20_r5_long\"\n",
        "bestB_l, histB_l = healing_train_constrained(\n",
        "    model=damaged_B2, mask_dict=mask_B2,\n",
        "    train_loader=train_B2, val_loader=val_B2, task=\"clf\",\n",
        "    max_epochs=150, lr=1e-3, weight_decay=1e-4, patience=20,\n",
        "    run_name=B_long\n",
        ")\n",
        "mB_l = clf_ctor().to(DEVICE)\n",
        "mB_l.load_state_dict(torch.load(bestB_l, map_location=DEVICE))\n",
        "metricsB_l = evaluate_model(mB_l, test_B2, task=\"clf\")\n",
        "print(\"B — LONG HEAL TEST:\", metricsB_l)\n",
        "figB_l_loss, figB_l_metric = plot_history(histB_l, B_long, task=\"clf\")\n",
        "\n",
        "# Log\n",
        "rowsB = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"baseline\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"accuracy\": float(mB_before[\"accuracy\"]), \"f1\": float(mB_before[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"post_damage\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"accuracy\": float(mB_after[\"accuracy\"]), \"f1\": float(mB_after[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_short\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"epochs\": len(histB_s[\"epoch\"]), \"accuracy\": float(metricsB_s[\"accuracy\"]), \"f1\": float(metricsB_s[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestB_s), \"fig_loss\": figB_s_loss, \"fig_metric\": figB_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_long\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"epochs\": len(histB_l[\"epoch\"]), \"accuracy\": float(metricsB_l[\"accuracy\"]), \"f1\": float(metricsB_l[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestB_l), \"fig_loss\": figB_l_loss, \"fig_metric\": figB_l_metric},\n",
        "]\n",
        "for r in rowsB:\n",
        "    log_result(r, csv_name=\"healing_runs.csv\")\n",
        "print(\"B — Logged 4 rows to\", RESULTS_DIR / \"healing_runs.csv\")\n",
        "print(\"B — Plots:\", figB_s_loss, figB_s_metric, \"|\", figB_l_loss, figB_l_metric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfmVnTg3ooZV",
        "outputId": "a4ef0ef7-549a-482f-cd0b-528215cfc76c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REG — BASELINE: {'mae': 0.37485018372535706, 'rmse': 0.5373355181330539}\n",
            "REG — POST-DAMAGE: {'mae': 0.6136718988418579, 'rmse': 0.9004139597221144}\n"
          ]
        }
      ],
      "source": [
        "# regression: damage first hidden layer, 20%, repeats=5\n",
        "#run on demand\n",
        "set_seed(42)\n",
        "\n",
        "first_lin_reg = layer_seq_indices(reg_ctor)[0]  # seq index 0 for reg model\n",
        "\n",
        "damaged_R, mask_R, train_R, val_R, test_R, mR_before, mR_after = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=reg_ctor,\n",
        "        ckpt_path=REG_CKPT,\n",
        "        task=\"reg\",\n",
        "        data_dir=CAL_DIR,          # ← local data dir\n",
        "        layer_lin_seq_index=first_lin_reg,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "print(\"REG — BASELINE:\", mR_before)\n",
        "print(\"REG — POST-DAMAGE:\", mR_after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQmGH6ZGo9g2",
        "outputId": "8b79ace3-1777-4bed-90d9-045ee7c0046f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[heal_reg_firstHidden_p20_r5_short] epoch 001 | train 0.5677 | val 0.4425 | metric 0.6486 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 002 | train 0.4676 | val 0.4181 | metric 0.6257 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 003 | train 0.4479 | val 0.4089 | metric 0.6176 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 004 | train 0.4293 | val 0.4010 | metric 0.6137 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 005 | train 0.4321 | val 0.3963 | metric 0.6076 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 008 | train 0.4227 | val 0.3913 | metric 0.6023 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 009 | train 0.4038 | val 0.3874 | metric 0.5977 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 010 | train 0.4046 | val 0.3908 | metric 0.6000 \n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 011 | train 0.4024 | val 0.3821 | metric 0.5945 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 012 | train 0.3979 | val 0.3743 | metric 0.5903 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 014 | train 0.3894 | val 0.3714 | metric 0.5897 *\n",
            "[heal_reg_firstHidden_p20_r5_short] epoch 015 | train 0.3825 | val 0.3649 | metric 0.5843 *\n",
            "REG — SHORT HEAL TEST: {'mae': 0.3913136422634125, 'rmse': 0.5505083368399453}\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 001 | train 0.6128 | val 0.4473 | metric 0.6520 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 002 | train 0.4845 | val 0.4148 | metric 0.6266 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 003 | train 0.4566 | val 0.4096 | metric 0.6204 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 006 | train 0.4302 | val 0.3946 | metric 0.6058 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 009 | train 0.4090 | val 0.3826 | metric 0.5968 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 010 | train 0.4092 | val 0.3890 | metric 0.6013 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 011 | train 0.4018 | val 0.3769 | metric 0.5946 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 014 | train 0.3955 | val 0.3732 | metric 0.5893 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 015 | train 0.3927 | val 0.3716 | metric 0.5883 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 016 | train 0.3868 | val 0.3675 | metric 0.5862 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 017 | train 0.3779 | val 0.3663 | metric 0.5859 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 019 | train 0.3845 | val 0.3643 | metric 0.5820 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 020 | train 0.3927 | val 0.3701 | metric 0.5859 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 022 | train 0.3721 | val 0.3589 | metric 0.5768 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 023 | train 0.3702 | val 0.3579 | metric 0.5771 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 024 | train 0.3767 | val 0.3547 | metric 0.5758 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 027 | train 0.3616 | val 0.3519 | metric 0.5740 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 030 | train 0.3639 | val 0.3527 | metric 0.5731 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 031 | train 0.3575 | val 0.3484 | metric 0.5682 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 032 | train 0.3602 | val 0.3476 | metric 0.5693 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 033 | train 0.3580 | val 0.3435 | metric 0.5675 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 034 | train 0.3637 | val 0.3430 | metric 0.5697 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 035 | train 0.3547 | val 0.3406 | metric 0.5664 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 036 | train 0.3521 | val 0.3389 | metric 0.5635 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 038 | train 0.3491 | val 0.3379 | metric 0.5645 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 039 | train 0.3488 | val 0.3343 | metric 0.5605 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 040 | train 0.3453 | val 0.3423 | metric 0.5684 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 041 | train 0.3533 | val 0.3270 | metric 0.5597 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 050 | train 0.3372 | val 0.3264 | metric 0.5558 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 053 | train 0.3321 | val 0.3253 | metric 0.5553 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 055 | train 0.3328 | val 0.3253 | metric 0.5554 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 060 | train 0.3377 | val 0.3321 | metric 0.5579 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 062 | train 0.3396 | val 0.3246 | metric 0.5541 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 063 | train 0.3322 | val 0.3232 | metric 0.5534 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 068 | train 0.3261 | val 0.3223 | metric 0.5539 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 069 | train 0.3341 | val 0.3221 | metric 0.5538 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 070 | train 0.3315 | val 0.3278 | metric 0.5558 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 080 | train 0.3290 | val 0.3230 | metric 0.5530 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 085 | train 0.3287 | val 0.3219 | metric 0.5520 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 090 | train 0.3279 | val 0.3219 | metric 0.5524 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 091 | train 0.3270 | val 0.3217 | metric 0.5523 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 092 | train 0.3244 | val 0.3212 | metric 0.5519 *\n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 100 | train 0.3271 | val 0.3231 | metric 0.5524 \n",
            "[heal_reg_firstHidden_p20_r5_long] epoch 110 | train 0.3197 | val 0.3222 | metric 0.5520 \n",
            "[heal_reg_firstHidden_p20_r5_long] Early stopping at epoch 112. Best val_loss=0.3212\n",
            "REG — LONG HEAL TEST: {'mae': 0.3671099841594696, 'rmse': 0.5211531197786209}\n",
            "REG — Logged 4 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n",
            "REG — Plots: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_firstHidden_p20_r5_short_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_firstHidden_p20_r5_short_RMSE.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_firstHidden_p20_r5_long_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_firstHidden_p20_r5_long_RMSE.png\n"
          ]
        }
      ],
      "source": [
        "# R2 — regression healing (short vs long), evaluate, log\n",
        "# run on demand\n",
        "# ---- SHORT HEAL (≈15 epochs) ----\n",
        "R_short = \"heal_reg_firstHidden_p20_r5_short\"\n",
        "bestR_s, histR_s = healing_train_constrained(\n",
        "    model=damaged_R,\n",
        "    mask_dict=mask_R,\n",
        "    train_loader=train_R,\n",
        "    val_loader=val_R,\n",
        "    task=\"reg\",\n",
        "    max_epochs=15,          # quick heal\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=5,\n",
        "    run_name=R_short\n",
        ")\n",
        "\n",
        "mR_s = reg_ctor().to(DEVICE)\n",
        "mR_s.load_state_dict(torch.load(bestR_s, map_location=DEVICE))\n",
        "metricsR_s = evaluate_model(mR_s, test_R, task=\"reg\")\n",
        "print(\"REG — SHORT HEAL TEST:\", metricsR_s)\n",
        "figR_s_loss, figR_s_metric = plot_history(histR_s, R_short, task=\"reg\")\n",
        "\n",
        "# ---- LONG HEAL (≤150 epochs) ----\n",
        "# Recreate the SAME post-damage state so long run starts from identical damage\n",
        "damaged_R2, mask_R2, train_R2, val_R2, test_R2, _, _ = \\\n",
        "    apply_progressive_neuron_damage_with_masks(\n",
        "        model_ctor=reg_ctor,\n",
        "        ckpt_path=REG_CKPT,\n",
        "        task=\"reg\",\n",
        "        data_dir=CAL_DIR,          # ← local data dir (fixed)\n",
        "        layer_lin_seq_index=first_lin_reg,\n",
        "        pct_neurons=0.20,\n",
        "        repeats=5,\n",
        "        seed=12345\n",
        "    )\n",
        "\n",
        "R_long = \"heal_reg_firstHidden_p20_r5_long\"\n",
        "bestR_l, histR_l = healing_train_constrained(\n",
        "    model=damaged_R2,\n",
        "    mask_dict=mask_R2,\n",
        "    train_loader=train_R2,\n",
        "    val_loader=val_R2,\n",
        "    task=\"reg\",\n",
        "    max_epochs=150,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=20,\n",
        "    run_name=R_long\n",
        ")\n",
        "\n",
        "mR_l = reg_ctor().to(DEVICE)\n",
        "mR_l.load_state_dict(torch.load(bestR_l, map_location=DEVICE))\n",
        "metricsR_l = evaluate_model(mR_l, test_R2, task=\"reg\")\n",
        "print(\"REG — LONG HEAL TEST:\", metricsR_l)\n",
        "figR_l_loss, figR_l_metric = plot_history(histR_l, R_long, task=\"reg\")\n",
        "\n",
        "# ---- LOG to healing_runs.csv (mae & rmse) ----\n",
        "rowsR = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"baseline\", \"layer_seq_index\": int(first_lin_reg), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"mae\": float(mR_before[\"mae\"]), \"rmse\": float(mR_before[\"rmse\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"post_damage\", \"layer_seq_index\": int(first_lin_reg), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"mae\": float(mR_after[\"mae\"]), \"rmse\": float(mR_after[\"rmse\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"healed_short\", \"layer_seq_index\": int(first_lin_reg), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"epochs\": len(histR_s[\"epoch\"]),\n",
        "     \"mae\": float(metricsR_s[\"mae\"]), \"rmse\": float(metricsR_s[\"rmse\"]),\n",
        "     \"best_ckpt\": str(bestR_s), \"fig_loss\": figR_s_loss, \"fig_metric\": figR_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"healed_long\", \"layer_seq_index\": int(first_lin_reg), \"pct\": 0.20, \"repeats\": 5,\n",
        "     \"epochs\": len(histR_l[\"epoch\"]),\n",
        "     \"mae\": float(metricsR_l[\"mae\"]), \"rmse\": float(metricsR_l[\"rmse\"]),\n",
        "     \"best_ckpt\": str(bestR_l), \"fig_loss\": figR_l_loss, \"fig_metric\": figR_l_metric},\n",
        "]\n",
        "for r in rowsR:\n",
        "    log_result(r, csv_name=\"healing_runs.csv\")\n",
        "\n",
        "print(\"REG — Logged 4 rows to\", RESULTS_DIR / \"healing_runs.csv\")\n",
        "print(\"REG — Plots:\", figR_s_loss, figR_s_metric, \"|\", figR_l_loss, figR_l_metric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Bkv2QCcMpeQY"
      },
      "outputs": [],
      "source": [
        "# --- DAMAGE: specific neuron indices (no masks, for immediate-drop scans) ---\n",
        "#rerun\n",
        "def damage_specific_neurons_in_layer(model: nn.Module, layer_lin_seq_index: int, neuron_indices, *, also_zero_next=True):\n",
        "    \"\"\"\n",
        "    Zero EXACT neurons in a given Linear layer:\n",
        "      - zero rows in target Linear's weight and matching bias entries\n",
        "      - optionally zero the corresponding columns in the NEXT Linear's weight\n",
        "    \"\"\"\n",
        "    if isinstance(neuron_indices, (int, np.integer)):\n",
        "        neuron_indices = [int(neuron_indices)]\n",
        "    neuron_indices = np.array(neuron_indices, dtype=int)\n",
        "\n",
        "    linear_layers = get_linear_layers(model)\n",
        "    # find target position within the linear-only list\n",
        "    target_pos = None\n",
        "    for k, (seq_i, lin) in enumerate(linear_layers):\n",
        "        if seq_i == layer_lin_seq_index:\n",
        "            target_pos = k\n",
        "            break\n",
        "    assert target_pos is not None, f\"No Linear layer at seq index {layer_lin_seq_index}\"\n",
        "    target_seq_i, target_lin = linear_layers[target_pos]\n",
        "    next_lin = linear_layers[target_pos + 1][1] if (target_pos + 1) < len(linear_layers) else None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        W = target_lin.weight  # [out_features, in_features]\n",
        "        W[neuron_indices, :] = 0.0\n",
        "        if target_lin.bias is not None:\n",
        "            target_lin.bias[neuron_indices] = 0.0\n",
        "\n",
        "        if also_zero_next and next_lin is not None:\n",
        "            Wn = next_lin.weight  # [next_out, next_in]\n",
        "            Wn[:, neuron_indices] = 0.0\n",
        "\n",
        "    return neuron_indices.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning layer seq_index=0 with 64 neurons...\n",
            "Baseline ACC=0.9535 | F1=0.9630\n",
            "  scanned 8/64\n",
            "  scanned 16/64\n",
            "  scanned 24/64\n",
            "  scanned 32/64\n",
            "  scanned 40/64\n",
            "  scanned 48/64\n",
            "  scanned 56/64\n",
            "  scanned 64/64\n",
            "\n",
            "Top-10 neurons by ACC drop:\n",
            "  neuron_index  acc_drop   f1_drop      acc       f1\n",
            "           30  0.023256  0.017508 0.930233 0.945455\n",
            "            9  0.011628  0.008835 0.941860 0.954128\n",
            "           20  0.011628  0.008835 0.941860 0.954128\n",
            "           45  0.011628  0.008835 0.941860 0.954128\n",
            "           62  0.011628  0.008835 0.941860 0.954128\n",
            "           40  0.011628  0.008835 0.941860 0.954128\n",
            "           47  0.011628  0.008835 0.941860 0.954128\n",
            "            0  0.000000 -0.000673 0.953488 0.963636\n",
            "            7  0.000000  0.000000 0.953488 0.962963\n",
            "           10  0.000000  0.000000 0.953488 0.962963\n",
            "Top-3 neurons: [30, 9, 20]\n",
            "Figure saved: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\per_neuron_scan_clf_first_accdrop.png\n"
          ]
        }
      ],
      "source": [
        "# --- PER-NEURON SCAN: classification / first hidden layer ---\n",
        "set_seed(42)\n",
        "\n",
        "# Identify the first Linear’s seq index and width\n",
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]\n",
        "tmp = clf_ctor().to(DEVICE)\n",
        "first_layer_info = [d for d in summarize_linear_layers(tmp) if d[\"seq_index\"] == first_lin_clf][0]\n",
        "n_neurons = first_layer_info[\"out_features\"]\n",
        "print(f\"Scanning layer seq_index={first_lin_clf} with {n_neurons} neurons...\")\n",
        "\n",
        "# Prepare data loaders (test set used for evaluation)\n",
        "_, _, test_loader_clf, _ = make_loaders(BREAST_DIR, task=\"clf\", batch_size=512)\n",
        "\n",
        "\n",
        "rows = []\n",
        "# Baseline once\n",
        "baseline_model = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "baseline_metrics = evaluate_model(baseline_model, test_loader_clf, task=\"clf\")\n",
        "base_acc = baseline_metrics[\"accuracy\"]\n",
        "base_f1  = baseline_metrics[\"f1\"]\n",
        "print(f\"Baseline ACC={base_acc:.4f} | F1={base_f1:.4f}\")\n",
        "\n",
        "for i in range(n_neurons):\n",
        "    m = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "    damage_specific_neurons_in_layer(m, layer_lin_seq_index=first_lin_clf, neuron_indices=[i])\n",
        "    metrics = evaluate_model(m, test_loader_clf, task=\"clf\")\n",
        "    acc, f1 = metrics[\"accuracy\"], metrics[\"f1\"]\n",
        "    rows.append({\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"per_neuron_scan\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"layer_seq_index\": int(first_lin_clf),\n",
        "        \"neuron_index\": int(i),\n",
        "        \"acc\": float(acc),\n",
        "        \"f1\": float(f1),\n",
        "        \"acc_drop\": float(base_acc - acc),\n",
        "        \"f1_drop\": float(base_f1 - f1),\n",
        "        \"ckpt\": CLF_CKPT\n",
        "    })\n",
        "    if (i+1) % 8 == 0 or i == n_neurons-1:\n",
        "        print(f\"  scanned {i+1}/{n_neurons}\")\n",
        "\n",
        "# Save to CSV (append-safe; but we'll also save a dedicated file)\n",
        "for r in rows:\n",
        "    log_result(r, csv_name=\"per_neuron_scan_clf_first.csv\")\n",
        "\n",
        "import pandas as pd\n",
        "scan_csv = PROJECT_ROOT / \"results\" / \"per_neuron_scan_clf_first.csv\"\n",
        "df_scan = pd.read_csv(scan_csv)\n",
        "\n",
        "# Sort by accuracy drop (descending) and show top 10\n",
        "df_sorted = df_scan.sort_values(\"acc_drop\", ascending=False)\n",
        "top10 = df_sorted.head(10)[[\"neuron_index\",\"acc_drop\",\"f1_drop\",\"acc\",\"f1\"]]\n",
        "print(\"\\nTop-10 neurons by ACC drop:\\n\", top10.to_string(index=False))\n",
        "\n",
        "# Save the sorted CSV and the Top-3 list\n",
        "df_sorted.to_csv(PROJECT_ROOT / \"results\" / \"per_neuron_scan_clf_first_sorted.csv\", index=False)\n",
        "top3 = df_sorted.head(3)[\"neuron_index\"].tolist()\n",
        "with open(PROJECT_ROOT / \"results\" / \"per_neuron_top3_clf_first.json\",\"w\") as f:\n",
        "    import json; json.dump({\"layer_seq_index\": int(first_lin_clf), \"top3\": top3}, f, indent=2)\n",
        "print(\"Top-3 neurons:\", top3)\n",
        "\n",
        "# Plot bar chart of per-neuron accuracy drop\n",
        "fig = plt.figure(figsize=(8,4))\n",
        "plt.bar(df_scan[\"neuron_index\"].values, df_scan[\"acc_drop\"].values)\n",
        "plt.xlabel(\"neuron index (first hidden layer)\"); plt.ylabel(\"Δ accuracy vs baseline\")\n",
        "plt.title(\"Per-neuron damage impact (classification, first hidden)\")\n",
        "fig_path = FIG_DIR / \"per_neuron_scan_clf_first_accdrop.png\"\n",
        "plt.savefig(fig_path, bbox_inches=\"tight\"); plt.close()\n",
        "print(\"Figure saved:\", fig_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Excluding Top-3 neuron IDs: [9, 20, 30]\n",
            "Baseline ACC=0.9535 | F1=0.9630\n",
            "Total pairs to scan: 1830\n",
            "  scanned 100/1830\n",
            "  scanned 200/1830\n",
            "  scanned 300/1830\n",
            "  scanned 400/1830\n",
            "  scanned 500/1830\n",
            "  scanned 600/1830\n",
            "  scanned 700/1830\n",
            "  scanned 800/1830\n",
            "  scanned 900/1830\n",
            "  scanned 1000/1830\n",
            "  scanned 1100/1830\n",
            "  scanned 1200/1830\n",
            "  scanned 1300/1830\n",
            "  scanned 1400/1830\n",
            "  scanned 1500/1830\n",
            "  scanned 1600/1830\n",
            "  scanned 1700/1830\n",
            "  scanned 1800/1830\n",
            "  scanned 1830/1830\n",
            "\n",
            "Top-10 pairs by ACC drop (this run):\n",
            "  neuron_i  neuron_j  acc_drop  f1_drop      acc       f1\n",
            "       40        45  0.034884 0.026026 0.918605 0.936937\n",
            "       45        53  0.034884 0.026026 0.918605 0.936937\n",
            "       26        45  0.023256 0.017508 0.930233 0.945455\n",
            "       17        45  0.023256 0.017508 0.930233 0.945455\n",
            "       41        45  0.023256 0.017508 0.930233 0.945455\n",
            "       18        45  0.023256 0.016534 0.930233 0.946429\n",
            "       25        45  0.023256 0.016534 0.930233 0.946429\n",
            "       45        47  0.023256 0.017508 0.930233 0.945455\n",
            "       15        45  0.023256 0.017508 0.930233 0.945455\n",
            "       12        45  0.023256 0.016534 0.930233 0.946429\n",
            "Saved per-run file: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\per_pair_scan_clf_first_20250813-123923.csv\n",
            "Appended to aggregate: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\per_pair_scan_clf_first.csv\n"
          ]
        }
      ],
      "source": [
        "# --- PER-PAIR SCAN: classification / first hidden (exclude Top-3) ---\n",
        "# run-on-demand\n",
        "import json\n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Identify layer & size\n",
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]\n",
        "tmp = clf_ctor().to(DEVICE)\n",
        "first_layer_info = [d for d in summarize_linear_layers(tmp) if d[\"seq_index\"] == first_lin_clf][0]\n",
        "n_neurons = first_layer_info[\"out_features\"]\n",
        "\n",
        "# Load Top-3 (from previous step)\n",
        "with open(PROJECT_ROOT / \"results\" / \"per_neuron_top3_clf_first.json\") as f:\n",
        "    top3 = json.load(f)[\"top3\"]\n",
        "top3 = set(int(x) for x in top3)\n",
        "print(\"Excluding Top-3 neuron IDs:\", sorted(top3))\n",
        "\n",
        "# Data loaders & baseline\n",
        "_, _, test_loader_clf, _ = make_loaders(BREAST_DIR, task=\"clf\", batch_size=512)\n",
        "baseline_model = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "baseline_metrics = evaluate_model(baseline_model, test_loader_clf, task=\"clf\")\n",
        "base_acc, base_f1 = baseline_metrics[\"accuracy\"], baseline_metrics[\"f1\"]\n",
        "print(f\"Baseline ACC={base_acc:.4f} | F1={base_f1:.4f}\")\n",
        "\n",
        "# Candidate neurons and all pairs among them\n",
        "candidates = [i for i in range(n_neurons) if i not in top3]\n",
        "pairs = list(combinations(candidates, 2))\n",
        "print(f\"Total pairs to scan: {len(pairs)}\")\n",
        "\n",
        "run_id = timestamp()\n",
        "rows = []\n",
        "\n",
        "for idx, (i, j) in enumerate(pairs, start=1):\n",
        "    m = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "    damage_specific_neurons_in_layer(m, layer_lin_seq_index=first_lin_clf, neuron_indices=[i, j])\n",
        "    metrics = evaluate_model(m, test_loader_clf, task=\"clf\")\n",
        "    acc, f1 = metrics[\"accuracy\"], metrics[\"f1\"]\n",
        "    rows.append({\n",
        "        \"run_id\": run_id,\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"per_pair_scan\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"layer_seq_index\": int(first_lin_clf),\n",
        "        \"neuron_i\": int(i),\n",
        "        \"neuron_j\": int(j),\n",
        "        \"acc\": float(acc),\n",
        "        \"f1\": float(f1),\n",
        "        \"acc_drop\": float(base_acc - acc),\n",
        "        \"f1_drop\": float(base_f1 - f1),\n",
        "        \"ckpt\": CLF_CKPT\n",
        "    })\n",
        "    if idx % 100 == 0 or idx == len(pairs):\n",
        "        print(f\"  scanned {idx}/{len(pairs)}\")\n",
        "\n",
        "# Save results (one file for this run, plus an aggregated file)\n",
        "df_pairs = pd.DataFrame(rows)\n",
        "per_run_path = PROJECT_ROOT / \"results\" / f\"per_pair_scan_clf_first_{run_id}.csv\"\n",
        "df_pairs.to_csv(per_run_path, index=False)\n",
        "\n",
        "agg_path = PROJECT_ROOT / \"results\" / \"per_pair_scan_clf_first.csv\"\n",
        "if agg_path.exists():\n",
        "    # append without header\n",
        "    df_pairs.to_csv(agg_path, mode=\"a\", header=False, index=False)\n",
        "else:\n",
        "    df_pairs.to_csv(agg_path, index=False)\n",
        "\n",
        "# Show Top-10 pairs by accuracy drop for this run\n",
        "df_sorted = df_pairs.sort_values(\"acc_drop\", ascending=False)\n",
        "top10 = df_sorted.head(10)[[\"neuron_i\",\"neuron_j\",\"acc_drop\",\"f1_drop\",\"acc\",\"f1\"]]\n",
        "print(\"\\nTop-10 pairs by ACC drop (this run):\\n\", top10.to_string(index=False))\n",
        "print(\"Saved per-run file:\", per_run_path)\n",
        "print(\"Appended to aggregate:\", agg_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rerun-after-restart\n",
        "def damage_specific_neurons_in_layer_with_masks(model: nn.Module, layer_lin_seq_index: int, neuron_indices, mask_dict=None):\n",
        "    \"\"\"\n",
        "    Like damage_specific_neurons_in_layer, but also updates a mask_dict so damaged entries are frozen (grad=0).\n",
        "    Returns: (neuron_indices_list, mask_dict)\n",
        "    \"\"\"\n",
        "    if isinstance(neuron_indices, (int, np.integer)):\n",
        "        neuron_indices = [int(neuron_indices)]\n",
        "    neuron_indices = np.array(neuron_indices, dtype=int)\n",
        "\n",
        "    linear_layers = get_linear_layers(model)\n",
        "    target_pos = None\n",
        "    for k, (seq_i, lin) in enumerate(linear_layers):\n",
        "        if seq_i == layer_lin_seq_index:\n",
        "            target_pos = k\n",
        "            break\n",
        "    assert target_pos is not None, f\"No Linear layer at seq index {layer_lin_seq_index}\"\n",
        "    target_seq_i, target_lin = linear_layers[target_pos]\n",
        "    next_lin = linear_layers[target_pos + 1][1] if (target_pos + 1) < len(linear_layers) else None\n",
        "\n",
        "    if mask_dict is None:\n",
        "        mask_dict = init_full_keep_masks_for_model(model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Zero target rows + bias; AND zero their grad masks\n",
        "        W = target_lin.weight  # [out, in]\n",
        "        W[neuron_indices, :] = 0.0\n",
        "        maskW = mask_dict[W] if W in mask_dict else mask_dict.setdefault(W, torch.ones_like(W))\n",
        "        maskW[neuron_indices, :] = 0.0\n",
        "\n",
        "        if target_lin.bias is not None:\n",
        "            b = target_lin.bias\n",
        "            b[neuron_indices] = 0.0\n",
        "            maskb = mask_dict[b] if b in mask_dict else mask_dict.setdefault(b, torch.ones_like(b))\n",
        "            maskb[neuron_indices] = 0.0\n",
        "\n",
        "        # Zero corresponding columns in next layer (outgoing)\n",
        "        if next_lin is not None:\n",
        "            Wn = next_lin.weight  # [next_out, next_in]\n",
        "            Wn[:, neuron_indices] = 0.0\n",
        "            maskWn = mask_dict[Wn] if Wn in mask_dict else mask_dict.setdefault(Wn, torch.ones_like(Wn))\n",
        "            maskWn[:, neuron_indices] = 0.0\n",
        "\n",
        "    return neuron_indices.tolist(), mask_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rerun-after-restart\n",
        "def apply_specific_neuron_damage_with_masks(model_ctor, ckpt_path, task, data_dir, layer_lin_seq_index, neuron_indices):\n",
        "    \"\"\"\n",
        "    Load clean checkpoint, apply specific neuron damage with masks, return\n",
        "    (damaged_model, mask_dict, train_loader, val_loader, test_loader, metrics_before, metrics_after)\n",
        "    \"\"\"\n",
        "    if task == \"clf\":\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"clf\", batch_size=256)\n",
        "    else:\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"reg\", batch_size=512)\n",
        "\n",
        "    model = clone_from_checkpoint(model_ctor, ckpt_path)\n",
        "    metrics_before = evaluate_model(model, test_loader, task)\n",
        "\n",
        "    mask_dict = init_full_keep_masks_for_model(model)\n",
        "    _, mask_dict = damage_specific_neurons_in_layer_with_masks(\n",
        "        model, layer_lin_seq_index=layer_lin_seq_index, neuron_indices=neuron_indices, mask_dict=mask_dict\n",
        "    )\n",
        "    metrics_after = evaluate_model(model, test_loader, task)\n",
        "    return model, mask_dict, train_loader, val_loader, test_loader, metrics_before, metrics_after\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rerun-after-restart\n",
        "def recovery_epoch_95pct_of_baseline(history: dict, baseline_acc: float):\n",
        "    \"\"\"\n",
        "    For classification: find first epoch whose val_metric >= 0.95 * baseline_acc.\n",
        "    Returns the epoch number (int) or None if not reached.\n",
        "    \"\"\"\n",
        "    threshold = 0.95 * float(baseline_acc)\n",
        "    for e, v in zip(history[\"epoch\"], history[\"val_metric\"]):\n",
        "        if v >= threshold:\n",
        "            return int(e)\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Worst single neuron (first hidden): 30\n",
            "Baseline ACC=0.9535 | F1=0.9630 | 95% threshold=0.9058\n",
            "SINGLE — BASELINE: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "SINGLE — POST-DAMAGE: {'accuracy': 0.9302325581395349, 'f1': 0.9454545454545454}\n",
            "[heal_clf_firstHidden_singleN30_short] epoch 001 | train 0.0513 | val 0.0587 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_singleN30_short] Early stopping at epoch 6. Best val_loss=0.0587\n",
            "SINGLE — SHORT TEST: {'accuracy': 0.9418604651162791, 'f1': 0.9541284403669725} | recovery_epoch_95%: 1\n",
            "[heal_clf_firstHidden_singleN30_long] epoch 001 | train 0.0465 | val 0.0586 | metric 0.9765 *\n",
            "[heal_clf_firstHidden_singleN30_long] epoch 010 | train 0.0551 | val 0.0745 | metric 0.9765 \n",
            "[heal_clf_firstHidden_singleN30_long] epoch 020 | train 0.0501 | val 0.0703 | metric 0.9765 \n",
            "[heal_clf_firstHidden_singleN30_long] Early stopping at epoch 21. Best val_loss=0.0586\n",
            "SINGLE — LONG TEST: {'accuracy': 0.9418604651162791, 'f1': 0.9541284403669725} | recovery_epoch_95%: 1\n",
            "Logged SINGLE-neuron healing rows to: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd, json\n",
        "\n",
        "set_seed(42)\n",
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]\n",
        "\n",
        "# Read worst single neuron from the sorted CSV (so it's robust)\n",
        "df_single = pd.read_csv(PROJECT_ROOT / \"results\" / \"per_neuron_scan_clf_first_sorted.csv\")\n",
        "worst_neuron = int(df_single.iloc[0][\"neuron_index\"])\n",
        "print(\"Worst single neuron (first hidden):\", worst_neuron)\n",
        "\n",
        "# Build loaders & baseline once (for threshold)\n",
        "_, _, test_loader_clf, _ = make_loaders(BREAST_DIR, task=\"clf\", batch_size=512)\n",
        "baseline_model = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "baseline_metrics = evaluate_model(baseline_model, test_loader_clf, task=\"clf\")\n",
        "base_acc = baseline_metrics[\"accuracy\"]; base_f1 = baseline_metrics[\"f1\"]\n",
        "print(f\"Baseline ACC={base_acc:.4f} | F1={base_f1:.4f} | 95% threshold={0.95*base_acc:.4f}\")\n",
        "\n",
        "# Create damaged model (specific neuron) + masks\n",
        "damaged_S, mask_S, train_S, val_S, test_S, S_before, S_after = apply_specific_neuron_damage_with_masks(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_lin_seq_index=first_lin_clf, neuron_indices=[worst_neuron]\n",
        ")\n",
        "print(\"SINGLE — BASELINE:\", S_before)\n",
        "print(\"SINGLE — POST-DAMAGE:\", S_after)\n",
        "\n",
        "# Short heal (≈15 epochs)\n",
        "S_short_name = f\"heal_clf_firstHidden_singleN{worst_neuron}_short\"\n",
        "bestS_s, histS_s = healing_train_constrained(\n",
        "    model=damaged_S, mask_dict=mask_S, train_loader=train_S, val_loader=val_S, task=\"clf\",\n",
        "    max_epochs=15, lr=1e-3, weight_decay=1e-4, patience=5, run_name=S_short_name\n",
        ")\n",
        "mS_s = clf_ctor().to(DEVICE); mS_s.load_state_dict(torch.load(bestS_s, map_location=DEVICE))\n",
        "metricsS_s = evaluate_model(mS_s, test_S, task=\"clf\")\n",
        "recov_epoch_s = recovery_epoch_95pct_of_baseline(histS_s, base_acc)\n",
        "print(\"SINGLE — SHORT TEST:\", metricsS_s, \"| recovery_epoch_95%:\", recov_epoch_s)\n",
        "figS_s_loss, figS_s_metric = plot_history(histS_s, S_short_name, task=\"clf\")\n",
        "\n",
        "# Recreate same damaged state for a clean long run\n",
        "damaged_S2, mask_S2, train_S2, val_S2, test_S2, _, _ = apply_specific_neuron_damage_with_masks(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_lin_seq_index=first_lin_clf, neuron_indices=[worst_neuron]\n",
        ")\n",
        "\n",
        "S_long_name = f\"heal_clf_firstHidden_singleN{worst_neuron}_long\"\n",
        "bestS_l, histS_l = healing_train_constrained(\n",
        "    model=damaged_S2, mask_dict=mask_S2, train_loader=train_S2, val_loader=val_S2, task=\"clf\",\n",
        "    max_epochs=150, lr=1e-3, weight_decay=1e-4, patience=20, run_name=S_long_name\n",
        ")\n",
        "mS_l = clf_ctor().to(DEVICE); mS_l.load_state_dict(torch.load(bestS_l, map_location=DEVICE))\n",
        "metricsS_l = evaluate_model(mS_l, test_S2, task=\"clf\")\n",
        "recov_epoch_l = recovery_epoch_95pct_of_baseline(histS_l, base_acc)\n",
        "print(\"SINGLE — LONG TEST:\", metricsS_l, \"| recovery_epoch_95%:\", recov_epoch_l)\n",
        "figS_l_loss, figS_l_metric = plot_history(histS_l, S_long_name, task=\"clf\")\n",
        "\n",
        "# Log to healing_runs.csv\n",
        "rowsS = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"single_baseline\", \"layer_seq_index\": int(first_lin_clf), \"neuron\": worst_neuron,\n",
        "     \"accuracy\": float(S_before[\"accuracy\"]), \"f1\": float(S_before[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"single_post_damage\", \"layer_seq_index\": int(first_lin_clf), \"neuron\": worst_neuron,\n",
        "     \"accuracy\": float(S_after[\"accuracy\"]), \"f1\": float(S_after[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"single_healed_short\", \"layer_seq_index\": int(first_lin_clf), \"neuron\": worst_neuron,\n",
        "     \"epochs\": len(histS_s[\"epoch\"]), \"recovery_epoch_95p\": recov_epoch_s,\n",
        "     \"accuracy\": float(metricsS_s[\"accuracy\"]), \"f1\": float(metricsS_s[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestS_s), \"fig_loss\": figS_s_loss, \"fig_metric\": figS_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"single_healed_long\", \"layer_seq_index\": int(first_lin_clf), \"neuron\": worst_neuron,\n",
        "     \"epochs\": len(histS_l[\"epoch\"]), \"recovery_epoch_95p\": recov_epoch_l,\n",
        "     \"accuracy\": float(metricsS_l[\"accuracy\"]), \"f1\": float(metricsS_l[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestS_l), \"fig_loss\": figS_l_loss, \"fig_metric\": figS_l_metric},\n",
        "]\n",
        "for r in rowsS: log_result(r, csv_name=\"healing_runs.csv\")\n",
        "print(\"Logged SINGLE-neuron healing rows to:\", PROJECT_ROOT / \"results\" / \"healing_runs.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Worst pair from latest run: (40, 45) | acc_drop: 0.0348837209302326\n",
            "Baseline ACC=0.9535 | 95% threshold=0.9058\n",
            "PAIR — BASELINE: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "PAIR — POST-DAMAGE: {'accuracy': 0.9186046511627907, 'f1': 0.9369369369369369}\n",
            "[heal_clf_firstHidden_pairN40_45_short] epoch 001 | train 0.0595 | val 0.0524 | metric 0.9647 *\n",
            "[heal_clf_firstHidden_pairN40_45_short] Early stopping at epoch 6. Best val_loss=0.0524\n",
            "PAIR — SHORT TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629} | recovery_epoch_95%: 1\n",
            "[heal_clf_firstHidden_pairN40_45_long] epoch 001 | train 0.0464 | val 0.0522 | metric 0.9647 *\n",
            "[heal_clf_firstHidden_pairN40_45_long] epoch 010 | train 0.0526 | val 0.0724 | metric 0.9647 \n",
            "[heal_clf_firstHidden_pairN40_45_long] epoch 020 | train 0.0488 | val 0.0667 | metric 0.9647 \n",
            "[heal_clf_firstHidden_pairN40_45_long] Early stopping at epoch 21. Best val_loss=0.0522\n",
            "PAIR — LONG TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629} | recovery_epoch_95%: 1\n",
            "Logged PAIR healing rows to: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n"
          ]
        }
      ],
      "source": [
        "# TAGS: experiment-run, heal, short-long, writes-logs\n",
        "import pandas as pd\n",
        "\n",
        "set_seed(42)\n",
        "first_lin_clf = layer_seq_indices(clf_ctor)[0]\n",
        "\n",
        "# Read the latest pair-scan (aggregate) & select worst pair by acc_drop from your last run_id\n",
        "df_pairs = pd.read_csv(PROJECT_ROOT / \"results\" / \"per_pair_scan_clf_first.csv\")\n",
        "# pick the most recent run_id by max string (timestamp format sorts lexicographically)\n",
        "latest_run = df_pairs[\"run_id\"].max()\n",
        "df_latest = df_pairs[df_pairs[\"run_id\"] == latest_run]\n",
        "row_worst = df_latest.sort_values(\"acc_drop\", ascending=False).iloc[0]\n",
        "pair = (int(row_worst[\"neuron_i\"]), int(row_worst[\"neuron_j\"]))\n",
        "print(\"Worst pair from latest run:\", pair, \"| acc_drop:\", float(row_worst[\"acc_drop\"]))\n",
        "\n",
        "# Baseline (for threshold)\n",
        "_, _, test_loader_clf, _ = make_loaders(BREAST_DIR, task=\"clf\", batch_size=512)\n",
        "baseline_model = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "baseline_metrics = evaluate_model(baseline_model, test_loader_clf, task=\"clf\")\n",
        "base_acc = baseline_metrics[\"accuracy\"]; base_f1 = baseline_metrics[\"f1\"]\n",
        "print(f\"Baseline ACC={base_acc:.4f} | 95% threshold={0.95*base_acc:.4f}\")\n",
        "\n",
        "# Create damaged model (specific pair) + masks\n",
        "damaged_P, mask_P, train_P, val_P, test_P, P_before, P_after = apply_specific_neuron_damage_with_masks(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_lin_seq_index=first_lin_clf, neuron_indices=list(pair)\n",
        ")\n",
        "print(\"PAIR — BASELINE:\", P_before)\n",
        "print(\"PAIR — POST-DAMAGE:\", P_after)\n",
        "\n",
        "# Short heal\n",
        "P_short_name = f\"heal_clf_firstHidden_pairN{pair[0]}_{pair[1]}_short\"\n",
        "bestP_s, histP_s = healing_train_constrained(\n",
        "    model=damaged_P, mask_dict=mask_P, train_loader=train_P, val_loader=val_P, task=\"clf\",\n",
        "    max_epochs=15, lr=1e-3, weight_decay=1e-4, patience=5, run_name=P_short_name\n",
        ")\n",
        "mP_s = clf_ctor().to(DEVICE); mP_s.load_state_dict(torch.load(bestP_s, map_location=DEVICE))\n",
        "metricsP_s = evaluate_model(mP_s, test_P, task=\"clf\")\n",
        "recov_epoch_s = recovery_epoch_95pct_of_baseline(histP_s, base_acc)\n",
        "print(\"PAIR — SHORT TEST:\", metricsP_s, \"| recovery_epoch_95%:\", recov_epoch_s)\n",
        "figP_s_loss, figP_s_metric = plot_history(histP_s, P_short_name, task=\"clf\")\n",
        "\n",
        "# Long heal (fresh damaged state)\n",
        "damaged_P2, mask_P2, train_P2, val_P2, test_P2, _, _ = apply_specific_neuron_damage_with_masks(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_lin_seq_index=first_lin_clf, neuron_indices=list(pair)\n",
        ")\n",
        "P_long_name = f\"heal_clf_firstHidden_pairN{pair[0]}_{pair[1]}_long\"\n",
        "bestP_l, histP_l = healing_train_constrained(\n",
        "    model=damaged_P2, mask_dict=mask_P2, train_loader=train_P2, val_loader=val_P2, task=\"clf\",\n",
        "    max_epochs=150, lr=1e-3, weight_decay=1e-4, patience=20, run_name=P_long_name\n",
        ")\n",
        "mP_l = clf_ctor().to(DEVICE); mP_l.load_state_dict(torch.load(bestP_l, map_location=DEVICE))\n",
        "metricsP_l = evaluate_model(mP_l, test_P2, task=\"clf\")\n",
        "recov_epoch_l = recovery_epoch_95pct_of_baseline(histP_l, base_acc)\n",
        "print(\"PAIR — LONG TEST:\", metricsP_l, \"| recovery_epoch_95%:\", recov_epoch_l)\n",
        "figP_l_loss, figP_l_metric = plot_history(histP_l, P_long_name, task=\"clf\")\n",
        "\n",
        "# Log\n",
        "rowsP = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"pair_baseline\", \"layer_seq_index\": int(first_lin_clf), \"neurons\": str(pair),\n",
        "     \"accuracy\": float(P_before[\"accuracy\"]), \"f1\": float(P_before[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"pair_post_damage\", \"layer_seq_index\": int(first_lin_clf), \"neurons\": str(pair),\n",
        "     \"accuracy\": float(P_after[\"accuracy\"]), \"f1\": float(P_after[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"pair_healed_short\", \"layer_seq_index\": int(first_lin_clf), \"neurons\": str(pair),\n",
        "     \"epochs\": len(histP_s[\"epoch\"]), \"recovery_epoch_95p\": recov_epoch_s,\n",
        "     \"accuracy\": float(metricsP_s[\"accuracy\"]), \"f1\": float(metricsP_s[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestP_s), \"fig_loss\": figP_s_loss, \"fig_metric\": figP_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"pair_healed_long\", \"layer_seq_index\": int(first_lin_clf), \"neurons\": str(pair),\n",
        "     \"epochs\": len(histP_l[\"epoch\"]), \"recovery_epoch_95p\": recov_epoch_l,\n",
        "     \"accuracy\": float(metricsP_l[\"accuracy\"]), \"f1\": float(metricsP_l[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestP_l), \"fig_loss\": figP_l_loss, \"fig_metric\": figP_l_metric},\n",
        "]\n",
        "for r in rowsP: log_result(r, csv_name=\"healing_runs.csv\")\n",
        "print(\"Logged PAIR healing rows to:\", PROJECT_ROOT / \"results\" / \"healing_runs.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Middle hidden layer seq_index: 3\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "           timestamp             phase        dataset task damage_type  \\\n",
            "130  20250813-125317  damage_immediate  breast_cancer  clf     neurons   \n",
            "131  20250813-125318  damage_immediate  breast_cancer  clf     neurons   \n",
            "132  20250813-125318  damage_immediate  breast_cancer  clf     neurons   \n",
            "133  20250813-125318  damage_immediate  breast_cancer  clf     neurons   \n",
            "134  20250813-125318  damage_immediate  breast_cancer  clf     neurons   \n",
            "135  20250813-125318  damage_immediate  breast_cancer  clf     neurons   \n",
            "136  20250813-125318  damage_immediate  breast_cancer  clf     neurons   \n",
            "137  20250813-125318  damage_immediate  breast_cancer  clf     neurons   \n",
            "138  20250813-125318  damage_immediate  breast_cancer  clf     neurons   \n",
            "139  20250813-125318  damage_immediate  breast_cancer  clf     neurons   \n",
            "140  20250813-125318  damage_immediate  breast_cancer  clf     neurons   \n",
            "141  20250813-125318  damage_immediate  breast_cancer  clf     neurons   \n",
            "\n",
            "     layer_seq_index  pct  repeats_total  repeat_eval  \\\n",
            "130                3  0.4              5            5   \n",
            "131                3  0.4             10            0   \n",
            "132                3  0.4             10            1   \n",
            "133                3  0.4             10            2   \n",
            "134                3  0.4             10            3   \n",
            "135                3  0.4             10            4   \n",
            "136                3  0.4             10            5   \n",
            "137                3  0.4             10            6   \n",
            "138                3  0.4             10            7   \n",
            "139                3  0.4             10            8   \n",
            "140                3  0.4             10            9   \n",
            "141                3  0.4             10           10   \n",
            "\n",
            "                                                  ckpt    metric  \\\n",
            "130  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "131  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "132  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "133  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "134  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "135  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "136  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "137  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "138  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "139  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "140  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "141  c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf...  accuracy   \n",
            "\n",
            "     baseline_metric     value  delta_from_baseline        f1  mae  \n",
            "130         0.953488  0.767442             0.186047  0.830508  NaN  \n",
            "131         0.953488  0.953488             0.000000  0.962963  NaN  \n",
            "132         0.953488  0.918605             0.034884  0.939130  NaN  \n",
            "133         0.953488  0.918605             0.034884  0.939130  NaN  \n",
            "134         0.953488  0.883721             0.069767  0.910714  NaN  \n",
            "135         0.953488  0.767442             0.186047  0.830508  NaN  \n",
            "136         0.953488  0.767442             0.186047  0.830508  NaN  \n",
            "137         0.953488  0.372093             0.581395  0.000000  NaN  \n",
            "138         0.953488  0.372093             0.581395  0.000000  NaN  \n",
            "139         0.953488  0.372093             0.581395  0.000000  NaN  \n",
            "140         0.953488  0.372093             0.581395  0.000000  NaN  \n",
            "141         0.953488  0.372093             0.581395  0.000000  NaN  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "set_seed(42)\n",
        "\n",
        "# middle hidden Linear seq index (classifier): [0, 3, 6, 9] → take the second = 3\n",
        "middle_lin_clf = layer_seq_indices(clf_ctor)[1]\n",
        "print(\"Middle hidden layer seq_index:\", middle_lin_clf)\n",
        "\n",
        "# 20% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=middle_lin_clf, pct=0.20, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# 40% neurons, repeats 5 and 10\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=middle_lin_clf, pct=0.40, repeats_list=[5, 10],\n",
        "    damage_type=\"neurons\", csv_name=\"damage_runs.csv\", dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# Peek just these rows\n",
        "import pandas as pd\n",
        "df = pd.read_csv(PROJECT_ROOT/\"results/damage_runs.csv\")\n",
        "mask = (\n",
        "    (df[\"dataset\"]==\"breast_cancer\") & (df[\"task\"]==\"clf\") &\n",
        "    (df[\"damage_type\"]==\"neurons\") & (df[\"layer_seq_index\"]==middle_lin_clf) &\n",
        "    (df[\"pct\"].isin([0.2, 0.4])) & (df[\"repeats_total\"].isin([5,10]))\n",
        ")\n",
        "print(df[mask].tail(12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning middle layer seq_index=3 with 64 neurons...\n",
            "Baseline ACC=0.9535 | F1=0.9630\n",
            "  scanned 8/64\n",
            "  scanned 16/64\n",
            "  scanned 24/64\n",
            "  scanned 32/64\n",
            "  scanned 40/64\n",
            "  scanned 48/64\n",
            "  scanned 56/64\n",
            "  scanned 64/64\n",
            "\n",
            "Top-10 neurons by ACC drop (middle layer):\n",
            "  neuron_index  acc_drop  f1_drop      acc       f1\n",
            "            5  0.011628 0.008835 0.941860 0.954128\n",
            "           62  0.011628 0.008835 0.941860 0.954128\n",
            "            3  0.000000 0.000000 0.953488 0.962963\n",
            "            0  0.000000 0.000000 0.953488 0.962963\n",
            "            6  0.000000 0.000000 0.953488 0.962963\n",
            "            4  0.000000 0.000000 0.953488 0.962963\n",
            "            7  0.000000 0.000000 0.953488 0.962963\n",
            "            1  0.000000 0.000000 0.953488 0.962963\n",
            "           15  0.000000 0.000000 0.953488 0.962963\n",
            "            9  0.000000 0.000000 0.953488 0.962963\n",
            "Top-3 neurons (middle): [5, 62, 3]\n",
            "Figure saved: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\per_neuron_scan_clf_middle_accdrop.png\n"
          ]
        }
      ],
      "source": [
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Middle layer seq index again\n",
        "middle_lin_clf = layer_seq_indices(clf_ctor)[1]\n",
        "\n",
        "# Find number of neurons in this layer\n",
        "tmp = clf_ctor().to(DEVICE)\n",
        "mid_info = [d for d in summarize_linear_layers(tmp) if d[\"seq_index\"] == middle_lin_clf][0]\n",
        "n_mid = mid_info[\"out_features\"]\n",
        "print(f\"Scanning middle layer seq_index={middle_lin_clf} with {n_mid} neurons...\")\n",
        "\n",
        "# Test loader\n",
        "_, _, test_loader_clf, _ = make_loaders(BREAST_DIR, task=\"clf\", batch_size=512)\n",
        "\n",
        "# Baseline\n",
        "baseline_model = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "baseline_metrics = evaluate_model(baseline_model, test_loader_clf, task=\"clf\")\n",
        "base_acc = baseline_metrics[\"accuracy\"]; base_f1 = baseline_metrics[\"f1\"]\n",
        "print(f\"Baseline ACC={base_acc:.4f} | F1={base_f1:.4f}\")\n",
        "\n",
        "rows = []\n",
        "for i in range(n_mid):\n",
        "    m = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "    damage_specific_neurons_in_layer(m, layer_lin_seq_index=middle_lin_clf, neuron_indices=[i])\n",
        "    metrics = evaluate_model(m, test_loader_clf, task=\"clf\")\n",
        "    acc, f1 = metrics[\"accuracy\"], metrics[\"f1\"]\n",
        "    rows.append({\n",
        "        \"timestamp\": timestamp(),\n",
        "        \"phase\": \"per_neuron_scan\",\n",
        "        \"dataset\": \"breast_cancer\",\n",
        "        \"task\": \"clf\",\n",
        "        \"layer_seq_index\": int(middle_lin_clf),\n",
        "        \"neuron_index\": int(i),\n",
        "        \"acc\": float(acc),\n",
        "        \"f1\": float(f1),\n",
        "        \"acc_drop\": float(base_acc - acc),\n",
        "        \"f1_drop\": float(base_f1 - f1),\n",
        "        \"ckpt\": CLF_CKPT\n",
        "    })\n",
        "    if (i+1) % 8 == 0 or i == n_mid-1:\n",
        "        print(f\"  scanned {i+1}/{n_mid}\")\n",
        "\n",
        "# Save per-neuron scan (middle)\n",
        "for r in rows:\n",
        "    log_result(r, csv_name=\"per_neuron_scan_clf_middle.csv\")\n",
        "\n",
        "import pandas as pd, json\n",
        "scan_csv = PROJECT_ROOT / \"results\" / \"per_neuron_scan_clf_middle.csv\"\n",
        "df_scan = pd.read_csv(scan_csv)\n",
        "\n",
        "# Sort by drop\n",
        "df_sorted = df_scan.sort_values(\"acc_drop\", ascending=False)\n",
        "top10 = df_sorted.head(10)[[\"neuron_index\",\"acc_drop\",\"f1_drop\",\"acc\",\"f1\"]]\n",
        "print(\"\\nTop-10 neurons by ACC drop (middle layer):\\n\", top10.to_string(index=False))\n",
        "\n",
        "# Save sorted + Top-3 file\n",
        "df_sorted.to_csv(PROJECT_ROOT / \"results\" / \"per_neuron_scan_clf_middle_sorted.csv\", index=False)\n",
        "top3_mid = df_sorted.head(3)[\"neuron_index\"].tolist()\n",
        "with open(PROJECT_ROOT / \"results\" / \"per_neuron_top3_clf_middle.json\",\"w\") as f:\n",
        "    json.dump({\"layer_seq_index\": int(middle_lin_clf), \"top3\": top3_mid}, f, indent=2)\n",
        "print(\"Top-3 neurons (middle):\", top3_mid)\n",
        "\n",
        "# Optional quick plot\n",
        "fig = plt.figure(figsize=(8,4))\n",
        "plt.bar(df_scan[\"neuron_index\"].values, df_scan[\"acc_drop\"].values)\n",
        "plt.xlabel(\"neuron index (middle hidden layer)\"); plt.ylabel(\"Δ accuracy vs baseline\")\n",
        "plt.title(\"Per-neuron damage impact (classification, middle hidden)\")\n",
        "fig_path = FIG_DIR / \"per_neuron_scan_clf_middle_accdrop.png\"\n",
        "plt.savefig(fig_path, bbox_inches=\"tight\"); plt.close()\n",
        "print(\"Figure saved:\", fig_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Worst middle-layer neuron: 5\n",
            "Baseline ACC=0.9535 | 95% threshold=0.9058\n",
            "MIDDLE SINGLE — BASELINE: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "MIDDLE SINGLE — POST-DAMAGE: {'accuracy': 0.9418604651162791, 'f1': 0.9541284403669725}\n",
            "[heal_clf_middle_singleN5_short] epoch 001 | train 0.0480 | val 0.0507 | metric 0.9765 *\n",
            "[heal_clf_middle_singleN5_short] Early stopping at epoch 6. Best val_loss=0.0507\n",
            "MIDDLE SINGLE — SHORT TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629} | recovery_epoch_95%: 1\n",
            "[heal_clf_middle_singleN5_long] epoch 001 | train 0.0447 | val 0.0507 | metric 0.9765 *\n",
            "[heal_clf_middle_singleN5_long] epoch 010 | train 0.0522 | val 0.0692 | metric 0.9765 \n",
            "[heal_clf_middle_singleN5_long] epoch 020 | train 0.0484 | val 0.0650 | metric 0.9765 \n",
            "[heal_clf_middle_singleN5_long] Early stopping at epoch 21. Best val_loss=0.0507\n",
            "MIDDLE SINGLE — LONG TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629} | recovery_epoch_95%: 1\n",
            "Logged MIDDLE single-neuron healing rows to: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "set_seed(42)\n",
        "middle_lin_clf = layer_seq_indices(clf_ctor)[1]\n",
        "\n",
        "# Load worst single neuron from sorted CSV we just created\n",
        "df_mid = pd.read_csv(PROJECT_ROOT / \"results\" / \"per_neuron_scan_clf_middle_sorted.csv\")\n",
        "worst_mid_neuron = int(df_mid.iloc[0][\"neuron_index\"])\n",
        "print(\"Worst middle-layer neuron:\", worst_mid_neuron)\n",
        "\n",
        "# Baseline for threshold\n",
        "_, _, test_loader_clf, _ = make_loaders(BREAST_DIR, task=\"clf\", batch_size=512)\n",
        "baseline_model = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "baseline_metrics = evaluate_model(baseline_model, test_loader_clf, task=\"clf\")\n",
        "base_acc = baseline_metrics[\"accuracy\"]; base_f1 = baseline_metrics[\"f1\"]\n",
        "print(f\"Baseline ACC={base_acc:.4f} | 95% threshold={0.95*base_acc:.4f}\")\n",
        "\n",
        "# Create damaged model (specific neuron) + masks\n",
        "damaged_M, mask_M, train_M, val_M, test_M, M_before, M_after = apply_specific_neuron_damage_with_masks(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_lin_seq_index=middle_lin_clf, neuron_indices=[worst_mid_neuron]\n",
        ")\n",
        "print(\"MIDDLE SINGLE — BASELINE:\", M_before)\n",
        "print(\"MIDDLE SINGLE — POST-DAMAGE:\", M_after)\n",
        "\n",
        "# Short heal (≈15 epochs)\n",
        "M_short_name = f\"heal_clf_middle_singleN{worst_mid_neuron}_short\"\n",
        "bestM_s, histM_s = healing_train_constrained(\n",
        "    model=damaged_M, mask_dict=mask_M, train_loader=train_M, val_loader=val_M, task=\"clf\",\n",
        "    max_epochs=15, lr=1e-3, weight_decay=1e-4, patience=5, run_name=M_short_name\n",
        ")\n",
        "mM_s = clf_ctor().to(DEVICE); mM_s.load_state_dict(torch.load(bestM_s, map_location=DEVICE))\n",
        "metricsM_s = evaluate_model(mM_s, test_M, task=\"clf\")\n",
        "recov_epoch_s = recovery_epoch_95pct_of_baseline(histM_s, base_acc)\n",
        "print(\"MIDDLE SINGLE — SHORT TEST:\", metricsM_s, \"| recovery_epoch_95%:\", recov_epoch_s)\n",
        "figM_s_loss, figM_s_metric = plot_history(histM_s, M_short_name, task=\"clf\")\n",
        "\n",
        "# Long heal (fresh same damage)\n",
        "damaged_M2, mask_M2, train_M2, val_M2, test_M2, _, _ = apply_specific_neuron_damage_with_masks(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_lin_seq_index=middle_lin_clf, neuron_indices=[worst_mid_neuron]\n",
        ")\n",
        "M_long_name = f\"heal_clf_middle_singleN{worst_mid_neuron}_long\"\n",
        "bestM_l, histM_l = healing_train_constrained(\n",
        "    model=damaged_M2, mask_dict=mask_M2, train_loader=train_M2, val_loader=val_M2, task=\"clf\",\n",
        "    max_epochs=150, lr=1e-3, weight_decay=1e-4, patience=20, run_name=M_long_name\n",
        ")\n",
        "mM_l = clf_ctor().to(DEVICE); mM_l.load_state_dict(torch.load(bestM_l, map_location=DEVICE))\n",
        "metricsM_l = evaluate_model(mM_l, test_M2, task=\"clf\")\n",
        "recov_epoch_l = recovery_epoch_95pct_of_baseline(histM_l, base_acc)\n",
        "print(\"MIDDLE SINGLE — LONG TEST:\", metricsM_l, \"| recovery_epoch_95%:\", recov_epoch_l)\n",
        "figM_l_loss, figM_l_metric = plot_history(histM_l, M_long_name, task=\"clf\")\n",
        "\n",
        "# Log\n",
        "rowsM = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"middle_single_baseline\", \"layer_seq_index\": int(middle_lin_clf), \"neuron\": worst_mid_neuron,\n",
        "     \"accuracy\": float(M_before[\"accuracy\"]), \"f1\": float(M_before[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"middle_single_post_damage\", \"layer_seq_index\": int(middle_lin_clf), \"neuron\": worst_mid_neuron,\n",
        "     \"accuracy\": float(M_after[\"accuracy\"]), \"f1\": float(M_after[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"middle_single_healed_short\", \"layer_seq_index\": int(middle_lin_clf), \"neuron\": worst_mid_neuron,\n",
        "     \"epochs\": len(histM_s[\"epoch\"]), \"recovery_epoch_95p\": recov_epoch_s,\n",
        "     \"accuracy\": float(metricsM_s[\"accuracy\"]), \"f1\": float(metricsM_s[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestM_s), \"fig_loss\": figM_s_loss, \"fig_metric\": figM_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"middle_single_healed_long\", \"layer_seq_index\": int(middle_lin_clf), \"neuron\": worst_mid_neuron,\n",
        "     \"epochs\": len(histM_l[\"epoch\"]), \"recovery_epoch_95p\": recov_epoch_l,\n",
        "     \"accuracy\": float(metricsM_l[\"accuracy\"]), \"f1\": float(metricsM_l[\"f1\"]),\n",
        "     \"best_ckpt\": str(bestM_l), \"fig_loss\": figM_l_loss, \"fig_metric\": figM_l_metric},\n",
        "]\n",
        "for r in rowsM: log_result(r, csv_name=\"healing_runs.csv\")\n",
        "print(\"Logged MIDDLE single-neuron healing rows to:\", PROJECT_ROOT / \"results\" / \"healing_runs.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rerun\n",
        "def run_damage_experiment(\n",
        "    model_ctor, ckpt_path, task, data_dir,\n",
        "    layer_seq_index, pct, repeats_list,\n",
        "    damage_type=\"neurons\", weight_mode=\"random\",\n",
        "    rng_seed=12345, csv_name=\"damage_runs.csv\",\n",
        "    dataset_name=None\n",
        "):\n",
        "    \"\"\"\n",
        "    For each repeats in repeats_list:\n",
        "      - reload clean checkpoint\n",
        "      - apply progressive damage (repeats times) on given layer\n",
        "      - evaluate after each repeat\n",
        "      - log baseline + each step with deltas\n",
        "    \"\"\"\n",
        "    assert damage_type in (\"neurons\",\"weights\")\n",
        "    rows = []\n",
        "    for repeats in repeats_list:\n",
        "        if damage_type == \"neurons\":\n",
        "            res = eval_task(\n",
        "                model_ctor=model_ctor, ckpt_path=ckpt_path, task=task, data_dir=data_dir,\n",
        "                damage_fn=damage_neurons_in_layer, repeats=repeats,\n",
        "                layer_lin_seq_index=layer_seq_index, pct_neurons=pct\n",
        "            )\n",
        "        else:\n",
        "            res = eval_task(\n",
        "                model_ctor=model_ctor, ckpt_path=ckpt_path, task=task, data_dir=data_dir,\n",
        "                damage_fn=damage_weights_in_layer, repeats=repeats,\n",
        "                layer_lin_seq_index=layer_seq_index, pct_weights=pct, mode=weight_mode\n",
        "            )\n",
        "\n",
        "        base = res[0][\"metrics\"]\n",
        "        for step in res:\n",
        "            r = step[\"repeat\"]; m = step[\"metrics\"]\n",
        "            row = {\n",
        "                \"timestamp\": timestamp(),\n",
        "                \"phase\": \"damage_immediate\",\n",
        "                \"dataset\": dataset_name,\n",
        "                \"task\": task,\n",
        "                \"damage_type\": damage_type,\n",
        "                \"layer_seq_index\": int(layer_seq_index),\n",
        "                \"pct\": float(pct),\n",
        "                \"repeats_total\": int(repeats),\n",
        "                \"repeat_eval\": int(r),\n",
        "                \"ckpt\": ckpt_path\n",
        "            }\n",
        "            if damage_type == \"weights\":\n",
        "                row[\"weight_mode\"] = weight_mode\n",
        "\n",
        "            if task == \"clf\":\n",
        "                row.update({\n",
        "                    \"metric\": \"accuracy\",\n",
        "                    \"baseline_metric\": float(base[\"accuracy\"]),\n",
        "                    \"value\": float(m[\"accuracy\"]),\n",
        "                    \"delta_from_baseline\": float(base[\"accuracy\"] - m[\"accuracy\"]),\n",
        "                    \"f1\": float(m[\"f1\"])\n",
        "                })\n",
        "            else:\n",
        "                row.update({\n",
        "                    \"metric\": \"rmse\",\n",
        "                    \"baseline_metric\": float(base[\"rmse\"]),\n",
        "                    \"value\": float(m[\"rmse\"]),\n",
        "                    \"delta_from_baseline\": float(m[\"rmse\"] - base[\"rmse\"]),\n",
        "                    \"mae\": float(m[\"mae\"])\n",
        "                })\n",
        "            rows.append(row)\n",
        "\n",
        "    for row in rows:\n",
        "        log_result(row, csv_name=csv_name)\n",
        "    print(f\"Logged {len(rows)} rows to {PROJECT_ROOT/'results'/csv_name}\")\n",
        "    return rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Middle hidden layer seq_index: 3\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "           timestamp  pct  repeats_total  repeat_eval weight_mode  \\\n",
            "174  20250813-130008  0.4             10            9      random   \n",
            "173  20250813-130008  0.4             10            8      random   \n",
            "175  20250813-130008  0.4             10           10      random   \n",
            "172  20250813-130008  0.4             10            7      random   \n",
            "158  20250813-130008  0.2             10           10      random   \n",
            "164  20250813-130008  0.4              5            5      random   \n",
            "170  20250813-130008  0.4             10            5      random   \n",
            "155  20250813-130008  0.2             10            7      random   \n",
            "171  20250813-130008  0.4             10            6      random   \n",
            "162  20250813-130008  0.4              5            3      random   \n",
            "157  20250813-130008  0.2             10            9      random   \n",
            "168  20250813-130008  0.4             10            3      random   \n",
            "\n",
            "     baseline_metric     value  delta_from_baseline  \n",
            "174         0.953488  0.244186             0.709302  \n",
            "173         0.953488  0.313953             0.639535  \n",
            "175         0.953488  0.558140             0.395349  \n",
            "172         0.953488  0.755814             0.197674  \n",
            "158         0.953488  0.779070             0.174419  \n",
            "164         0.953488  0.860465             0.093023  \n",
            "170         0.953488  0.860465             0.093023  \n",
            "155         0.953488  0.906977             0.046512  \n",
            "171         0.953488  0.906977             0.046512  \n",
            "162         0.953488  0.918605             0.034884  \n",
            "157         0.953488  0.918605             0.034884  \n",
            "168         0.953488  0.918605             0.034884  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "set_seed(42)\n",
        "\n",
        "middle_lin_clf = layer_seq_indices(clf_ctor)[1]  # seq index 3 for classifier\n",
        "print(\"Middle hidden layer seq_index:\", middle_lin_clf)\n",
        "\n",
        "# RANDOM weights: 20% + 40%, repeats [5,10]\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=middle_lin_clf, pct=0.20, repeats_list=[5,10],\n",
        "    damage_type=\"weights\", weight_mode=\"random\", csv_name=\"damage_runs.csv\",\n",
        "    dataset_name=\"breast_cancer\"\n",
        ")\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=middle_lin_clf, pct=0.40, repeats_list=[5,10],\n",
        "    damage_type=\"weights\", weight_mode=\"random\", csv_name=\"damage_runs.csv\",\n",
        "    dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# BLOCK weights: 20% + 40%, repeats [5,10]\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=middle_lin_clf, pct=0.20, repeats_list=[5,10],\n",
        "    damage_type=\"weights\", weight_mode=\"block\", csv_name=\"damage_runs.csv\",\n",
        "    dataset_name=\"breast_cancer\"\n",
        ")\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "    layer_seq_index=middle_lin_clf, pct=0.40, repeats_list=[5,10],\n",
        "    damage_type=\"weights\", weight_mode=\"block\", csv_name=\"damage_runs.csv\",\n",
        "    dataset_name=\"breast_cancer\"\n",
        ")\n",
        "\n",
        "# Show the worst drops we just logged (sorted)\n",
        "import pandas as pd\n",
        "df = pd.read_csv(PROJECT_ROOT/\"results/damage_runs.csv\")\n",
        "mask = (\n",
        "    (df[\"dataset\"]==\"breast_cancer\") &\n",
        "    (df[\"task\"]==\"clf\") &\n",
        "    (df[\"damage_type\"]==\"weights\") &\n",
        "    (df[\"layer_seq_index\"]==middle_lin_clf) &\n",
        "    (df[\"pct\"].isin([0.2,0.4])) &\n",
        "    (df[\"repeats_total\"].isin([5,10]))\n",
        ")\n",
        "view = df[mask].copy()\n",
        "view = view.sort_values(\"delta_from_baseline\", ascending=False)\n",
        "print(view.head(12)[[\"timestamp\",\"pct\",\"repeats_total\",\"repeat_eval\",\"weight_mode\",\"baseline_metric\",\"value\",\"delta_from_baseline\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TAGS: define-once, healing-utils\n",
        "def damage_weights_in_layer_with_masks(model: nn.Module, layer_lin_seq_index: int,\n",
        "                                       pct_weights: float, rng: np.random.Generator,\n",
        "                                       mode=\"random\", mask_dict=None):\n",
        "    \"\"\"\n",
        "    Zero a percentage of weights in the given Linear layer and freeze them via masks.\n",
        "    - mode='random': independent random positions\n",
        "    - mode='block' : one contiguous rectangular block (approx area = pct * total)\n",
        "    \"\"\"\n",
        "    linear_layers = get_linear_layers(model)\n",
        "    target_lin = None\n",
        "    for seq_i, lin in linear_layers:\n",
        "        if seq_i == layer_lin_seq_index:\n",
        "            target_lin = lin\n",
        "            break\n",
        "    assert target_lin is not None, f\"No Linear layer at seq index {layer_lin_seq_index}\"\n",
        "\n",
        "    if mask_dict is None:\n",
        "        mask_dict = init_full_keep_masks_for_model(model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        W = target_lin.weight  # [out, in]\n",
        "        H, K = W.shape\n",
        "        total = H * K\n",
        "        n_dmg = max(1, int(round(pct_weights * total)))\n",
        "\n",
        "        # ensure mask exists\n",
        "        if W not in mask_dict:\n",
        "            mask_dict[W] = _ensure_mask(W, 1.0)\n",
        "        mW = mask_dict[W]\n",
        "\n",
        "        if mode == \"random\":\n",
        "            idx = rng.choice(total, size=n_dmg, replace=False)\n",
        "            rows = (idx // K).astype(int)\n",
        "            cols = (idx % K).astype(int)\n",
        "            W[rows, cols] = 0.0\n",
        "            mW[rows, cols] = 0.0\n",
        "        elif mode == \"block\":\n",
        "            h = max(1, int(round(np.sqrt(n_dmg))))\n",
        "            k = max(1, int(round(n_dmg / h)))\n",
        "            r0 = int(rng.integers(0, max(1, H - h + 1)))\n",
        "            c0 = int(rng.integers(0, max(1, K - k + 1)))\n",
        "            W[r0:r0+h, c0:c0+k] = 0.0\n",
        "            mW[r0:r0+h, c0:c0+k] = 0.0\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'random' or 'block'\")\n",
        "\n",
        "    return n_dmg, mask_dict\n",
        "\n",
        "def apply_progressive_weight_damage_with_masks(model_ctor, ckpt_path, task, data_dir,\n",
        "                                               layer_lin_seq_index, pct_weights, repeats,\n",
        "                                               mode=\"random\", seed=12345):\n",
        "    \"\"\"\n",
        "    Load clean model, apply progressive weight damage (repeats times), updating masks, then return loaders+metrics.\n",
        "    \"\"\"\n",
        "    if task == \"clf\":\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"clf\", batch_size=256)\n",
        "    else:\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"reg\", batch_size=512)\n",
        "\n",
        "    model = clone_from_checkpoint(model_ctor, ckpt_path)\n",
        "    metrics_before = evaluate_model(model, test_loader, task)\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    mask_dict = init_full_keep_masks_for_model(model)\n",
        "\n",
        "    for _ in range(repeats):\n",
        "        _, mask_dict = damage_weights_in_layer_with_masks(\n",
        "            model, layer_lin_seq_index=layer_lin_seq_index,\n",
        "            pct_weights=pct_weights, rng=rng, mode=mode, mask_dict=mask_dict\n",
        "        )\n",
        "\n",
        "    metrics_after = evaluate_model(model, test_loader, task)\n",
        "    return model, mask_dict, train_loader, val_loader, test_loader, metrics_before, metrics_after\n",
        "\n",
        "def recovery_epoch_95pct_of_baseline(history, base_acc):\n",
        "    thr = 0.95 * base_acc\n",
        "    for e, v in zip(history[\"epoch\"], history[\"val_metric\"]):\n",
        "        if v >= thr:\n",
        "            return int(e)\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Worst RANDOM: {'pct': 0.4, 'repeats_total': 10, 'delta_from_baseline': 0.3953488372093023, 'value': 0.5581395348837209, 'baseline_metric': 0.9534883720930232}\n",
            "Worst BLOCK : {'pct': 0.4, 'repeats_total': 10, 'delta_from_baseline': 0.0348837209302326, 'value': 0.9186046511627908, 'baseline_metric': 0.9534883720930232}\n",
            "Baseline ACC=0.9535 | 95% threshold=0.9058\n",
            "\n",
            "middle_weights_random — BASELINE: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "middle_weights_random — POST-DAMAGE: {'accuracy': 0.5581395348837209, 'f1': 0.4722222222222222}\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 001 | train 0.6913 | val 0.6856 | metric 0.5647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 002 | train 0.6842 | val 0.6781 | metric 0.6588 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 003 | train 0.6755 | val 0.6712 | metric 0.6471 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 004 | train 0.6688 | val 0.6646 | metric 0.6235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 005 | train 0.6607 | val 0.6584 | metric 0.6235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 006 | train 0.6537 | val 0.6522 | metric 0.6235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 007 | train 0.6467 | val 0.6462 | metric 0.6235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 008 | train 0.6410 | val 0.6403 | metric 0.6235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 009 | train 0.6357 | val 0.6343 | metric 0.6235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 010 | train 0.6254 | val 0.6279 | metric 0.6353 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 011 | train 0.6250 | val 0.6213 | metric 0.6353 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 012 | train 0.6159 | val 0.6142 | metric 0.6471 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 013 | train 0.6012 | val 0.6065 | metric 0.6471 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 014 | train 0.5964 | val 0.5983 | metric 0.6471 *\n",
            "[heal_clf_middle_weights_random_p40_r10_short] epoch 015 | train 0.5848 | val 0.5896 | metric 0.6706 *\n",
            "middle_weights_random — SHORT HEAL TEST: {'accuracy': 0.6395348837209303, 'f1': 0.7769784172661871} | recovery_epoch_95%: None\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 001 | train 0.6922 | val 0.6855 | metric 0.5647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 002 | train 0.6833 | val 0.6779 | metric 0.6588 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 003 | train 0.6761 | val 0.6709 | metric 0.6588 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 004 | train 0.6674 | val 0.6644 | metric 0.6353 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 005 | train 0.6601 | val 0.6582 | metric 0.6235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 006 | train 0.6511 | val 0.6521 | metric 0.6235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 007 | train 0.6464 | val 0.6462 | metric 0.6235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 008 | train 0.6376 | val 0.6404 | metric 0.6235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 009 | train 0.6346 | val 0.6346 | metric 0.6235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 010 | train 0.6322 | val 0.6287 | metric 0.6353 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 011 | train 0.6268 | val 0.6226 | metric 0.6353 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 012 | train 0.6056 | val 0.6161 | metric 0.6471 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 013 | train 0.6021 | val 0.6091 | metric 0.6471 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 014 | train 0.5994 | val 0.6016 | metric 0.6588 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 015 | train 0.5943 | val 0.5937 | metric 0.6706 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 016 | train 0.5869 | val 0.5849 | metric 0.6706 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 017 | train 0.5886 | val 0.5754 | metric 0.6706 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 018 | train 0.5677 | val 0.5654 | metric 0.6706 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 019 | train 0.5540 | val 0.5548 | metric 0.6941 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 020 | train 0.5371 | val 0.5438 | metric 0.7294 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 021 | train 0.5453 | val 0.5324 | metric 0.7412 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 022 | train 0.5214 | val 0.5205 | metric 0.7647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 023 | train 0.5155 | val 0.5081 | metric 0.7882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 024 | train 0.4837 | val 0.4952 | metric 0.8235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 025 | train 0.4848 | val 0.4818 | metric 0.8235 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 026 | train 0.4684 | val 0.4680 | metric 0.8471 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 027 | train 0.4561 | val 0.4540 | metric 0.8706 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 028 | train 0.4598 | val 0.4397 | metric 0.8706 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 029 | train 0.4408 | val 0.4253 | metric 0.8824 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 030 | train 0.4259 | val 0.4107 | metric 0.8941 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 031 | train 0.4221 | val 0.3962 | metric 0.9059 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 032 | train 0.4044 | val 0.3817 | metric 0.8941 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 033 | train 0.3886 | val 0.3672 | metric 0.9059 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 034 | train 0.3781 | val 0.3527 | metric 0.9059 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 035 | train 0.3443 | val 0.3383 | metric 0.9176 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 036 | train 0.3413 | val 0.3240 | metric 0.9176 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 037 | train 0.3213 | val 0.3099 | metric 0.9176 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 038 | train 0.3165 | val 0.2960 | metric 0.9176 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 039 | train 0.2953 | val 0.2824 | metric 0.9176 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 040 | train 0.3027 | val 0.2693 | metric 0.9176 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 041 | train 0.2823 | val 0.2567 | metric 0.9176 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 042 | train 0.2739 | val 0.2447 | metric 0.9412 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 043 | train 0.2682 | val 0.2332 | metric 0.9412 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 044 | train 0.2682 | val 0.2226 | metric 0.9412 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 045 | train 0.2447 | val 0.2125 | metric 0.9412 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 046 | train 0.2442 | val 0.2031 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 047 | train 0.2143 | val 0.1940 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 048 | train 0.2188 | val 0.1852 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 049 | train 0.1943 | val 0.1766 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 050 | train 0.2041 | val 0.1682 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 051 | train 0.1964 | val 0.1603 | metric 0.9647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 052 | train 0.1829 | val 0.1530 | metric 0.9647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 053 | train 0.1766 | val 0.1461 | metric 0.9647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 054 | train 0.1687 | val 0.1397 | metric 0.9647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 055 | train 0.1721 | val 0.1336 | metric 0.9647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 056 | train 0.1691 | val 0.1280 | metric 0.9647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 057 | train 0.1517 | val 0.1227 | metric 0.9647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 058 | train 0.1685 | val 0.1178 | metric 0.9647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 059 | train 0.1440 | val 0.1137 | metric 0.9647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 060 | train 0.1401 | val 0.1101 | metric 0.9647 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 061 | train 0.1443 | val 0.1067 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 062 | train 0.1534 | val 0.1033 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 063 | train 0.1254 | val 0.1000 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 064 | train 0.1320 | val 0.0969 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 065 | train 0.1213 | val 0.0938 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 066 | train 0.1312 | val 0.0912 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 067 | train 0.1161 | val 0.0888 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 068 | train 0.1084 | val 0.0863 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 069 | train 0.1180 | val 0.0839 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 070 | train 0.1071 | val 0.0818 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 071 | train 0.1131 | val 0.0803 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 072 | train 0.1119 | val 0.0792 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 073 | train 0.0884 | val 0.0777 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 074 | train 0.0957 | val 0.0762 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 075 | train 0.0938 | val 0.0743 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 076 | train 0.0920 | val 0.0725 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 077 | train 0.0849 | val 0.0705 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 078 | train 0.0965 | val 0.0689 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 079 | train 0.1074 | val 0.0679 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 080 | train 0.0928 | val 0.0673 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 081 | train 0.0913 | val 0.0668 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 082 | train 0.0911 | val 0.0663 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 083 | train 0.0754 | val 0.0660 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 084 | train 0.0832 | val 0.0653 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 085 | train 0.0725 | val 0.0649 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 086 | train 0.0700 | val 0.0648 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 087 | train 0.0829 | val 0.0647 | metric 0.9882 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 088 | train 0.0867 | val 0.0644 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 089 | train 0.0801 | val 0.0642 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 090 | train 0.0800 | val 0.0646 | metric 0.9765 \n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 100 | train 0.0593 | val 0.0639 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 101 | train 0.0681 | val 0.0636 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 102 | train 0.0736 | val 0.0633 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 103 | train 0.0597 | val 0.0631 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 104 | train 0.0552 | val 0.0627 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 105 | train 0.0649 | val 0.0625 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 107 | train 0.0764 | val 0.0621 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 108 | train 0.0674 | val 0.0621 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 109 | train 0.0645 | val 0.0620 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 110 | train 0.0596 | val 0.0618 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 115 | train 0.0760 | val 0.0616 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 119 | train 0.0666 | val 0.0615 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 120 | train 0.0550 | val 0.0614 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 130 | train 0.0552 | val 0.0622 | metric 0.9765 \n",
            "[heal_clf_middle_weights_random_p40_r10_long] epoch 140 | train 0.0622 | val 0.0625 | metric 0.9765 \n",
            "[heal_clf_middle_weights_random_p40_r10_long] Early stopping at epoch 140. Best val_loss=0.0614\n",
            "middle_weights_random — LONG HEAL TEST: {'accuracy': 0.9651162790697675, 'f1': 0.9724770642201835} | recovery_epoch_95%: 31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18436\\2494450625.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df0, pd.DataFrame([row])], ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "middle_weights_block — BASELINE: {'accuracy': 0.9534883720930233, 'f1': 0.9629629629629629}\n",
            "middle_weights_block — POST-DAMAGE: {'accuracy': 0.9186046511627907, 'f1': 0.9369369369369369}\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 001 | train 0.2561 | val 0.1989 | metric 0.9412 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 002 | train 0.2391 | val 0.1796 | metric 0.9412 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 003 | train 0.2066 | val 0.1631 | metric 0.9412 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 004 | train 0.1671 | val 0.1481 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 005 | train 0.1691 | val 0.1351 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 006 | train 0.1635 | val 0.1237 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 007 | train 0.1493 | val 0.1137 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 008 | train 0.1310 | val 0.1055 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 009 | train 0.1142 | val 0.0985 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 010 | train 0.1033 | val 0.0924 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 011 | train 0.1336 | val 0.0875 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 012 | train 0.0938 | val 0.0832 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 013 | train 0.0923 | val 0.0791 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 014 | train 0.0990 | val 0.0752 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_short] epoch 015 | train 0.0855 | val 0.0717 | metric 0.9765 *\n",
            "middle_weights_block — SHORT HEAL TEST: {'accuracy': 0.9418604651162791, 'f1': 0.9541284403669725} | recovery_epoch_95%: 1\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 001 | train 0.2473 | val 0.1988 | metric 0.9412 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 002 | train 0.2306 | val 0.1786 | metric 0.9412 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 003 | train 0.1981 | val 0.1620 | metric 0.9412 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 004 | train 0.1769 | val 0.1476 | metric 0.9412 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 005 | train 0.1703 | val 0.1351 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 006 | train 0.1511 | val 0.1242 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 007 | train 0.1465 | val 0.1147 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 008 | train 0.1278 | val 0.1062 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 009 | train 0.1198 | val 0.0987 | metric 0.9529 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 010 | train 0.1138 | val 0.0926 | metric 0.9647 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 011 | train 0.1091 | val 0.0875 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 012 | train 0.1141 | val 0.0837 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 013 | train 0.0913 | val 0.0803 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 014 | train 0.0983 | val 0.0769 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 015 | train 0.1033 | val 0.0740 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 016 | train 0.0890 | val 0.0713 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 017 | train 0.0861 | val 0.0692 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 018 | train 0.0779 | val 0.0673 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 019 | train 0.0821 | val 0.0656 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 020 | train 0.0631 | val 0.0642 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 021 | train 0.0879 | val 0.0632 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 022 | train 0.0829 | val 0.0622 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 023 | train 0.0647 | val 0.0609 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 024 | train 0.0760 | val 0.0602 | metric 0.9765 *\n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 030 | train 0.0656 | val 0.0638 | metric 0.9765 \n",
            "[heal_clf_middle_weights_block_p40_r10_long] epoch 040 | train 0.0627 | val 0.0655 | metric 0.9765 \n",
            "[heal_clf_middle_weights_block_p40_r10_long] Early stopping at epoch 44. Best val_loss=0.0602\n",
            "middle_weights_block — LONG HEAL TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9636363636363636} | recovery_epoch_95%: 1\n",
            "\n",
            "Logged RANDOM/BLOCK weight-heal rows to: c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "set_seed(42)\n",
        "middle_lin_clf = layer_seq_indices(clf_ctor)[1]\n",
        "\n",
        "# Find worst rows for RANDOM and BLOCK\n",
        "df = pd.read_csv(PROJECT_ROOT/\"results/damage_runs.csv\")\n",
        "mask = (\n",
        "    (df[\"dataset\"]==\"breast_cancer\") & (df[\"task\"]==\"clf\") &\n",
        "    (df[\"damage_type\"]==\"weights\") & (df[\"layer_seq_index\"]==middle_lin_clf) &\n",
        "    (df[\"pct\"].isin([0.2,0.4])) & (df[\"repeats_total\"].isin([5,10]))\n",
        ")\n",
        "dd = df[mask].copy()\n",
        "# pick the evaluation AFTER all repeats (repeat_eval == repeats_total) to capture end-state\n",
        "dd = dd[dd[\"repeat_eval\"] == dd[\"repeats_total\"]]\n",
        "\n",
        "worst_random = dd[dd[\"weight_mode\"]==\"random\"].sort_values(\"delta_from_baseline\", ascending=False).iloc[0]\n",
        "worst_block  = dd[dd[\"weight_mode\"]==\"block\"].sort_values(\"delta_from_baseline\", ascending=False).iloc[0]\n",
        "\n",
        "print(\"Worst RANDOM:\", worst_random[[\"pct\",\"repeats_total\",\"delta_from_baseline\",\"value\",\"baseline_metric\"]].to_dict())\n",
        "print(\"Worst BLOCK :\", worst_block[[\"pct\",\"repeats_total\",\"delta_from_baseline\",\"value\",\"baseline_metric\"]].to_dict())\n",
        "\n",
        "# Baseline ACC for threshold\n",
        "_, _, test_loader_clf, _ = make_loaders(BREAST_DIR, task=\"clf\", batch_size=512)\n",
        "baseline_model = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "baseline_metrics = evaluate_model(baseline_model, test_loader_clf, task=\"clf\")\n",
        "base_acc = baseline_metrics[\"accuracy\"]; base_f1 = baseline_metrics[\"f1\"]\n",
        "print(f\"Baseline ACC={base_acc:.4f} | 95% threshold={0.95*base_acc:.4f}\")\n",
        "\n",
        "def heal_weight_case(label, pct, repeats, mode):\n",
        "    # build damaged state with masks\n",
        "    damaged, msk, tr, va, te, m_before, m_after = apply_progressive_weight_damage_with_masks(\n",
        "        model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "        layer_lin_seq_index=middle_lin_clf, pct_weights=float(pct), repeats=int(repeats),\n",
        "        mode=mode, seed=12345\n",
        "    )\n",
        "    print(f\"\\n{label} — BASELINE:\", m_before)\n",
        "    print(f\"{label} — POST-DAMAGE:\", m_after)\n",
        "\n",
        "    # short heal\n",
        "    short_name = f\"heal_clf_middle_weights_{mode}_p{int(float(pct)*100)}_r{int(repeats)}_short\"\n",
        "    best_s, hist_s = healing_train_constrained(\n",
        "        model=damaged, mask_dict=msk, train_loader=tr, val_loader=va, task=\"clf\",\n",
        "        max_epochs=15, lr=1e-3, weight_decay=1e-4, patience=5, run_name=short_name\n",
        "    )\n",
        "    m_s = clf_ctor().to(DEVICE); m_s.load_state_dict(torch.load(best_s, map_location=DEVICE))\n",
        "    metrics_s = evaluate_model(m_s, te, task=\"clf\")\n",
        "    rec_s = recovery_epoch_95pct_of_baseline(hist_s, base_acc)\n",
        "    print(f\"{label} — SHORT HEAL TEST:\", metrics_s, \"| recovery_epoch_95%:\", rec_s)\n",
        "    fig_s_loss, fig_s_metric = plot_history(hist_s, short_name, task=\"clf\")\n",
        "\n",
        "    # long heal (recreate same damage)\n",
        "    damaged2, msk2, tr2, va2, te2, _, _ = apply_progressive_weight_damage_with_masks(\n",
        "        model_ctor=clf_ctor, ckpt_path=CLF_CKPT, task=\"clf\", data_dir=BREAST_DIR,\n",
        "        layer_lin_seq_index=middle_lin_clf, pct_weights=float(pct), repeats=int(repeats),\n",
        "        mode=mode, seed=12345\n",
        "    )\n",
        "    long_name = f\"heal_clf_middle_weights_{mode}_p{int(float(pct)*100)}_r{int(repeats)}_long\"\n",
        "    best_l, hist_l = healing_train_constrained(\n",
        "        model=damaged2, mask_dict=msk2, train_loader=tr2, val_loader=va2, task=\"clf\",\n",
        "        max_epochs=150, lr=1e-3, weight_decay=1e-4, patience=20, run_name=long_name\n",
        "    )\n",
        "    m_l = clf_ctor().to(DEVICE); m_l.load_state_dict(torch.load(best_l, map_location=DEVICE))\n",
        "    metrics_l = evaluate_model(m_l, te2, task=\"clf\")\n",
        "    rec_l = recovery_epoch_95pct_of_baseline(hist_l, base_acc)\n",
        "    print(f\"{label} — LONG HEAL TEST:\", metrics_l, \"| recovery_epoch_95%:\", rec_l)\n",
        "    fig_l_loss, fig_l_metric = plot_history(hist_l, long_name, task=\"clf\")\n",
        "\n",
        "    # log\n",
        "    rows = [\n",
        "        {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "         \"stage\": f\"{label}_baseline\", \"layer_seq_index\": int(middle_lin_clf),\n",
        "         \"pct\": float(pct), \"repeats\": int(repeats), \"weight_mode\": mode,\n",
        "         \"accuracy\": float(m_before[\"accuracy\"]), \"f1\": float(m_before[\"f1\"])},\n",
        "        {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "         \"stage\": f\"{label}_post_damage\", \"layer_seq_index\": int(middle_lin_clf),\n",
        "         \"pct\": float(pct), \"repeats\": int(repeats), \"weight_mode\": mode,\n",
        "         \"accuracy\": float(m_after[\"accuracy\"]), \"f1\": float(m_after[\"f1\"])},\n",
        "        {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "         \"stage\": f\"{label}_healed_short\", \"layer_seq_index\": int(middle_lin_clf),\n",
        "         \"pct\": float(pct), \"repeats\": int(repeats), \"weight_mode\": mode,\n",
        "         \"epochs\": len(hist_s[\"epoch\"]), \"recovery_epoch_95p\": rec_s,\n",
        "         \"accuracy\": float(metrics_s[\"accuracy\"]), \"f1\": float(metrics_s[\"f1\"]),\n",
        "         \"best_ckpt\": str(best_s), \"fig_loss\": fig_s_loss, \"fig_metric\": fig_s_metric},\n",
        "        {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "         \"stage\": f\"{label}_healed_long\", \"layer_seq_index\": int(middle_lin_clf),\n",
        "         \"pct\": float(pct), \"repeats\": int(repeats), \"weight_mode\": mode,\n",
        "         \"epochs\": len(hist_l[\"epoch\"]), \"recovery_epoch_95p\": rec_l,\n",
        "         \"accuracy\": float(metrics_l[\"accuracy\"]), \"f1\": float(metrics_l[\"f1\"]),\n",
        "         \"best_ckpt\": str(best_l), \"fig_loss\": fig_l_loss, \"fig_metric\": fig_l_metric},\n",
        "    ]\n",
        "    for r in rows:\n",
        "        log_result(r, csv_name=\"healing_runs.csv\")\n",
        "\n",
        "# Heal worst RANDOM\n",
        "heal_weight_case(\"middle_weights_random\",\n",
        "                 pct=worst_random[\"pct\"], repeats=worst_random[\"repeats_total\"], mode=\"random\")\n",
        "\n",
        "# Heal worst BLOCK\n",
        "heal_weight_case(\"middle_weights_block\",\n",
        "                 pct=worst_block[\"pct\"], repeats=worst_block[\"repeats_total\"], mode=\"block\")\n",
        "\n",
        "print(\"\\nLogged RANDOM/BLOCK weight-heal rows to:\", PROJECT_ROOT / \"results\" / \"healing_runs.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning OUTPUT layer seq_index=9 with 2 neurons...\n",
            "Baseline ACC=0.9535 | F1=0.9630\n",
            "  neuron 0: ACC=0.9419, Δ=0.0116\n",
            "  neuron 1: ACC=0.9651, Δ=-0.0116\n",
            "\n",
            "Per-neuron scan (OUTPUT) sorted by ΔACC:\n",
            "  neuron_index      acc  acc_drop       f1   f1_drop\n",
            "            0 0.941860  0.011628 0.954128  0.008835\n",
            "            0 0.941860  0.011628 0.954128  0.008835\n",
            "            1 0.965116 -0.011628 0.972973 -0.010010\n",
            "            1 0.965116 -0.011628 0.972973 -0.010010\n",
            "Worst single OUTPUT neuron: 0\n",
            "Figure saved: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\per_neuron_scan_clf_output_accdrop.png\n"
          ]
        }
      ],
      "source": [
        "# Per-neuron scan: OUTPUT layer (classification)\n",
        "set_seed(42)\n",
        "\n",
        "last_lin_clf = layer_seq_indices(clf_ctor)[-1]\n",
        "tmp = clf_ctor().to(DEVICE)\n",
        "out_info = [d for d in summarize_linear_layers(tmp) if d[\"seq_index\"] == last_lin_clf][0]\n",
        "n_out_neurons = out_info[\"out_features\"]  # should be 2\n",
        "print(f\"Scanning OUTPUT layer seq_index={last_lin_clf} with {n_out_neurons} neurons...\")\n",
        "\n",
        "_, _, test_loader_out, _ = make_loaders(clf_dir, task=\"clf\", batch_size=512)\n",
        "\n",
        "base_model = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "base_metrics = evaluate_model(base_model, test_loader_out, task=\"clf\")\n",
        "base_acc, base_f1 = base_metrics[\"accuracy\"], base_metrics[\"f1\"]\n",
        "print(f\"Baseline ACC={base_acc:.4f} | F1={base_f1:.4f}\")\n",
        "\n",
        "rows = []\n",
        "for i in range(n_out_neurons):\n",
        "    m = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "    damage_specific_neurons_in_layer(m, layer_lin_seq_index=last_lin_clf, neuron_indices=[i])\n",
        "    met = evaluate_model(m, test_loader_out, task=\"clf\")\n",
        "    rows.append({\n",
        "        \"timestamp\": timestamp(), \"phase\": \"per_neuron_scan\",\n",
        "        \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "        \"layer_seq_index\": int(last_lin_clf), \"neuron_index\": int(i),\n",
        "        \"acc\": float(met[\"accuracy\"]), \"f1\": float(met[\"f1\"]),\n",
        "        \"acc_drop\": float(base_acc - met[\"accuracy\"]), \"f1_drop\": float(base_f1 - met[\"f1\"]),\n",
        "        \"ckpt\": CLF_CKPT\n",
        "    })\n",
        "    print(f\"  neuron {i}: ACC={met['accuracy']:.4f}, Δ={base_acc - met['accuracy']:.4f}\")\n",
        "\n",
        "for r in rows:\n",
        "    log_result(r, csv_name=\"per_neuron_scan_clf_output.csv\")\n",
        "\n",
        "import pandas as pd\n",
        "scan_csv = PROJECT_ROOT / \"results\" / \"per_neuron_scan_clf_output.csv\"\n",
        "df = pd.read_csv(scan_csv)\n",
        "df_sorted = df.sort_values(\"acc_drop\", ascending=False)\n",
        "print(\"\\nPer-neuron scan (OUTPUT) sorted by ΔACC:\\n\", df_sorted[[\"neuron_index\",\"acc\",\"acc_drop\",\"f1\",\"f1_drop\"]].to_string(index=False))\n",
        "\n",
        "top1_out = int(df_sorted.iloc[0][\"neuron_index\"])\n",
        "print(\"Worst single OUTPUT neuron:\", top1_out)\n",
        "\n",
        "# Plot ΔACC per output neuron\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.bar(df[\"neuron_index\"].values, df[\"acc_drop\"].values)\n",
        "plt.xlabel(\"output neuron index\"); plt.ylabel(\"Δ accuracy vs baseline\")\n",
        "plt.title(\"Per-neuron damage impact (OUTPUT layer, clf)\")\n",
        "fig_path = FIG_DIR / \"per_neuron_scan_clf_output_accdrop.png\"\n",
        "plt.savefig(fig_path, bbox_inches=\"tight\"); plt.close()\n",
        "print(\"Figure saved:\", fig_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clf_dir: c:\\Users\\Admin\\Desktop\\ffnn-healing\\data\\breast_cancer\n",
            "CLF_CKPT: c:\\Users\\Admin\\Desktop\\ffnn-healing\\models\\clf_breast_cancer_baseline_20250813-103737_best.pt\n"
          ]
        }
      ],
      "source": [
        "# O1-prep — ensure paths exist after a restart\n",
        "from pathlib import Path, PurePath\n",
        "PROJECT_ROOT = Path(PROJECT_ROOT)  # in case it was a str\n",
        "clf_dir = str(PROJECT_ROOT / \"data\" / \"breast_cancer\")\n",
        "\n",
        "# Fallback to find the latest classifier checkpoint if CLF_CKPT is missing\n",
        "import glob, os\n",
        "if \"CLF_CKPT\" not in globals() or not os.path.exists(str(CLF_CKPT)):\n",
        "    model_candidates = sorted(\n",
        "        glob.glob(str(PROJECT_ROOT / \"models\" / \"clf_breast_cancer_baseline_*_best.pt\")),\n",
        "        key=os.path.getmtime\n",
        "    )\n",
        "    assert model_candidates, \"No classifier baseline checkpoint found in models/.\"\n",
        "    CLF_CKPT = model_candidates[-1]\n",
        "print(\"clf_dir:\", clf_dir)\n",
        "print(\"CLF_CKPT:\", CLF_CKPT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline ACC=0.9535 | F1=0.9630 | 95% threshold=0.9058\n",
            "OUTPUT SINGLE — POST-DAMAGE: {'accuracy': 0.9418604651162791, 'f1': 0.9541284403669725}\n",
            "[heal_clf_OUTPUT_singleN0_short] epoch 001 | train 0.1189 | val 0.0776 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_singleN0_short] epoch 002 | train 0.0928 | val 0.0660 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_singleN0_short] epoch 003 | train 0.0892 | val 0.0608 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_singleN0_short] epoch 004 | train 0.0695 | val 0.0590 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_singleN0_short] epoch 005 | train 0.0607 | val 0.0585 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_singleN0_short] epoch 010 | train 0.0506 | val 0.0612 | metric 0.9765 \n",
            "[heal_clf_OUTPUT_singleN0_short] Early stopping at epoch 10. Best val_loss=0.0585\n",
            "OUTPUT SINGLE — SHORT TEST: {'accuracy': 0.9651162790697675, 'f1': 0.972972972972973} | recovery_epoch_95%: 1\n",
            "[heal_clf_OUTPUT_singleN0_long] epoch 001 | train 0.1219 | val 0.0822 | metric 0.9647 *\n",
            "[heal_clf_OUTPUT_singleN0_long] epoch 002 | train 0.0903 | val 0.0720 | metric 0.9647 *\n",
            "[heal_clf_OUTPUT_singleN0_long] epoch 003 | train 0.0759 | val 0.0657 | metric 0.9647 *\n",
            "[heal_clf_OUTPUT_singleN0_long] epoch 004 | train 0.0736 | val 0.0618 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_singleN0_long] epoch 005 | train 0.0676 | val 0.0594 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_singleN0_long] epoch 006 | train 0.0675 | val 0.0578 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_singleN0_long] epoch 007 | train 0.0616 | val 0.0570 | metric 0.9765 *\n",
            "[heal_clf_OUTPUT_singleN0_long] epoch 008 | train 0.0630 | val 0.0563 | metric 0.9647 *\n",
            "[heal_clf_OUTPUT_singleN0_long] epoch 009 | train 0.0615 | val 0.0562 | metric 0.9647 *\n",
            "[heal_clf_OUTPUT_singleN0_long] epoch 010 | train 0.0630 | val 0.0564 | metric 0.9647 \n",
            "[heal_clf_OUTPUT_singleN0_long] epoch 020 | train 0.0572 | val 0.0628 | metric 0.9765 \n",
            "[heal_clf_OUTPUT_singleN0_long] Early stopping at epoch 29. Best val_loss=0.0562\n",
            "OUTPUT SINGLE — LONG TEST: {'accuracy': 0.9534883720930233, 'f1': 0.9636363636363636} | recovery_epoch_95%: 1\n",
            "O1b — Logged 4 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n",
            "O1b — Plots: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_singleN0_short_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_singleN0_short_accuracy.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_singleN0_long_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_singleN0_long_accuracy.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18436\\2494450625.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df0, pd.DataFrame([row])], ignore_index=True)\n",
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18436\\2494450625.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df0, pd.DataFrame([row])], ignore_index=True)\n",
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18436\\2494450625.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df0, pd.DataFrame([row])], ignore_index=True)\n",
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18436\\2494450625.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df0, pd.DataFrame([row])], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "# O1b — Heal worst single OUTPUT neuron (classification)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# --- tiny helper (only defines if missing): damage specific neurons WITH masks ---\n",
        "try:\n",
        "    damage_specific_neurons_in_layer_with_masks\n",
        "except NameError:\n",
        "    def damage_specific_neurons_in_layer_with_masks(model: nn.Module, layer_lin_seq_index: int, neuron_indices, rng=None, mask_dict=None):\n",
        "        \"\"\"\n",
        "        Zero EXACT neurons in a given Linear layer and build masks to FREEZE those entries during healing.\n",
        "        - Zero rows in target Linear's weight and matching bias entries.\n",
        "        - If there is a NEXT Linear layer, zero the corresponding columns in that next weight too (not the case for OUTPUT).\n",
        "        Returns: (neuron_indices_list, mask_dict)\n",
        "        \"\"\"\n",
        "        if isinstance(neuron_indices, (int, np.integer)):\n",
        "            neuron_indices = [int(neuron_indices)]\n",
        "        neuron_indices = np.array(neuron_indices, dtype=int)\n",
        "\n",
        "        linear_layers = get_linear_layers(model)\n",
        "        target_pos = None\n",
        "        for k, (seq_i, lin) in enumerate(linear_layers):\n",
        "            if seq_i == layer_lin_seq_index:\n",
        "                target_pos = k; break\n",
        "        assert target_pos is not None, f\"No Linear layer at seq index {layer_lin_seq_index}\"\n",
        "        target_seq_i, target_lin = linear_layers[target_pos]\n",
        "        next_lin = linear_layers[target_pos + 1][1] if (target_pos + 1) < len(linear_layers) else None\n",
        "\n",
        "        if mask_dict is None:\n",
        "            mask_dict = init_full_keep_masks_for_model(model)  # 1 = trainable, 0 = frozen\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # zero rows in target weight + bias for damaged neurons\n",
        "            W = target_lin.weight  # [out, in]\n",
        "            W[neuron_indices, :] = 0.0\n",
        "            maskW = mask_dict[W] if W in mask_dict else mask_dict.setdefault(W, torch.ones_like(W))\n",
        "            maskW[neuron_indices, :] = 0.0   # freeze\n",
        "\n",
        "            if target_lin.bias is not None:\n",
        "                b = target_lin.bias\n",
        "                b[neuron_indices] = 0.0\n",
        "                maskb = mask_dict[b] if b in mask_dict else mask_dict.setdefault(b, torch.ones_like(b))\n",
        "                maskb[neuron_indices] = 0.0   # freeze\n",
        "\n",
        "            # If there is a next linear, freeze outgoing columns there too\n",
        "            if next_lin is not None:\n",
        "                Wn = next_lin.weight  # [next_out, next_in]\n",
        "                Wn[:, neuron_indices] = 0.0\n",
        "                maskWn = mask_dict[Wn] if Wn in mask_dict else mask_dict.setdefault(Wn, torch.ones_like(Wn))\n",
        "                maskWn[:, neuron_indices] = 0.0\n",
        "\n",
        "        return neuron_indices.tolist(), mask_dict\n",
        "\n",
        "# --- pick worst output neuron from O1a (your print showed neuron 0) ---\n",
        "last_lin_clf = layer_seq_indices(clf_ctor)[-1]\n",
        "worst_out_neuron = 0\n",
        "\n",
        "# --- data loaders + baseline\n",
        "train_L, val_L, test_L, _ = make_loaders(clf_dir, task=\"clf\", batch_size=256)\n",
        "base_model = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "baseline = evaluate_model(base_model, test_L, task=\"clf\")\n",
        "base_acc, base_f1 = baseline[\"accuracy\"], baseline[\"f1\"]\n",
        "acc95 = 0.95 * base_acc\n",
        "print(f\"Baseline ACC={base_acc:.4f} | F1={base_f1:.4f} | 95% threshold={acc95:.4f}\")\n",
        "\n",
        "# --- make damaged model with masks (freeze the damaged output neuron)\n",
        "damaged = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "mask_dict = init_full_keep_masks_for_model(damaged)\n",
        "_, mask_dict = damage_specific_neurons_in_layer_with_masks(\n",
        "    damaged, layer_lin_seq_index=last_lin_clf, neuron_indices=[worst_out_neuron], mask_dict=mask_dict\n",
        ")\n",
        "post_damage = evaluate_model(damaged, test_L, task=\"clf\")\n",
        "print(\"OUTPUT SINGLE — POST-DAMAGE:\", post_damage)\n",
        "\n",
        "# --- helper to detect when validation accuracy reaches 95% of baseline\n",
        "def recovery_epoch_95(history, baseline_acc):\n",
        "    import numpy as _np\n",
        "    target = 0.95 * baseline_acc\n",
        "    vals = _np.array(history[\"val_metric\"], dtype=float)\n",
        "    hits = _np.where(vals >= target)[0]\n",
        "    return int(history[\"epoch\"][hits[0]]) if hits.size > 0 else None\n",
        "\n",
        "# ---- SHORT HEAL (≈15 epochs)\n",
        "name_short = f\"heal_clf_OUTPUT_singleN{worst_out_neuron}_short\"\n",
        "best_s, hist_s = healing_train_constrained(\n",
        "    model=damaged, mask_dict=mask_dict,\n",
        "    train_loader=train_L, val_loader=val_L, task=\"clf\",\n",
        "    max_epochs=15, lr=1e-3, weight_decay=1e-4, patience=5,\n",
        "    run_name=name_short\n",
        ")\n",
        "m_s = clf_ctor().to(DEVICE); m_s.load_state_dict(torch.load(best_s, map_location=DEVICE))\n",
        "metrics_s = evaluate_model(m_s, test_L, task=\"clf\")\n",
        "rec_s = recovery_epoch_95(hist_s, base_acc)\n",
        "print(f\"OUTPUT SINGLE — SHORT TEST: {metrics_s} | recovery_epoch_95%: {rec_s}\")\n",
        "fig_s_loss, fig_s_metric = plot_history(hist_s, name_short, task=\"clf\")\n",
        "\n",
        "# ---- LONG HEAL (≤150 epochs) — rebuild the same damage fresh\n",
        "damaged2 = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "mask_dict2 = init_full_keep_masks_for_model(damaged2)\n",
        "_, mask_dict2 = damage_specific_neurons_in_layer_with_masks(\n",
        "    damaged2, layer_lin_seq_index=last_lin_clf, neuron_indices=[worst_out_neuron], mask_dict=mask_dict2\n",
        ")\n",
        "name_long = f\"heal_clf_OUTPUT_singleN{worst_out_neuron}_long\"\n",
        "best_l, hist_l = healing_train_constrained(\n",
        "    model=damaged2, mask_dict=mask_dict2,\n",
        "    train_loader=train_L, val_loader=val_L, task=\"clf\",\n",
        "    max_epochs=150, lr=1e-3, weight_decay=1e-4, patience=20,\n",
        "    run_name=name_long\n",
        ")\n",
        "m_l = clf_ctor().to(DEVICE); m_l.load_state_dict(torch.load(best_l, map_location=DEVICE))\n",
        "metrics_l = evaluate_model(m_l, test_L, task=\"clf\")\n",
        "rec_l = recovery_epoch_95(hist_l, base_acc)\n",
        "print(f\"OUTPUT SINGLE — LONG TEST: {metrics_l} | recovery_epoch_95%: {rec_l}\")\n",
        "fig_l_loss, fig_l_metric = plot_history(hist_l, name_long, task=\"clf\")\n",
        "\n",
        "# ---- LOG to healing_runs.csv\n",
        "rows = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"baseline\", \"layer_seq_index\": int(last_lin_clf), \"pct\": None, \"repeats\": 1,\n",
        "     \"neuron_index\": int(worst_out_neuron), \"accuracy\": float(base_acc), \"f1\": float(base_f1)},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"post_damage\", \"layer_seq_index\": int(last_lin_clf), \"pct\": None, \"repeats\": 1,\n",
        "     \"neuron_index\": int(worst_out_neuron), \"accuracy\": float(post_damage[\"accuracy\"]), \"f1\": float(post_damage[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_short\", \"layer_seq_index\": int(last_lin_clf), \"pct\": None, \"repeats\": 1,\n",
        "     \"neuron_index\": int(worst_out_neuron),\n",
        "     \"epochs\": len(hist_s[\"epoch\"]), \"recovery_epoch_95\": rec_s,\n",
        "     \"accuracy\": float(metrics_s[\"accuracy\"]), \"f1\": float(metrics_s[\"f1\"]),\n",
        "     \"best_ckpt\": str(best_s), \"fig_loss\": fig_s_loss, \"fig_metric\": fig_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_long\", \"layer_seq_index\": int(last_lin_clf), \"pct\": None, \"repeats\": 1,\n",
        "     \"neuron_index\": int(worst_out_neuron),\n",
        "     \"epochs\": len(hist_l[\"epoch\"]), \"recovery_epoch_95\": rec_l,\n",
        "     \"accuracy\": float(metrics_l[\"accuracy\"]), \"f1\": float(metrics_l[\"f1\"]),\n",
        "     \"best_ckpt\": str(best_l), \"fig_loss\": fig_l_loss, \"fig_metric\": fig_l_metric},\n",
        "]\n",
        "for r in rows: log_result(r, csv_name=\"healing_runs.csv\")\n",
        "\n",
        "print(\"O1b — Logged 4 rows to\", PROJECT_ROOT / \"results\" / \"healing_runs.csv\")\n",
        "print(\"O1b — Plots:\", fig_s_loss, fig_s_metric, \"|\", fig_l_loss, fig_l_metric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline ACC=0.9535 | F1=0.9630 | 95% threshold=0.9058\n",
            "OUTPUT PAIR — POST-DAMAGE: {'accuracy': 0.37209302325581395, 'f1': 0.0}\n",
            "[heal_clf_OUTPUT_pair_p100_r1_short] epoch 001 | train 0.6931 | val 0.6931 | metric 0.3765 *\n",
            "[heal_clf_OUTPUT_pair_p100_r1_short] Early stopping at epoch 9. Best val_loss=0.6931\n",
            "OUTPUT PAIR — SHORT TEST: {'accuracy': 0.37209302325581395, 'f1': 0.0} | recovery_epoch_95%: None\n",
            "[heal_clf_OUTPUT_pair_p100_r1_long] epoch 001 | train 0.6931 | val 0.6931 | metric 0.3765 *\n",
            "[heal_clf_OUTPUT_pair_p100_r1_long] epoch 010 | train 0.6931 | val 0.6931 | metric 0.3765 \n",
            "[heal_clf_OUTPUT_pair_p100_r1_long] epoch 020 | train 0.6931 | val 0.6931 | metric 0.3765 \n",
            "[heal_clf_OUTPUT_pair_p100_r1_long] epoch 030 | train 0.6931 | val 0.6931 | metric 0.3765 \n",
            "[heal_clf_OUTPUT_pair_p100_r1_long] Early stopping at epoch 31. Best val_loss=0.6931\n",
            "OUTPUT PAIR — LONG TEST: {'accuracy': 0.37209302325581395, 'f1': 0.0} | recovery_epoch_95%: None\n",
            "O1c — Logged 4 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n",
            "O1c — Plots: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_pair_p100_r1_short_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_pair_p100_r1_short_accuracy.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_pair_p100_r1_long_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_clf_OUTPUT_pair_p100_r1_long_accuracy.png\n"
          ]
        }
      ],
      "source": [
        "# OUTPUT layer: damage BOTH neurons (pair) + heal short/long\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "last_lin_clf = layer_seq_indices(clf_ctor)[-1]\n",
        "\n",
        "# Recreate loaders & baseline\n",
        "_, _, test_loader_out, _ = make_loaders(clf_dir, task=\"clf\", batch_size=512)\n",
        "base_model = clone_from_checkpoint(clf_ctor, CLF_CKPT)\n",
        "base_metrics = evaluate_model(base_model, test_loader_out, task=\"clf\")\n",
        "base_acc = base_metrics[\"accuracy\"]; base_f1 = base_metrics[\"f1\"]\n",
        "thr95 = 0.95 * base_acc\n",
        "print(f\"Baseline ACC={base_acc:.4f} | F1={base_f1:.4f} | 95% threshold={thr95:.4f}\")\n",
        "\n",
        "# --- Damage BOTH outputs by setting pct_neurons=1.0 once (equivalent to neurons [0,1])\n",
        "damaged_pair, mask_pair, trainL, valL, testL, m_before, m_after = apply_progressive_neuron_damage_with_masks(\n",
        "    model_ctor=clf_ctor,\n",
        "    ckpt_path=CLF_CKPT,\n",
        "    task=\"clf\",\n",
        "    data_dir=clf_dir,\n",
        "    layer_lin_seq_index=last_lin_clf,\n",
        "    pct_neurons=1.0,   # 100% of output neurons -> both\n",
        "    repeats=1,\n",
        "    seed=12345\n",
        ")\n",
        "print(\"OUTPUT PAIR — POST-DAMAGE:\", m_after)\n",
        "\n",
        "def first_epoch_at_or_above(history, threshold):\n",
        "    for e, v in zip(history[\"epoch\"], history[\"val_metric\"]):\n",
        "        if v >= threshold:\n",
        "            return int(e)\n",
        "    return None\n",
        "\n",
        "# ---- SHORT heal ----\n",
        "run_short = \"heal_clf_OUTPUT_pair_p100_r1_short\"\n",
        "best_s, hist_s = healing_train_constrained(\n",
        "    model=damaged_pair, mask_dict=mask_pair,\n",
        "    train_loader=trainL, val_loader=valL, task=\"clf\",\n",
        "    max_epochs=30, lr=1e-3, weight_decay=1e-4, patience=8,\n",
        "    run_name=run_short\n",
        ")\n",
        "m_s = clf_ctor().to(DEVICE); m_s.load_state_dict(torch.load(best_s, map_location=DEVICE))\n",
        "metrics_s = evaluate_model(m_s, testL, task=\"clf\")\n",
        "recov_s = first_epoch_at_or_above(hist_s, thr95)\n",
        "print(\"OUTPUT PAIR — SHORT TEST:\", metrics_s, \"| recovery_epoch_95%:\", recov_s)\n",
        "fig_s_loss, fig_s_metric = plot_history(hist_s, run_short, task=\"clf\")\n",
        "\n",
        "# ---- LONG heal ----\n",
        "# Recreate the SAME damage again to start clean\n",
        "damaged_pair2, mask_pair2, trainL2, valL2, testL2, _, _ = apply_progressive_neuron_damage_with_masks(\n",
        "    model_ctor=clf_ctor,\n",
        "    ckpt_path=CLF_CKPT,\n",
        "    task=\"clf\",\n",
        "    data_dir=clf_dir,\n",
        "    layer_lin_seq_index=last_lin_clf,\n",
        "    pct_neurons=1.0,\n",
        "    repeats=1,\n",
        "    seed=12345\n",
        ")\n",
        "run_long = \"heal_clf_OUTPUT_pair_p100_r1_long\"\n",
        "best_l, hist_l = healing_train_constrained(\n",
        "    model=damaged_pair2, mask_dict=mask_pair2,\n",
        "    train_loader=trainL2, val_loader=valL2, task=\"clf\",\n",
        "    max_epochs=180, lr=1e-3, weight_decay=1e-4, patience=30,\n",
        "    run_name=run_long\n",
        ")\n",
        "m_l = clf_ctor().to(DEVICE); m_l.load_state_dict(torch.load(best_l, map_location=DEVICE))\n",
        "metrics_l = evaluate_model(m_l, testL2, task=\"clf\")\n",
        "recov_l = first_epoch_at_or_above(hist_l, thr95)\n",
        "print(\"OUTPUT PAIR — LONG TEST:\", metrics_l, \"| recovery_epoch_95%:\", recov_l)\n",
        "fig_l_loss, fig_l_metric = plot_history(hist_l, run_long, task=\"clf\")\n",
        "\n",
        "# ---- Log\n",
        "rows = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"baseline\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 1.0, \"repeats\": 1,\n",
        "     \"accuracy\": float(m_before[\"accuracy\"]), \"f1\": float(m_before[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"post_damage_pair\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 1.0, \"repeats\": 1,\n",
        "     \"accuracy\": float(m_after[\"accuracy\"]), \"f1\": float(m_after[\"f1\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_short_pair\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 1.0, \"repeats\": 1,\n",
        "     \"epochs\": len(hist_s[\"epoch\"]), \"accuracy\": float(metrics_s[\"accuracy\"]), \"f1\": float(metrics_s[\"f1\"]),\n",
        "     \"best_ckpt\": str(best_s), \"fig_loss\": fig_s_loss, \"fig_metric\": fig_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"breast_cancer\", \"task\": \"clf\",\n",
        "     \"stage\": \"healed_long_pair\", \"layer_seq_index\": int(last_lin_clf), \"pct\": 1.0, \"repeats\": 1,\n",
        "     \"epochs\": len(hist_l[\"epoch\"]), \"accuracy\": float(metrics_l[\"accuracy\"]), \"f1\": float(metrics_l[\"f1\"]),\n",
        "     \"best_ckpt\": str(best_l), \"fig_loss\": fig_l_loss, \"fig_metric\": fig_l_metric},\n",
        "]\n",
        "for r in rows:\n",
        "    log_result(r, csv_name=\"healing_runs.csv\")\n",
        "\n",
        "print(\"O1c — Logged 4 rows to\", PROJECT_ROOT/\"results/healing_runs.csv\")\n",
        "print(\"O1c — Plots:\", fig_s_loss, fig_s_metric, \"|\", fig_l_loss, fig_l_metric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline RMSE=0.5373 | MAE=0.3749 | 105% RMSE threshold=0.5642\n",
            "REG OUTPUT — POST-DAMAGE: {'mae': 2.065962314605713, 'rmse': 2.3644262141207717}\n",
            "[heal_reg_OUTPUT_p100_r1_short] epoch 001 | train 5.6109 | val 5.5542 | metric 2.3621 *\n",
            "[heal_reg_OUTPUT_p100_r1_short] Early stopping at epoch 9. Best val_loss=5.5542\n",
            "REG OUTPUT — SHORT TEST: {'mae': 2.065962314605713, 'rmse': 2.3644262141207717} | recovery_epoch_105%RMSE: None\n",
            "[heal_reg_OUTPUT_p100_r1_long] epoch 001 | train 5.6072 | val 5.5542 | metric 2.3621 *\n",
            "[heal_reg_OUTPUT_p100_r1_long] epoch 010 | train 5.6253 | val 5.5542 | metric 2.3621 \n",
            "[heal_reg_OUTPUT_p100_r1_long] epoch 020 | train 5.6136 | val 5.5542 | metric 2.3621 \n",
            "[heal_reg_OUTPUT_p100_r1_long] epoch 030 | train 5.6288 | val 5.5542 | metric 2.3621 \n",
            "[heal_reg_OUTPUT_p100_r1_long] Early stopping at epoch 31. Best val_loss=5.5542\n",
            "REG OUTPUT — LONG TEST: {'mae': 2.065962314605713, 'rmse': 2.3644262141207717} | recovery_epoch_105%RMSE: None\n",
            "RO1 — Logged 4 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n",
            "RO1 — Plots: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_OUTPUT_p100_r1_short_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_OUTPUT_p100_r1_short_RMSE.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_OUTPUT_p100_r1_long_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_OUTPUT_p100_r1_long_RMSE.png\n"
          ]
        }
      ],
      "source": [
        "#  REGRESSION: damage OUTPUT neuron (100%) + masked healing (short/long)\n",
        "set_seed(42)\n",
        "\n",
        "# paths (robust fallback)\n",
        "try:\n",
        "    reg_dir\n",
        "except NameError:\n",
        "    from pathlib import Path\n",
        "    reg_dir = str((PROJECT_ROOT / \"data\" / \"california_housing\").resolve())\n",
        "\n",
        "last_lin_reg = layer_seq_indices(reg_ctor)[-1]  # output layer seq idx\n",
        "\n",
        "# loaders & baseline\n",
        "_, _, test_loader_reg_out, _ = make_loaders(reg_dir, task=\"reg\", batch_size=512)\n",
        "base_reg = clone_from_checkpoint(reg_ctor, REG_CKPT)\n",
        "base_metrics = evaluate_model(base_reg, test_loader_reg_out, task=\"reg\")\n",
        "base_rmse = base_metrics[\"rmse\"]; base_mae = base_metrics[\"mae\"]\n",
        "thr105 = 1.05 * base_rmse  # recovery = back within +5% RMSE of baseline\n",
        "print(f\"Baseline RMSE={base_rmse:.4f} | MAE={base_mae:.4f} | 105% RMSE threshold={thr105:.4f}\")\n",
        "\n",
        "# --- Damage: 100% of output (repeats=1) with masks\n",
        "dam_reg, mask_reg, trainR, valR, testR, m_before, m_after = apply_progressive_neuron_damage_with_masks(\n",
        "    model_ctor=reg_ctor,\n",
        "    ckpt_path=REG_CKPT,\n",
        "    task=\"reg\",\n",
        "    data_dir=reg_dir,\n",
        "    layer_lin_seq_index=last_lin_reg,\n",
        "    pct_neurons=1.0,  # the only output neuron\n",
        "    repeats=1,\n",
        "    seed=12345\n",
        ")\n",
        "print(\"REG OUTPUT — POST-DAMAGE:\", m_after)\n",
        "\n",
        "def first_epoch_at_or_below(history, threshold):\n",
        "    # history['val_metric'] is RMSE for 'reg'\n",
        "    for e, v in zip(history[\"epoch\"], history[\"val_metric\"]):\n",
        "        if v <= threshold:\n",
        "            return int(e)\n",
        "    return None\n",
        "\n",
        "# ---- SHORT heal ----\n",
        "run_short = \"heal_reg_OUTPUT_p100_r1_short\"\n",
        "best_s, hist_s = healing_train_constrained(\n",
        "    model=dam_reg, mask_dict=mask_reg,\n",
        "    train_loader=trainR, val_loader=valR, task=\"reg\",\n",
        "    max_epochs=30, lr=1e-3, weight_decay=1e-4, patience=8,\n",
        "    run_name=run_short\n",
        ")\n",
        "m_s = reg_ctor().to(DEVICE); m_s.load_state_dict(torch.load(best_s, map_location=DEVICE))\n",
        "metrics_s = evaluate_model(m_s, testR, task=\"reg\")\n",
        "recov_s = first_epoch_at_or_below(hist_s, thr105)\n",
        "print(\"REG OUTPUT — SHORT TEST:\", metrics_s, \"| recovery_epoch_105%RMSE:\", recov_s)\n",
        "fig_s_loss, fig_s_metric = plot_history(hist_s, run_short, task=\"reg\")\n",
        "\n",
        "# ---- LONG heal ----\n",
        "dam_reg2, mask_reg2, trainR2, valR2, testR2, _, _ = apply_progressive_neuron_damage_with_masks(\n",
        "    model_ctor=reg_ctor,\n",
        "    ckpt_path=REG_CKPT,\n",
        "    task=\"reg\",\n",
        "    data_dir=reg_dir,\n",
        "    layer_lin_seq_index=last_lin_reg,\n",
        "    pct_neurons=1.0,\n",
        "    repeats=1,\n",
        "    seed=12345\n",
        ")\n",
        "run_long = \"heal_reg_OUTPUT_p100_r1_long\"\n",
        "best_l, hist_l = healing_train_constrained(\n",
        "    model=dam_reg2, mask_dict=mask_reg2,\n",
        "    train_loader=trainR2, val_loader=valR2, task=\"reg\",\n",
        "    max_epochs=180, lr=1e-3, weight_decay=1e-4, patience=30,\n",
        "    run_name=run_long\n",
        ")\n",
        "m_l = reg_ctor().to(DEVICE); m_l.load_state_dict(torch.load(best_l, map_location=DEVICE))\n",
        "metrics_l = evaluate_model(m_l, testR2, task=\"reg\")\n",
        "recov_l = first_epoch_at_or_below(hist_l, thr105)\n",
        "print(\"REG OUTPUT — LONG TEST:\", metrics_l, \"| recovery_epoch_105%RMSE:\", recov_l)\n",
        "fig_l_loss, fig_l_metric = plot_history(hist_l, run_long, task=\"reg\")\n",
        "\n",
        "# ---- Log\n",
        "rows = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"baseline\", \"layer_seq_index\": int(last_lin_reg), \"pct\": 1.0, \"repeats\": 1,\n",
        "     \"mae\": float(m_before[\"mae\"]), \"rmse\": float(m_before[\"rmse\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"post_damage_pair\", \"layer_seq_index\": int(last_lin_reg), \"pct\": 1.0, \"repeats\": 1,\n",
        "     \"mae\": float(m_after[\"mae\"]), \"rmse\": float(m_after[\"rmse\"])},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"healed_short_pair\", \"layer_seq_index\": int(last_lin_reg), \"pct\": 1.0, \"repeats\": 1,\n",
        "     \"epochs\": len(hist_s[\"epoch\"]),\n",
        "     \"mae\": float(metrics_s[\"mae\"]), \"rmse\": float(metrics_s[\"rmse\"]),\n",
        "     \"best_ckpt\": str(best_s), \"fig_loss\": fig_s_loss, \"fig_metric\": fig_s_metric},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"healed_long_pair\", \"layer_seq_index\": int(last_lin_reg), \"pct\": 1.0, \"repeats\": 1,\n",
        "     \"epochs\": len(hist_l[\"epoch\"]),\n",
        "     \"mae\": float(metrics_l[\"mae\"]), \"rmse\": float(metrics_l[\"rmse\"]),\n",
        "     \"best_ckpt\": str(best_l), \"fig_loss\": fig_l_loss, \"fig_metric\": fig_l_metric},\n",
        "]\n",
        "for r in rows:\n",
        "    log_result(r, csv_name=\"healing_runs.csv\")\n",
        "\n",
        "print(\"RO1 — Logged 4 rows to\", PROJECT_ROOT / \"results\" / \"healing_runs.csv\")\n",
        "print(\"RO1 — Plots:\", fig_s_loss, fig_s_metric, \"|\", fig_l_loss, fig_l_metric)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regression middle hidden layer seq_index: 3\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "Logged 17 rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\damage_runs.csv\n",
            "           timestamp  pct  repeats_total  repeat_eval weight_mode  \\\n",
            "266  20250813-134637  0.4              5            5       block   \n",
            "267  20250813-134637  0.4             10            0       block   \n",
            "268  20250813-134637  0.4             10            1       block   \n",
            "269  20250813-134637  0.4             10            2       block   \n",
            "270  20250813-134637  0.4             10            3       block   \n",
            "271  20250813-134637  0.4             10            4       block   \n",
            "272  20250813-134637  0.4             10            5       block   \n",
            "273  20250813-134637  0.4             10            6       block   \n",
            "274  20250813-134637  0.4             10            7       block   \n",
            "275  20250813-134637  0.4             10            8       block   \n",
            "276  20250813-134637  0.4             10            9       block   \n",
            "277  20250813-134637  0.4             10           10       block   \n",
            "\n",
            "     baseline_metric     value  delta_from_baseline  \n",
            "266         0.537336  0.974811             0.437476  \n",
            "267         0.537336  0.537336             0.000000  \n",
            "268         0.537336  0.640544             0.103208  \n",
            "269         0.537336  0.684765             0.147429  \n",
            "270         0.537336  0.964019             0.426683  \n",
            "271         0.537336  0.974281             0.436946  \n",
            "272         0.537336  0.974811             0.437476  \n",
            "273         0.537336  0.974811             0.437476  \n",
            "274         0.537336  0.984085             0.446749  \n",
            "275         0.537336  1.008727             0.471391  \n",
            "276         0.537336  1.008727             0.471391  \n",
            "277         0.537336  1.102445             0.565110  \n"
          ]
        }
      ],
      "source": [
        "# Regression · middle hidden · weight damage (random & block)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Middle layer seq index for the regression model\n",
        "mid_lin_reg = layer_seq_indices(reg_ctor)[1]  # should be 3 given [0,3,6,9] summary\n",
        "print(\"Regression middle hidden layer seq_index:\", mid_lin_reg)\n",
        "\n",
        "# 20% & 40% weights, repeats = [5, 10] — RANDOM mask\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=reg_dir,\n",
        "    layer_seq_index=mid_lin_reg, pct=0.20, repeats_list=[5, 10],\n",
        "    damage_type=\"weights\", weight_mode=\"random\", csv_name=\"damage_runs.csv\", dataset_name=\"california_housing\"\n",
        ")\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=reg_dir,\n",
        "    layer_seq_index=mid_lin_reg, pct=0.40, repeats_list=[5, 10],\n",
        "    damage_type=\"weights\", weight_mode=\"random\", csv_name=\"damage_runs.csv\", dataset_name=\"california_housing\"\n",
        ")\n",
        "\n",
        "# 20% & 40% weights, repeats = [5, 10] — BLOCK mask\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=reg_dir,\n",
        "    layer_seq_index=mid_lin_reg, pct=0.20, repeats_list=[5, 10],\n",
        "    damage_type=\"weights\", weight_mode=\"block\", csv_name=\"damage_runs.csv\", dataset_name=\"california_housing\"\n",
        ")\n",
        "_ = run_damage_experiment(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=reg_dir,\n",
        "    layer_seq_index=mid_lin_reg, pct=0.40, repeats_list=[5, 10],\n",
        "    damage_type=\"weights\", weight_mode=\"block\", csv_name=\"damage_runs.csv\", dataset_name=\"california_housing\"\n",
        ")\n",
        "\n",
        "# Peek what we just logged\n",
        "import pandas as pd\n",
        "df = pd.read_csv(PROJECT_ROOT/\"results/damage_runs.csv\")\n",
        "mask = (\n",
        "    (df[\"dataset\"]==\"california_housing\") & (df[\"task\"]==\"reg\") &\n",
        "    (df[\"damage_type\"]==\"weights\") & (df[\"layer_seq_index\"]==mid_lin_reg) &\n",
        "    (df[\"pct\"].isin([0.2, 0.4])) & (df[\"repeats_total\"].isin([5,10]))\n",
        ")\n",
        "print(df[mask].tail(12)[[\n",
        "    \"timestamp\",\"pct\",\"repeats_total\",\"repeat_eval\",\"weight_mode\",\n",
        "    \"baseline_metric\",\"value\",\"delta_from_baseline\"\n",
        "]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Worst RANDOM: {'pct': 0.4, 'repeats_total': 10, 'delta_from_baseline': 0.6596000433848619, 'value': 1.1969355615179158, 'baseline_metric': 0.5373355181330539}\n",
            "Worst BLOCK : {'pct': 0.4, 'repeats_total': 10, 'delta_from_baseline': 0.5651095819028344, 'value': 1.1024451000358884, 'baseline_metric': 0.5373355181330539}\n",
            "Baseline RMSE=0.5373 | 105% threshold=0.5642\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 001 | train 1.2602 | val 1.0895 | metric 1.0472 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 002 | train 1.0957 | val 0.9508 | metric 0.9751 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 003 | train 0.9306 | val 0.7799 | metric 0.8758 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 004 | train 0.7570 | val 0.6231 | metric 0.7683 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 005 | train 0.6328 | val 0.5418 | metric 0.7080 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 006 | train 0.5686 | val 0.5018 | metric 0.6752 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 007 | train 0.5519 | val 0.4863 | metric 0.6632 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 008 | train 0.5268 | val 0.4708 | metric 0.6532 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 009 | train 0.4989 | val 0.4587 | metric 0.6432 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 010 | train 0.4904 | val 0.4547 | metric 0.6416 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 012 | train 0.4791 | val 0.4421 | metric 0.6350 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_short] epoch 014 | train 0.4559 | val 0.4348 | metric 0.6304 *\n",
            "RANDOM — SHORT TEST: {'mae': 0.4307464361190796, 'rmse': 0.5969960674111635} | recovery_epoch_105%RMSE: None\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 001 | train 1.2569 | val 1.0909 | metric 1.0476 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 002 | train 1.0985 | val 0.9557 | metric 0.9778 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 003 | train 0.9355 | val 0.7826 | metric 0.8779 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 004 | train 0.7664 | val 0.6321 | metric 0.7765 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 005 | train 0.6492 | val 0.5489 | metric 0.7154 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 006 | train 0.5750 | val 0.4996 | metric 0.6805 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 007 | train 0.5366 | val 0.4851 | metric 0.6629 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 008 | train 0.5337 | val 0.4780 | metric 0.6542 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 009 | train 0.5135 | val 0.4620 | metric 0.6468 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 010 | train 0.4885 | val 0.4575 | metric 0.6442 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 011 | train 0.4878 | val 0.4491 | metric 0.6409 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 013 | train 0.4741 | val 0.4461 | metric 0.6368 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 014 | train 0.4704 | val 0.4421 | metric 0.6339 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 015 | train 0.4656 | val 0.4397 | metric 0.6310 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 016 | train 0.4636 | val 0.4359 | metric 0.6307 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 017 | train 0.4516 | val 0.4291 | metric 0.6270 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 019 | train 0.4505 | val 0.4224 | metric 0.6228 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 020 | train 0.4434 | val 0.4154 | metric 0.6199 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 022 | train 0.4420 | val 0.4075 | metric 0.6159 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 027 | train 0.4240 | val 0.4017 | metric 0.6102 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 029 | train 0.4194 | val 0.4005 | metric 0.6078 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 030 | train 0.4228 | val 0.3999 | metric 0.6077 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 031 | train 0.4157 | val 0.3990 | metric 0.6084 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 032 | train 0.4235 | val 0.3976 | metric 0.6060 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 033 | train 0.4137 | val 0.3941 | metric 0.6035 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 036 | train 0.4156 | val 0.3910 | metric 0.6021 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 037 | train 0.4030 | val 0.3908 | metric 0.6018 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 038 | train 0.4043 | val 0.3864 | metric 0.5995 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 040 | train 0.4057 | val 0.3853 | metric 0.5986 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 041 | train 0.4098 | val 0.3824 | metric 0.5972 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 042 | train 0.4054 | val 0.3807 | metric 0.5955 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 045 | train 0.3956 | val 0.3799 | metric 0.5945 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 046 | train 0.3981 | val 0.3784 | metric 0.5936 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 050 | train 0.3946 | val 0.3757 | metric 0.5913 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 056 | train 0.3849 | val 0.3732 | metric 0.5892 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 057 | train 0.3892 | val 0.3697 | metric 0.5882 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 059 | train 0.3852 | val 0.3678 | metric 0.5865 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 060 | train 0.3803 | val 0.3731 | metric 0.5908 \n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 062 | train 0.3924 | val 0.3665 | metric 0.5862 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 063 | train 0.3870 | val 0.3651 | metric 0.5850 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 066 | train 0.3849 | val 0.3634 | metric 0.5830 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 068 | train 0.3838 | val 0.3617 | metric 0.5826 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 070 | train 0.3873 | val 0.3689 | metric 0.5876 \n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 071 | train 0.3825 | val 0.3614 | metric 0.5816 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 074 | train 0.3785 | val 0.3599 | metric 0.5801 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 075 | train 0.3753 | val 0.3573 | metric 0.5796 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 080 | train 0.3732 | val 0.3576 | metric 0.5776 \n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 084 | train 0.3687 | val 0.3571 | metric 0.5773 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 085 | train 0.3679 | val 0.3540 | metric 0.5763 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 086 | train 0.3665 | val 0.3538 | metric 0.5756 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 088 | train 0.3665 | val 0.3529 | metric 0.5753 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 090 | train 0.3715 | val 0.3540 | metric 0.5757 \n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 094 | train 0.3627 | val 0.3510 | metric 0.5738 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 100 | train 0.3619 | val 0.3533 | metric 0.5738 \n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 103 | train 0.3599 | val 0.3500 | metric 0.5727 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 108 | train 0.3629 | val 0.3497 | metric 0.5732 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 110 | train 0.3603 | val 0.3477 | metric 0.5722 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 119 | train 0.3628 | val 0.3474 | metric 0.5721 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 120 | train 0.3600 | val 0.3485 | metric 0.5727 \n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 122 | train 0.3623 | val 0.3471 | metric 0.5719 *\n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 130 | train 0.3625 | val 0.3496 | metric 0.5725 \n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] epoch 140 | train 0.3607 | val 0.3486 | metric 0.5720 \n",
            "[heal_reg_midWEIGHTS_random_p40_r10_long] Early stopping at epoch 142. Best val_loss=0.3471\n",
            "RANDOM — LONG TEST: {'mae': 0.37934011220932007, 'rmse': 0.5377141980375202} | recovery_epoch_105%RMSE: None\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 001 | train 0.9192 | val 0.6224 | metric 0.7769 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 002 | train 0.6162 | val 0.5181 | metric 0.6973 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 003 | train 0.5397 | val 0.4789 | metric 0.6670 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 004 | train 0.5099 | val 0.4620 | metric 0.6552 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 005 | train 0.4877 | val 0.4459 | metric 0.6438 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 006 | train 0.4755 | val 0.4421 | metric 0.6394 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 007 | train 0.4578 | val 0.4219 | metric 0.6261 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 009 | train 0.4449 | val 0.4037 | metric 0.6154 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 010 | train 0.4334 | val 0.3991 | metric 0.6128 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 011 | train 0.4307 | val 0.3983 | metric 0.6082 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 012 | train 0.4195 | val 0.3952 | metric 0.6078 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 013 | train 0.4133 | val 0.3876 | metric 0.6009 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_short] epoch 014 | train 0.4051 | val 0.3829 | metric 0.5997 *\n",
            "BLOCK — SHORT TEST: {'mae': 0.40124398469924927, 'rmse': 0.5643267374535746} | recovery_epoch_105%RMSE: None\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 001 | train 0.8923 | val 0.6090 | metric 0.7666 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 002 | train 0.6094 | val 0.5186 | metric 0.6959 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 003 | train 0.5489 | val 0.4817 | metric 0.6673 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 004 | train 0.5179 | val 0.4599 | metric 0.6535 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 005 | train 0.4837 | val 0.4454 | metric 0.6428 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 006 | train 0.4764 | val 0.4351 | metric 0.6340 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 008 | train 0.4486 | val 0.4239 | metric 0.6248 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 009 | train 0.4438 | val 0.4135 | metric 0.6192 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 010 | train 0.4266 | val 0.4091 | metric 0.6181 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 011 | train 0.4649 | val 0.4089 | metric 0.6149 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 012 | train 0.4332 | val 0.3968 | metric 0.6093 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 013 | train 0.4213 | val 0.3948 | metric 0.6053 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 015 | train 0.4057 | val 0.3855 | metric 0.5989 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 016 | train 0.4086 | val 0.3832 | metric 0.5972 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 017 | train 0.3954 | val 0.3792 | metric 0.5951 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 018 | train 0.3995 | val 0.3772 | metric 0.5924 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 019 | train 0.3931 | val 0.3725 | metric 0.5885 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 020 | train 0.3992 | val 0.3732 | metric 0.5892 \n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 021 | train 0.3869 | val 0.3707 | metric 0.5856 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 022 | train 0.3874 | val 0.3693 | metric 0.5848 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 024 | train 0.3766 | val 0.3637 | metric 0.5804 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 025 | train 0.3817 | val 0.3593 | metric 0.5785 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 028 | train 0.3733 | val 0.3563 | metric 0.5733 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 029 | train 0.3681 | val 0.3562 | metric 0.5745 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 030 | train 0.3596 | val 0.3651 | metric 0.5793 \n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 031 | train 0.3654 | val 0.3511 | metric 0.5691 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 033 | train 0.3638 | val 0.3507 | metric 0.5692 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 034 | train 0.3557 | val 0.3466 | metric 0.5674 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 036 | train 0.3523 | val 0.3462 | metric 0.5681 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 037 | train 0.3521 | val 0.3419 | metric 0.5644 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 040 | train 0.3474 | val 0.3404 | metric 0.5625 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 043 | train 0.3484 | val 0.3402 | metric 0.5612 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 044 | train 0.3399 | val 0.3393 | metric 0.5609 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 045 | train 0.3386 | val 0.3377 | metric 0.5596 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 046 | train 0.3440 | val 0.3344 | metric 0.5591 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 048 | train 0.3368 | val 0.3343 | metric 0.5568 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 049 | train 0.3411 | val 0.3333 | metric 0.5576 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 050 | train 0.3373 | val 0.3361 | metric 0.5569 \n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 053 | train 0.3367 | val 0.3303 | metric 0.5548 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 054 | train 0.3359 | val 0.3276 | metric 0.5542 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 055 | train 0.3398 | val 0.3249 | metric 0.5527 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 058 | train 0.3294 | val 0.3235 | metric 0.5520 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 060 | train 0.3249 | val 0.3208 | metric 0.5507 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 063 | train 0.3242 | val 0.3205 | metric 0.5481 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 067 | train 0.3233 | val 0.3204 | metric 0.5476 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 069 | train 0.3208 | val 0.3191 | metric 0.5476 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 070 | train 0.3189 | val 0.3200 | metric 0.5475 \n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 073 | train 0.3214 | val 0.3168 | metric 0.5456 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 074 | train 0.3204 | val 0.3165 | metric 0.5461 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 075 | train 0.3193 | val 0.3138 | metric 0.5466 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 076 | train 0.3169 | val 0.3135 | metric 0.5450 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 078 | train 0.3185 | val 0.3127 | metric 0.5458 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 080 | train 0.3134 | val 0.3149 | metric 0.5450 \n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 084 | train 0.3114 | val 0.3121 | metric 0.5439 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 090 | train 0.3111 | val 0.3097 | metric 0.5430 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 095 | train 0.3074 | val 0.3069 | metric 0.5418 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 100 | train 0.3044 | val 0.3161 | metric 0.5412 \n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 110 | train 0.3020 | val 0.3083 | metric 0.5395 \n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 111 | train 0.3038 | val 0.3064 | metric 0.5383 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 118 | train 0.3023 | val 0.3055 | metric 0.5374 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 120 | train 0.2980 | val 0.3053 | metric 0.5378 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 121 | train 0.2930 | val 0.3053 | metric 0.5376 *\n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 130 | train 0.2997 | val 0.3079 | metric 0.5381 \n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] epoch 140 | train 0.2985 | val 0.3064 | metric 0.5378 \n",
            "[heal_reg_midWEIGHTS_block_p40_r10_long] Early stopping at epoch 141. Best val_loss=0.3053\n",
            "BLOCK — LONG TEST: {'mae': 0.3445846140384674, 'rmse': 0.4972044608234059} | recovery_epoch_105%RMSE: 40\n",
            "RW2 — Logged RANDOM & BLOCK healing rows to c:\\Users\\Admin\\Desktop\\ffnn-healing\\results\\healing_runs.csv\n",
            "RW2 — Plots: c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_midWEIGHTS_random_p40_r10_short_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_midWEIGHTS_random_p40_r10_short_RMSE.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_midWEIGHTS_random_p40_r10_long_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_midWEIGHTS_random_p40_r10_long_RMSE.png || c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_midWEIGHTS_block_p40_r10_short_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_midWEIGHTS_block_p40_r10_short_RMSE.png | c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_midWEIGHTS_block_p40_r10_long_loss.png c:\\Users\\Admin\\Desktop\\ffnn-healing\\figures\\heal_reg_midWEIGHTS_block_p40_r10_long_RMSE.png\n"
          ]
        }
      ],
      "source": [
        "# Regression · middle hidden · weights: heal worst RANDOM and BLOCK\n",
        "\n",
        "import numpy as np, pandas as pd, math, json, copy\n",
        "import torch, torch.nn as nn\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "mid_lin_reg = layer_seq_indices(reg_ctor)[1]  # middle layer seq index\n",
        "\n",
        "# --- helpers: weight damage WITH masks (freezes damaged weights during healing) ---\n",
        "def damage_weights_in_layer_with_masks(model: nn.Module, layer_lin_seq_index: int,\n",
        "                                       pct_weights: float, rng: np.random.Generator,\n",
        "                                       mode=\"random\", mask_dict=None):\n",
        "    linear_layers = get_linear_layers(model)\n",
        "    target_lin = None\n",
        "    for seq_i, lin in linear_layers:\n",
        "        if seq_i == layer_lin_seq_index:\n",
        "            target_lin = lin; break\n",
        "    assert target_lin is not None, f\"No Linear at seq {layer_lin_seq_index}\"\n",
        "\n",
        "    if mask_dict is None:\n",
        "        mask_dict = init_full_keep_masks_for_model(model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        W = target_lin.weight  # [out,in]\n",
        "        H, K = W.shape\n",
        "        total = H*K\n",
        "        n_dmg = max(1, int(round(pct_weights * total)))\n",
        "        if mode == \"random\":\n",
        "            idx = rng.choice(total, size=n_dmg, replace=False)\n",
        "            rows = (idx // K).astype(int)\n",
        "            cols = (idx % K).astype(int)\n",
        "            W[rows, cols] = 0.0\n",
        "            mask_dict[W][rows, cols] = 0.0\n",
        "        elif mode == \"block\":\n",
        "            h = max(1, int(round(np.sqrt(n_dmg))))\n",
        "            k = max(1, int(round(n_dmg / h)))\n",
        "            r0 = int(rng.integers(0, max(1, H - h + 1)))\n",
        "            c0 = int(rng.integers(0, max(1, K - k + 1)))\n",
        "            W[r0:r0+h, c0:c0+k] = 0.0\n",
        "            mask_dict[W][r0:r0+h, c0:c0+k] = 0.0\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'random' or 'block'\")\n",
        "    return mask_dict\n",
        "\n",
        "def apply_progressive_weight_damage_with_masks(model_ctor, ckpt_path, task, data_dir,\n",
        "                                               layer_lin_seq_index, pct_weights, repeats, mode=\"random\", seed=12345):\n",
        "    # loaders\n",
        "    if task == \"clf\":\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"clf\", batch_size=256)\n",
        "    else:\n",
        "        train_loader, val_loader, test_loader, _ = make_loaders(data_dir, \"reg\", batch_size=512)\n",
        "\n",
        "    model = clone_from_checkpoint(model_ctor, ckpt_path)\n",
        "    base = evaluate_model(model, test_loader, task)\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    mask_dict = init_full_keep_masks_for_model(model)\n",
        "    for _ in range(repeats):\n",
        "        mask_dict = damage_weights_in_layer_with_masks(\n",
        "            model, layer_lin_seq_index=layer_lin_seq_index, pct_weights=pct_weights, rng=rng, mode=mode, mask_dict=mask_dict\n",
        "        )\n",
        "    after = evaluate_model(model, test_loader, task)\n",
        "    return model, mask_dict, train_loader, val_loader, test_loader, base, after\n",
        "\n",
        "def recovery_epoch_leq(history, threshold):\n",
        "    \"\"\"First epoch where val_metric <= threshold (for RMSE).\"\"\"\n",
        "    for e, v in zip(history[\"epoch\"], history[\"val_metric\"]):\n",
        "        if v <= threshold:\n",
        "            return int(e)\n",
        "    return None\n",
        "\n",
        "# --- pick worst random & worst block from damage_runs.csv ---\n",
        "df = pd.read_csv(PROJECT_ROOT/\"results/damage_runs.csv\")\n",
        "filt = (\n",
        "    (df[\"dataset\"]==\"california_housing\") & (df[\"task\"]==\"reg\") &\n",
        "    (df[\"damage_type\"]==\"weights\") & (df[\"layer_seq_index\"]==mid_lin_reg) &\n",
        "    (df[\"repeats_total\"]==10) & (df[\"pct\"]==0.4) & (df[\"repeat_eval\"]==10)\n",
        ")\n",
        "df_sel = df[filt].copy()\n",
        "\n",
        "worst_random = df_sel[df_sel[\"weight_mode\"]==\"random\"].sort_values(\"delta_from_baseline\", ascending=False).iloc[0].to_dict()\n",
        "worst_block  = df_sel[df_sel[\"weight_mode\"]==\"block\" ].sort_values(\"delta_from_baseline\", ascending=False).iloc[0].to_dict()\n",
        "\n",
        "print(\"Worst RANDOM:\", {k: worst_random[k] for k in [\"pct\",\"repeats_total\",\"delta_from_baseline\",\"value\",\"baseline_metric\"]})\n",
        "print(\"Worst BLOCK :\", {k: worst_block[k]  for k in [\"pct\",\"repeats_total\",\"delta_from_baseline\",\"value\",\"baseline_metric\"]})\n",
        "\n",
        "# Threshold = 105% of baseline RMSE (recovery means RMSE ≤ 1.05 * baseline)\n",
        "baseline_rmse = float(worst_random[\"baseline_metric\"])\n",
        "rmse_thresh = 1.05 * baseline_rmse\n",
        "print(f\"Baseline RMSE={baseline_rmse:.4f} | 105% threshold={rmse_thresh:.4f}\")\n",
        "\n",
        "# ============ RANDOM case ============\n",
        "r_short_name = \"heal_reg_midWEIGHTS_random_p40_r10_short\"\n",
        "damR, maskR, trR, vaR, teR, baseR, afterR = apply_progressive_weight_damage_with_masks(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=reg_dir,\n",
        "    layer_lin_seq_index=mid_lin_reg, pct_weights=0.40, repeats=10, mode=\"random\", seed=12345\n",
        ")\n",
        "best_r_s, hist_r_s = healing_train_constrained(\n",
        "    model=damR, mask_dict=maskR, train_loader=trR, val_loader=vaR, task=\"reg\",\n",
        "    max_epochs=15, lr=1e-3, weight_decay=1e-4, patience=5, run_name=r_short_name\n",
        ")\n",
        "m_r_s = reg_ctor().to(DEVICE); m_r_s.load_state_dict(torch.load(best_r_s, map_location=DEVICE))\n",
        "met_r_s = evaluate_model(m_r_s, teR, task=\"reg\")\n",
        "rec_ep_r_s = recovery_epoch_leq(hist_r_s, rmse_thresh)\n",
        "print(\"RANDOM — SHORT TEST:\", met_r_s, \"| recovery_epoch_105%RMSE:\", rec_ep_r_s)\n",
        "\n",
        "# fresh damage for LONG\n",
        "damR2, maskR2, trR2, vaR2, teR2, _, _ = apply_progressive_weight_damage_with_masks(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=reg_dir,\n",
        "    layer_lin_seq_index=mid_lin_reg, pct_weights=0.40, repeats=10, mode=\"random\", seed=12345\n",
        ")\n",
        "r_long_name = \"heal_reg_midWEIGHTS_random_p40_r10_long\"\n",
        "best_r_l, hist_r_l = healing_train_constrained(\n",
        "    model=damR2, mask_dict=maskR2, train_loader=trR2, val_loader=vaR2, task=\"reg\",\n",
        "    max_epochs=150, lr=1e-3, weight_decay=1e-4, patience=20, run_name=r_long_name\n",
        ")\n",
        "m_r_l = reg_ctor().to(DEVICE); m_r_l.load_state_dict(torch.load(best_r_l, map_location=DEVICE))\n",
        "met_r_l = evaluate_model(m_r_l, teR2, task=\"reg\")\n",
        "rec_ep_r_l = recovery_epoch_leq(hist_r_l, rmse_thresh)\n",
        "print(\"RANDOM — LONG TEST:\", met_r_l, \"| recovery_epoch_105%RMSE:\", rec_ep_r_l)\n",
        "\n",
        "fig_r_s_loss, fig_r_s_metric = plot_history(hist_r_s, r_short_name, task=\"reg\")\n",
        "fig_r_l_loss, fig_r_l_metric = plot_history(hist_r_l, r_long_name, task=\"reg\")\n",
        "\n",
        "# Log RANDOM\n",
        "rows = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"baseline\", \"layer_seq_index\": int(mid_lin_reg), \"pct\": 0.40, \"repeats\": 10,\n",
        "     \"rmse\": float(baseR[\"rmse\"]), \"mae\": float(baseR[\"mae\"]), \"weight_mode\":\"random\"},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"post_damage\", \"layer_seq_index\": int(mid_lin_reg), \"pct\": 0.40, \"repeats\": 10,\n",
        "     \"rmse\": float(afterR[\"rmse\"]), \"mae\": float(afterR[\"mae\"]), \"weight_mode\":\"random\"},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"healed_short\", \"layer_seq_index\": int(mid_lin_reg), \"pct\": 0.40, \"repeats\": 10,\n",
        "     \"epochs\": len(hist_r_s[\"epoch\"]), \"rmse\": float(met_r_s[\"rmse\"]), \"mae\": float(met_r_s[\"mae\"]),\n",
        "     \"recovery_epoch_105pct\": rec_ep_r_s, \"best_ckpt\": str(best_r_s),\n",
        "     \"fig_loss\": fig_r_s_loss, \"fig_metric\": fig_r_s_metric, \"weight_mode\":\"random\"},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"healed_long\", \"layer_seq_index\": int(mid_lin_reg), \"pct\": 0.40, \"repeats\": 10,\n",
        "     \"epochs\": len(hist_r_l[\"epoch\"]), \"rmse\": float(met_r_l[\"rmse\"]), \"mae\": float(met_r_l[\"mae\"]),\n",
        "     \"recovery_epoch_105pct\": rec_ep_r_l, \"best_ckpt\": str(best_r_l),\n",
        "     \"fig_loss\": fig_r_l_loss, \"fig_metric\": fig_r_l_metric, \"weight_mode\":\"random\"},\n",
        "]\n",
        "for rr in rows: log_result(rr, csv_name=\"healing_runs.csv\")\n",
        "\n",
        "# ============ BLOCK case ============\n",
        "b_short_name = \"heal_reg_midWEIGHTS_block_p40_r10_short\"\n",
        "damB, maskB, trB, vaB, teB, baseB, afterB = apply_progressive_weight_damage_with_masks(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=reg_dir,\n",
        "    layer_lin_seq_index=mid_lin_reg, pct_weights=0.40, repeats=10, mode=\"block\", seed=12345\n",
        ")\n",
        "best_b_s, hist_b_s = healing_train_constrained(\n",
        "    model=damB, mask_dict=maskB, train_loader=trB, val_loader=vaB, task=\"reg\",\n",
        "    max_epochs=15, lr=1e-3, weight_decay=1e-4, patience=5, run_name=b_short_name\n",
        ")\n",
        "m_b_s = reg_ctor().to(DEVICE); m_b_s.load_state_dict(torch.load(best_b_s, map_location=DEVICE))\n",
        "met_b_s = evaluate_model(m_b_s, teB, task=\"reg\")\n",
        "rec_ep_b_s = recovery_epoch_leq(hist_b_s, rmse_thresh)\n",
        "print(\"BLOCK — SHORT TEST:\", met_b_s, \"| recovery_epoch_105%RMSE:\", rec_ep_b_s)\n",
        "\n",
        "# fresh damage for LONG\n",
        "damB2, maskB2, trB2, vaB2, teB2, _, _ = apply_progressive_weight_damage_with_masks(\n",
        "    model_ctor=reg_ctor, ckpt_path=REG_CKPT, task=\"reg\", data_dir=reg_dir,\n",
        "    layer_lin_seq_index=mid_lin_reg, pct_weights=0.40, repeats=10, mode=\"block\", seed=12345\n",
        ")\n",
        "b_long_name = \"heal_reg_midWEIGHTS_block_p40_r10_long\"\n",
        "best_b_l, hist_b_l = healing_train_constrained(\n",
        "    model=damB2, mask_dict=maskB2, train_loader=trB2, val_loader=vaB2, task=\"reg\",\n",
        "    max_epochs=150, lr=1e-3, weight_decay=1e-4, patience=20, run_name=b_long_name\n",
        ")\n",
        "m_b_l = reg_ctor().to(DEVICE); m_b_l.load_state_dict(torch.load(best_b_l, map_location=DEVICE))\n",
        "met_b_l = evaluate_model(m_b_l, teB2, task=\"reg\")\n",
        "rec_ep_b_l = recovery_epoch_leq(hist_b_l, rmse_thresh)\n",
        "print(\"BLOCK — LONG TEST:\", met_b_l, \"| recovery_epoch_105%RMSE:\", rec_ep_b_l)\n",
        "\n",
        "fig_b_s_loss, fig_b_s_metric = plot_history(hist_b_s, b_short_name, task=\"reg\")\n",
        "fig_b_l_loss, fig_b_l_metric = plot_history(hist_b_l, b_long_name, task=\"reg\")\n",
        "\n",
        "# Log BLOCK\n",
        "rows = [\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"baseline\", \"layer_seq_index\": int(mid_lin_reg), \"pct\": 0.40, \"repeats\": 10,\n",
        "     \"rmse\": float(baseB[\"rmse\"]), \"mae\": float(baseB[\"mae\"]), \"weight_mode\":\"block\"},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"post_damage\", \"layer_seq_index\": int(mid_lin_reg), \"pct\": 0.40, \"repeats\": 10,\n",
        "     \"rmse\": float(afterB[\"rmse\"]), \"mae\": float(afterB[\"mae\"]), \"weight_mode\":\"block\"},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"healed_short\", \"layer_seq_index\": int(mid_lin_reg), \"pct\": 0.40, \"repeats\": 10,\n",
        "     \"epochs\": len(hist_b_s[\"epoch\"]), \"rmse\": float(met_b_s[\"rmse\"]), \"mae\": float(met_b_s[\"mae\"]),\n",
        "     \"recovery_epoch_105pct\": rec_ep_b_s, \"best_ckpt\": str(best_b_s),\n",
        "     \"fig_loss\": fig_b_s_loss, \"fig_metric\": fig_b_s_metric, \"weight_mode\":\"block\"},\n",
        "    {\"timestamp\": timestamp(), \"phase\": \"healing\", \"dataset\": \"california_housing\", \"task\": \"reg\",\n",
        "     \"stage\": \"healed_long\", \"layer_seq_index\": int(mid_lin_reg), \"pct\": 0.40, \"repeats\": 10,\n",
        "     \"epochs\": len(hist_b_l[\"epoch\"]), \"rmse\": float(met_b_l[\"rmse\"]), \"mae\": float(met_b_l[\"mae\"]),\n",
        "     \"recovery_epoch_105pct\": rec_ep_b_l, \"best_ckpt\": str(best_b_l),\n",
        "     \"fig_loss\": fig_b_l_loss, \"fig_metric\": fig_b_l_metric, \"weight_mode\":\"block\"},\n",
        "]\n",
        "for rr in rows: log_result(rr, csv_name=\"healing_runs.csv\")\n",
        "\n",
        "print(\"RW2 — Logged RANDOM & BLOCK healing rows to\", PROJECT_ROOT / \"results\" / \"healing_runs.csv\")\n",
        "print(\"RW2 — Plots:\",\n",
        "      fig_r_s_loss, fig_r_s_metric, \"|\", fig_r_l_loss, fig_r_l_metric, \"||\",\n",
        "      fig_b_s_loss, fig_b_s_metric, \"|\", fig_b_l_loss, fig_b_l_metric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ffnn-healing",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
